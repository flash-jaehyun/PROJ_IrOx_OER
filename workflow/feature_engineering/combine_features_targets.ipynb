{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining features and adsorption energies into one dataframe\n",
    "---\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.max_colwidth = 100\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_eff_ox,\n",
    "    get_df_ads,\n",
    "    get_df_features,\n",
    "    get_df_jobs,\n",
    "    get_df_slab,\n",
    "    get_df_jobs_data,\n",
    "    get_df_dft,\n",
    "    get_df_job_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"g_o\", \"g_oh\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eff_ox = get_df_eff_ox()\n",
    "\n",
    "df_ads = get_df_ads()\n",
    "df_ads = df_ads.set_index([\"compenv\", \"slab_id\", \"active_site\", ], drop=False)\n",
    "\n",
    "df_features = get_df_features()\n",
    "df_features.index = df_features.index.droplevel(level=5)\n",
    "\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "df_slab = get_df_slab()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "df_jobs_data[\"rerun_from_oh\"] = df_jobs_data[\"rerun_from_oh\"].fillna(value=False)\n",
    "\n",
    "df_dft = get_df_dft()\n",
    "\n",
    "df_job_ids = get_df_job_ids()\n",
    "df_job_ids = df_job_ids.set_index(\"job_id\")\n",
    "df_job_ids = df_job_ids[~df_job_ids.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_features[\"features\"].columns.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting other relevent data columns from various data objects"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for index_i, row_i in df_ads.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(\n",
    "        list(df_ads.index.names), index_i, ))\n",
    "    # #####################################################\n",
    "    slab_id_i = row_i.slab_id\n",
    "    job_id_o = row_i.job_id_o\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    row_ids_i = df_job_ids.loc[job_id_o]\n",
    "    # #####################################################\n",
    "    bulk_id_i = row_ids_i.bulk_id\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_dft_i = df_dft.loc[bulk_id_i]\n",
    "    # #####################################################\n",
    "    stoich_i = row_dft_i.stoich\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_slab_i = df_slab.loc[slab_id_i]\n",
    "    # #####################################################\n",
    "    phase_i = row_slab_i.phase\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"phase\"] = phase_i\n",
    "    data_dict_i[\"stoich\"] = stoich_i\n",
    "    # #####################################################\n",
    "    data_dict_i.update(index_dict_i)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df_extra_data = pd.DataFrame(data_dict_list)\n",
    "df_extra_data = df_extra_data.set_index(\n",
    "    [\"compenv\", \"slab_id\", \"active_site\", ], drop=True)\n",
    "\n",
    "new_columns = []\n",
    "for col_i in df_extra_data.columns:\n",
    "    new_columns.append(\n",
    "        (\"data\", col_i, \"\")\n",
    "        )\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns)\n",
    "df_extra_data.columns = idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating features data by looping over `df_ads`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_ads.shape[0]:\", df_ads.shape[0])\n",
    "\n",
    "# #########################################################\n",
    "o_rows_list = []\n",
    "o_index_list = []\n",
    "# #########################################################\n",
    "oh_rows_list = []\n",
    "oh_index_list = []\n",
    "# #########################################################\n",
    "failed_indices_oh = []\n",
    "for index_i, row_i in df_ads.iterrows():\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(list(df_ads.index.names), index_i))\n",
    "    # #####################################################\n",
    "    job_id_o_i = row_i.job_id_o\n",
    "    job_id_oh_i = row_i.job_id_oh\n",
    "    job_id_bare_i = row_i.job_id_bare\n",
    "    # #####################################################\n",
    "\n",
    "    \n",
    "\n",
    "    # #####################################################\n",
    "    ads_i = \"o\"\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    df_feat_i = df_features.loc[idx[\n",
    "        index_dict_i[\"compenv\"],\n",
    "        index_dict_i[\"slab_id\"],\n",
    "        ads_i,\n",
    "        index_dict_i[\"active_site\"],\n",
    "        :], :]\n",
    "\n",
    "    row_feat_i = df_feat_i[df_feat_i.data.job_id_max == job_id_o_i]\n",
    "    mess_i = \"There should only be one row after the previous filtering\"\n",
    "    assert row_feat_i.shape[0] == 1, mess_i\n",
    "    row_feat_i = row_feat_i.iloc[0]\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    o_rows_list.append(row_feat_i)\n",
    "    o_index_list.append(row_feat_i.name)\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    ads_i = \"oh\"\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    df_feat_i = df_features.loc[idx[\n",
    "        index_dict_i[\"compenv\"],\n",
    "        index_dict_i[\"slab_id\"],\n",
    "        ads_i,\n",
    "        index_dict_i[\"active_site\"],\n",
    "        :], :]\n",
    "\n",
    "    if df_feat_i.shape[0] > 0:\n",
    "        row_feat_i = df_feat_i[df_feat_i.data.job_id_max == job_id_oh_i]\n",
    "\n",
    "        if row_feat_i.shape[0] > 0:\n",
    "            mess_i = \"There should only be one row after the previous filtering\"\n",
    "            assert row_feat_i.shape[0] == 1, mess_i\n",
    "            row_feat_i = row_feat_i.iloc[0]\n",
    "\n",
    "\n",
    "            # #############################################\n",
    "            oh_rows_list.append(row_feat_i)\n",
    "            oh_index_list.append(row_feat_i.name)\n",
    "        else:\n",
    "            # failed_indices_oh.append(index_i)\n",
    "            failed_indices_oh.append(job_id_oh_i)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "idx = pd.MultiIndex.from_tuples(o_index_list, names=df_features.index.names)\n",
    "df_o = pd.DataFrame(o_rows_list, idx)\n",
    "df_o.index = df_o.index.droplevel(level=[2, 4, ])\n",
    "# #########################################################\n",
    "idx = pd.MultiIndex.from_tuples(oh_index_list, names=df_features.index.names)\n",
    "df_oh = pd.DataFrame(oh_rows_list, idx)\n",
    "df_oh.index = df_oh.index.droplevel(level=[2, 4, ])\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking failed_indices_oh against systems that couldn't be processed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_atoms_sorted_ind\n",
    "\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_atoms_sorted_ind_i = df_atoms_sorted_ind[\n",
    "    df_atoms_sorted_ind.job_id.isin(failed_indices_oh)\n",
    "    ]\n",
    "\n",
    "df_tmp_8 = df_atoms_sorted_ind_i[df_atoms_sorted_ind_i.failed_to_sort == False]\n",
    "\n",
    "if df_tmp_8.shape[0] > 0:\n",
    "    print(\"Check out df_tmp_8, there where some *OH rows that weren't processed but maybe should be\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and combining feature data columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_methods import tmp_combine_dfs_with_same_cols_2\n",
    "from local_methods import combine_dfs_with_same_cols\n",
    "\n",
    "df_dict_i = {\n",
    "    \"oh\": df_oh[[\"data\"]],\n",
    "    \"o\": df_o[[\"data\"]],\n",
    "    }\n",
    "\n",
    "df_data_comb = combine_dfs_with_same_cols(\n",
    "    df_dict=df_dict_i,\n",
    "    verbose=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Adding another empty level to column index\n",
    "new_cols = []\n",
    "for col_i in df_data_comb.columns:\n",
    "    # new_col_i = (\"\", col_i[0], col_i[1])\n",
    "    new_col_i = (col_i[0], col_i[1], \"\", )\n",
    "    new_cols.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_data_comb.columns = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `df_features_comb` and adding another column level for ads"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "ads_i = \"o\"\n",
    "\n",
    "df_features_o = df_o[[\"features\"]]\n",
    "columns_i = df_features_o.columns\n",
    "\n",
    "new_columns_i = []\n",
    "for col_i in columns_i:\n",
    "    new_col_i = (col_i[0], ads_i, col_i[1])\n",
    "    new_columns_i.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns_i)\n",
    "df_features_o.columns = idx\n",
    "\n",
    "# #########################################################\n",
    "ads_i = \"oh\"\n",
    "\n",
    "df_features_oh = df_oh[[\"features\"]]\n",
    "columns_i = df_features_oh.columns\n",
    "\n",
    "new_columns_i = []\n",
    "for col_i in columns_i:\n",
    "    new_col_i = (col_i[0], ads_i, col_i[1])\n",
    "    new_columns_i.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns_i)\n",
    "df_features_oh.columns = idx\n",
    "\n",
    "# #########################################################\n",
    "\n",
    "df_features_comb = pd.concat([\n",
    "    df_features_o,\n",
    "    df_features_oh,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more levels to df_ads to combine\n",
    "\n",
    "new_cols = []\n",
    "for col_i in df_ads.columns:\n",
    "    # new_col_i = (\"\", \"\", col_i)\n",
    "    new_col_i = (col_i, \"\", \"\", )\n",
    "    new_cols.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_ads.columns = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all dataframes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = pd.concat([\n",
    "    df_features_comb,\n",
    "    df_data_comb,\n",
    "    df_ads,\n",
    "    df_extra_data,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing features data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_all_comb[[\"features\"]]\n",
    "\n",
    "df_features_stan = copy.deepcopy(df_features)\n",
    "\n",
    "for col_i in df_features_stan.columns:\n",
    "    mean_val = df_features_stan[col_i].mean()\n",
    "    std_val = df_features_stan[col_i].std()\n",
    "    df_features_stan[col_i] = (df_features_stan[col_i] - mean_val) / std_val\n",
    "\n",
    "\n",
    "new_columns = []\n",
    "for col_i in df_features_stan.columns:\n",
    "    new_col_i = (col_i[0] + \"_stan\", col_i[1], col_i[2])\n",
    "    new_columns.append(new_col_i)\n",
    "df_features_stan.columns = pd.MultiIndex.from_tuples(new_columns)\n",
    "\n",
    "df_all_comb = pd.concat([\n",
    "    df_all_comb,\n",
    "    df_features_stan,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `name_str` column"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    # #########################################################\n",
    "    name_i = row_i.name\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #########################################################\n",
    "    \n",
    "    name_i = compenv_i + \"__\" + slab_id_i + \"__\" + str(int(active_site_i)).zfill(3)\n",
    "\n",
    "    return(name_i)\n",
    "\n",
    "df_all_comb[\"data\", \"name_str\", \"\"] = df_all_comb.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads_columns = [i[0] for i in df_ads.columns.tolist()]\n",
    "\n",
    "for i in target_cols:\n",
    "    df_ads_columns.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns_all = [i[0] for i in df_all_comb[\"data\"].columns]\n",
    "\n",
    "df_ads_columns_to_add = []\n",
    "df_ads_columns_to_drop = []\n",
    "for col_i in df_ads_columns:\n",
    "    if col_i not in data_columns_all:\n",
    "        df_ads_columns_to_add.append(col_i)\n",
    "    else:\n",
    "        df_ads_columns_to_drop.append(col_i)\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "for col_i in df_ads_columns_to_drop:\n",
    "    df_all_comb.drop(columns=(col_i, \"\", \"\"), inplace=True)\n",
    "\n",
    "# #########################################################\n",
    "new_columns = []\n",
    "for col_i in df_all_comb.columns:\n",
    "    if col_i[0] in df_ads_columns_to_add:\n",
    "        new_columns.append(\n",
    "            (\"data\", col_i[0], \"\", )\n",
    "            )\n",
    "    elif col_i[0] in target_cols:\n",
    "        new_columns.append(\n",
    "            (\"targets\", col_i[0], \"\", )\n",
    "            )\n",
    "    else:\n",
    "        new_columns.append(col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns)\n",
    "df_all_comb.columns = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding magmom comparison data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_magmoms_comp_i(df_magmoms_comp_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def method(row_i):\n",
    "        new_column_values_dict = dict(\n",
    "            job_id_0=None,\n",
    "            job_id_1=None,\n",
    "            job_id_2=None,\n",
    "            )\n",
    "\n",
    "        job_ids_tri = row_i.job_ids_tri\n",
    "\n",
    "        ids_sorted = list(np.sort(list(job_ids_tri)))\n",
    "\n",
    "        job_id_0 = ids_sorted[0]\n",
    "        job_id_1 = ids_sorted[1]\n",
    "        job_id_2 = ids_sorted[2]\n",
    "\n",
    "        new_column_values_dict[\"job_id_0\"] = job_id_0\n",
    "        new_column_values_dict[\"job_id_1\"] = job_id_1\n",
    "        new_column_values_dict[\"job_id_2\"] = job_id_2\n",
    "\n",
    "        for key, value in new_column_values_dict.items():\n",
    "            row_i[key] = value\n",
    "        return(row_i)\n",
    "\n",
    "    df_magmoms_comp_i = df_magmoms_comp_i.apply(method, axis=1)\n",
    "    df_magmoms_comp_i = df_magmoms_comp_i.set_index([\"job_id_0\", \"job_id_1\", \"job_id_2\", ])\n",
    "\n",
    "    return(df_magmoms_comp_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_magmoms, read_magmom_comp_data\n",
    "\n",
    "df_magmoms = get_df_magmoms()\n",
    "\n",
    "\n",
    "data_dict_list = []\n",
    "for name_i, row_i in df_all_comb.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(df_all_comb.index.names, name_i))\n",
    "    # #####################################################\n",
    "\n",
    "    magmom_data_i = read_magmom_comp_data(name=name_i)\n",
    "    if magmom_data_i is not None:\n",
    "        df_magmoms_comp_i = magmom_data_i[\"df_magmoms_comp\"]\n",
    "        df_magmoms_comp_i = process_df_magmoms_comp_i(df_magmoms_comp_i)\n",
    "\n",
    "        # tmp = df_magmoms_comp_i.sum_norm_abs_magmom_diff.min()\n",
    "        # tmp_list.append(tmp)\n",
    "\n",
    "        job_ids = []\n",
    "        for ads_j in [\"o\", \"oh\", \"bare\", ]:\n",
    "            job_id_j = row_i[\"data\"][\"job_id_\" + ads_j][\"\"]\n",
    "            if job_id_j is not None:\n",
    "                job_ids.append(job_id_j)\n",
    "\n",
    "\n",
    "        sum_norm_abs_magmom_diff_i = None\n",
    "        if len(job_ids) == 3:\n",
    "            job_ids = list(np.sort(job_ids))\n",
    "            job_id_0 = job_ids[0]\n",
    "            job_id_1 = job_ids[1]\n",
    "            job_id_2 = job_ids[2]\n",
    "\n",
    "            row_mags_i = df_magmoms_comp_i.loc[\n",
    "                (job_id_0, job_id_1, job_id_2, )\n",
    "                ]\n",
    "            sum_norm_abs_magmom_diff_i = row_mags_i.sum_norm_abs_magmom_diff\n",
    "            norm_sum_norm_abs_magmom_diff_i = sum_norm_abs_magmom_diff_i / 3\n",
    "            \n",
    "        # #################################################\n",
    "        data_dict_i.update(index_dict_i)\n",
    "        # #################################################\n",
    "        data_dict_i[\"sum_norm_abs_magmom_diff\"] = sum_norm_abs_magmom_diff_i\n",
    "        data_dict_i[\"norm_sum_norm_abs_magmom_diff\"] = norm_sum_norm_abs_magmom_diff_i\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "# #########################################################\n",
    "df_tmp = pd.DataFrame(data_dict_list)\n",
    "df_tmp = df_tmp.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "\n",
    "# #########################################################\n",
    "new_cols = []\n",
    "for col_i in df_tmp.columns:\n",
    "    new_col_i = (\"data\", col_i, \"\")\n",
    "    new_cols.append(new_col_i)\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_tmp.columns = idx\n",
    "\n",
    "df_all_comb = pd.concat([df_all_comb, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding plot format properties"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj_data import stoich_color_dict\n",
    "\n",
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "# for index_i, row_i in df_features_targets.iterrows():\n",
    "for index_i, row_i in df_all_comb.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(list(df_all_comb.index.names), index_i))\n",
    "    # #####################################################\n",
    "    row_data_i = row_i[\"data\"]\n",
    "    # #####################################################\n",
    "    stoich_i = row_data_i[\"stoich\"][\"\"]\n",
    "    norm_sum_norm_abs_magmom_diff_i = \\\n",
    "        row_data_i[\"norm_sum_norm_abs_magmom_diff\"][\"\"]\n",
    "    # #####################################################\n",
    "\n",
    "    if stoich_i == \"AB2\":\n",
    "        color__stoich_i = stoich_color_dict[\"AB2\"]\n",
    "    elif stoich_i == \"AB3\":\n",
    "        color__stoich_i = stoich_color_dict[\"AB3\"]\n",
    "    else:\n",
    "        color__stoich_i = stoich_color_dict[\"None\"]\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[(\"format\", \"color\", \"stoich\")] = color__stoich_i\n",
    "    data_dict_i[(\"format\", \"color\", \"norm_sum_norm_abs_magmom_diff\")] = \\\n",
    "        norm_sum_norm_abs_magmom_diff_i\n",
    "    # #####################################################\n",
    "    data_dict_i.update(index_dict_i)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_format = pd.DataFrame(data_dict_list)\n",
    "df_format = df_format.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "df_format.columns = pd.MultiIndex.from_tuples(df_format.columns)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = pd.concat(\n",
    "    [\n",
    "        df_all_comb,\n",
    "        df_format,\n",
    "        ],\n",
    "    axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing multiindex to get order columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = df_all_comb.reindex(columns=[\n",
    "    'targets',\n",
    "    'data',\n",
    "    'format',\n",
    "    'features',\n",
    "    'features_stan',\n",
    "    ], level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that aren't supposed to be processed (bad slabs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_slabs_to_run\n",
    "df_slabs_to_run = get_df_slabs_to_run()\n",
    "df_slabs_to_not_run = df_slabs_to_run[df_slabs_to_run.status == \"bad\"]\n",
    "\n",
    "slab_ids_to_not_include = df_slabs_to_not_run.slab_id.tolist()\n",
    "\n",
    "df_index = df_all_comb.index.to_frame()\n",
    "df_all_comb = df_all_comb.loc[\n",
    "    ~df_index.slab_id.isin(slab_ids_to_not_include)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many NaN rows there are for each feature"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting rid of NERSC jobs and phase 1 systems\")\n",
    "\n",
    "indices_to_keep = []\n",
    "for i in df_all_comb.index:\n",
    "    if i[0] != \"nersc\":\n",
    "        indices_to_keep.append(i)\n",
    "\n",
    "df_all_comb = df_all_comb.loc[\n",
    "    indices_to_keep\n",
    "    ]\n",
    "\n",
    "df_all_comb = df_all_comb[df_all_comb[\"data\"][\"phase\"] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_i in df_all_comb.features.columns:\n",
    "    if verbose:\n",
    "        df_tmp_i = df_all_comb[df_all_comb[\"features\"][col_i].isna()]\n",
    "        print(col_i, \":\", df_tmp_i.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_targets = df_all_comb\n",
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/feature_engineering\",\n",
    "    \"out_data\")\n",
    "file_name_i = \"df_features_targets.pickle\"\n",
    "path_i = os.path.join(directory, file_name_i)\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_features_targets, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_features_targets\n",
    "\n",
    "df_features_targets_tmp = get_df_features_targets()\n",
    "df_features_targets_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"combine_features_targets.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
