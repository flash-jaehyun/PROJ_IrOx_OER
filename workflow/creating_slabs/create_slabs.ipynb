{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from IrOx polymorph dataset\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/workflow/creating_slabs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# #########################################################\n",
    "from catkit.gen.surface import SlabGenerator\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import drop_columns\n",
    "from misc_modules.misc_methods import GetFriendlyID\n",
    "from ase_modules.ase_methods import view_in_vesta\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_dft, symmetrize_atoms,\n",
    "    get_structure_coord_df, remove_atoms)\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import analyse_local_coord_env, check_if_sys_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance from top z-coord of slab that we'll remove atoms from\n",
    "dz = 4\n",
    "\n",
    "facets = [\n",
    "    (1, 0, 0),\n",
    "    # (0, 1, 0),\n",
    "    # (0, 0, 1),\n",
    "\n",
    "    # Weird cuts\n",
    "    (2, 1, 4)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "# DFT dataframe\n",
    "df_dft = get_df_dft()\n",
    "\n",
    "# #########################################################\n",
    "# Previous df_slab dataframe\n",
    "path_i = os.path.join(\n",
    "    \"out_data\",\n",
    "    \"df_slab.pickle\")\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    print(\"File exists!\")\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_slab_old = pickle.load(fle)\n",
    "else:\n",
    "    df_slab_old = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data/final_slabs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/slab_progression\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only taking the most stable polymorphs for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dft_ab2_i = df_dft[df_dft.stoich == \"AB2\"].sort_values(\"dH\").iloc[0:10]\n",
    "# df_dft_ab3_i = df_dft[df_dft.stoich == \"AB3\"].sort_values(\"dH\").iloc[0:10]\n",
    "\n",
    "df_dft_ab2_i = df_dft[df_dft.stoich == \"AB2\"].sort_values(\"dH\").iloc[0:3]\n",
    "df_dft_ab3_i = df_dft[df_dft.stoich == \"AB3\"].sort_values(\"dH\").iloc[0:2]\n",
    "\n",
    "df_i = pd.concat([\n",
    "    df_dft_ab2_i,\n",
    "    df_dft_ab3_i,\n",
    "    ], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from bulks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.36s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dict_list = []\n",
    "iterator = tqdm(df_i.index.tolist())\n",
    "for i_cnt, bulk_id in enumerate(iterator):\n",
    "    row_i = df_dft.loc[bulk_id]\n",
    "\n",
    "    # #####################################################\n",
    "    # Row parameters ######################################\n",
    "    bulk_id_i = row_i.name\n",
    "    atoms = row_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    for facet in facets:\n",
    "        data_dict_i = dict()\n",
    "\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "    \n",
    "        facet_str = \"\".join([str(i) for i in list(facet)])\n",
    "        data_dict_i[\"facet\"] = facet_str\n",
    "\n",
    "        sys_processed = check_if_sys_processed(\n",
    "            bulk_id_i=bulk_id_i,\n",
    "            facet_str=facet_str,\n",
    "            df_slab_old=df_slab_old)\n",
    "\n",
    "        # Only run if not in df_slab_old (already run)\n",
    "        if not sys_processed:\n",
    "            slab_id_i = GetFriendlyID(append_random_num=True)\n",
    "            data_dict_i[\"slab_id\"] = slab_id_i\n",
    "\n",
    "            SG = SlabGenerator(\n",
    "                atoms, facet, 10, vacuum=15,\n",
    "                fixed=None, layer_type='ang',\n",
    "                attach_graph=True,\n",
    "                standardize_bulk=True,\n",
    "                primitive=True, tol=1e-08)\n",
    "\n",
    "            slab_i = SG.get_slab()\n",
    "            slab_i.set_pbc([True, True, True])\n",
    "\n",
    "            # #############################################\n",
    "            data_dict_list.append(data_dict_i)\n",
    "\n",
    "            # #############################################\n",
    "            # Write slab to file ##########################\n",
    "            file_name_i = bulk_id_i + \"_\" + slab_id_i + \"_\" + facet_str + \"_0\" + \".cif\"\n",
    "            slab_i.write(os.path.join(\n",
    "                \"out_data\",\n",
    "                \"slab_progression\",\n",
    "                file_name_i))\n",
    "\n",
    "            # Rereading the structure file to get it back into ase format (instead of CatKit class)\n",
    "            slab_i = io.read(os.path.join(\n",
    "                \"out_data\",\n",
    "                \"slab_progression\",\n",
    "                file_name_i))\n",
    "            data_dict_i[\"slab_0\"] = slab_i\n",
    "            # #############################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_slab = pd.DataFrame(data_dict_list)\n",
    "num_new_rows = len(data_dict_list)\n",
    "\n",
    "if num_new_rows > 0:\n",
    "    df_slab = df_slab.set_index(\"slab_id\")\n",
    "elif num_new_rows == 0:\n",
    "    print(\"There aren't any new rows to process\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing surface Ir atoms that weren't oxygen saturated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:10<00:00, 26.05s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "iterator = tqdm(df_slab.index.tolist())\n",
    "for i_cnt, slab_id in enumerate(iterator):\n",
    "    row_i = df_slab.loc[slab_id]\n",
    "\n",
    "    # #####################################################\n",
    "    slab_id_i = row_i.name\n",
    "    bulk_id_i = row_i.bulk_id\n",
    "    facet_i = row_i.facet\n",
    "    slab = row_i.slab_0\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_bulk_i = df_dft.loc[bulk_id_i]\n",
    "\n",
    "    bulk = row_bulk_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    sys_processed = check_if_sys_processed(\n",
    "        bulk_id_i=bulk_id_i,\n",
    "        facet_str=facet_i,\n",
    "        df_slab_old=df_slab_old)\n",
    "\n",
    "    # Only run if not in df_slab_old (already run)\n",
    "    if not sys_processed:\n",
    "\n",
    "        # out_data_dict = analyse_local_coord_env(atoms=bulk)\n",
    "        # coor_env_dict_bulk = out_data_dict[\"coor_env_dict\"]\n",
    "\n",
    "\n",
    "        # #################################################\n",
    "        # #################################################\n",
    "        z_positions = slab.positions[:,2]\n",
    "\n",
    "        z_max = np.max(z_positions)\n",
    "        z_min = np.min(z_positions)\n",
    "\n",
    "        # #################################################\n",
    "        # #################################################\n",
    "        df_coord_slab_i = get_structure_coord_df(slab)\n",
    "\n",
    "        # #################################################\n",
    "        metal_atoms_to_remove = []\n",
    "        for atom in slab:\n",
    "            if atom.symbol == metal_atom_symbol:\n",
    "                z_pos_i = atom.position[2]\n",
    "                if z_pos_i >= z_max - dz or z_pos_i <= z_min + dz:\n",
    "                    row_coord = df_coord_slab_i[\n",
    "                        df_coord_slab_i.structure_index == atom.index].iloc[0]\n",
    "                    num_o_neighbors = row_coord.neighbor_count[\"O\"]\n",
    "\n",
    "                    if num_o_neighbors < 6:\n",
    "                        metal_atoms_to_remove.append(atom.index)\n",
    "\n",
    "        slab_new = remove_atoms(atoms=slab, atoms_to_remove=metal_atoms_to_remove)\n",
    "\n",
    "        # #################################################\n",
    "        # Write slab to file ##############################\n",
    "        file_name_i = bulk_id_i + \"_\" + slab_id_i + \"_\" + facet_i + \"_1\" + \".cif\"\n",
    "        slab_new.write(os.path.join(\n",
    "            \"out_data\",\n",
    "            \"slab_progression\",\n",
    "            file_name_i))\n",
    "\n",
    "        data_dict[slab_id_i] = slab_new\n",
    "\n",
    "# #########################################################\n",
    "df_slab[\"slab_1\"] = df_slab.index.map(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing extra oxygens at surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:36<00:00, 19.34s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "# for i_cnt, row_i in df_slab.iterrows():\n",
    "iterator = tqdm(df_slab.index.tolist())\n",
    "for i_cnt, slab_id in enumerate(iterator):\n",
    "\n",
    "    # row_i = df_slab[df_slab.bulk_id == \"8p8evt9pcg\"]\n",
    "    # row_i = row_i.iloc[0]\n",
    "    row_i = df_slab.loc[slab_id]\n",
    "\n",
    "    # #####################################################\n",
    "    slab_id_i = row_i.name\n",
    "    bulk_id_i = row_i.bulk_id\n",
    "\n",
    "    facet_i = row_i.facet\n",
    "\n",
    "    slab = row_i.slab_1\n",
    "    # #####################################################\n",
    "    \n",
    "    sys_processed = check_if_sys_processed(\n",
    "        bulk_id_i=bulk_id_i,\n",
    "        facet_str=facet_i,\n",
    "        df_slab_old=df_slab_old)\n",
    "\n",
    "    # Only run if not in df_slab_old (already run)\n",
    "    if not sys_processed:\n",
    "\n",
    "        df_coord_slab_i = get_structure_coord_df(slab)\n",
    "\n",
    "        df_i = df_coord_slab_i[df_coord_slab_i.element == \"O\"]\n",
    "        df_i = df_i[df_i.num_neighbors == 0]\n",
    "\n",
    "        o_atoms_to_remove = df_i.structure_index.tolist()\n",
    "\n",
    "        slab_new = remove_atoms(slab, atoms_to_remove=o_atoms_to_remove)\n",
    "\n",
    "        # #####################################################\n",
    "        # Write slab to file ##################################\n",
    "        file_name_i = bulk_id_i + \"_\" + slab_id_i + \"_\" + facet_i + \"_2\" + \".cif\"\n",
    "        slab_new.write(os.path.join(\n",
    "            \"out_data\",\n",
    "            \"slab_progression\",\n",
    "            file_name_i))\n",
    "\n",
    "        data_dict[slab_id_i] = slab_new\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_slab[\"slab_2\"] = df_slab.index.map(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined new and old `df_slab` dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab = pd.concat([\n",
    "    df_slab_old,\n",
    "    df_slab,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write final slab structures and `df_slabs` to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    slab = row_i.slab_2\n",
    "    bulk_id_i = row_i.bulk_id\n",
    "    slab_id_i = row_i.name\n",
    "    facet_i = row_i.facet\n",
    "\n",
    "    file_name_i = bulk_id_i + \"_\" + slab_id_i + \"_\" + facet_i + \"_final\" + \".cif\"\n",
    "    slab.write(os.path.join(\n",
    "        \"out_data/final_slabs\",\n",
    "        file_name_i))\n",
    "\n",
    "tmp = df_slab.apply(\n",
    "    method,\n",
    "    axis=1)\n",
    "\n",
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"df_slab.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_slab, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data_dict_list = []\n",
    "# for i_cnt, row_i in df_i.iterrows():\n",
    "#     # row_i = df_i.iloc[3]\n",
    "\n",
    "#     # #####################################################\n",
    "#     # Row parameters ######################################\n",
    "#     id_i = row_i.name\n",
    "#     atoms = row_i.atoms\n",
    "#     # #####################################################\n",
    "\n",
    "#     for facet in facets:\n",
    "#         data_dict_i = dict()\n",
    "\n",
    "#         data_dict_i[\"bulk_id\"] = id_i\n",
    "    \n",
    "#         facet_str = \"\".join([str(i) for i in list(facet)])\n",
    "#         data_dict_i[\"facet\"] = facet_str\n",
    "\n",
    "#         slab_id_i = GetFriendlyID(append_random_num=True)\n",
    "#         data_dict_i[\"slab_id\"] = slab_id_i\n",
    "\n",
    "#         SG = SlabGenerator(\n",
    "#             atoms,\n",
    "#             facet,\n",
    "#             10,\n",
    "#             vacuum=15,\n",
    "#             fixed=None,\n",
    "#             layer_type='ang',\n",
    "#             attach_graph=True,\n",
    "#             standardize_bulk=True,\n",
    "#             primitive=True,\n",
    "#             tol=1e-08,\n",
    "#             )\n",
    "\n",
    "#         slab_i = SG.get_slab()\n",
    "#         slab_i.set_pbc([True, True, True])\n",
    "\n",
    "#         # #################################################\n",
    "#         data_dict_list.append(data_dict_i)\n",
    "\n",
    "#         # #################################################\n",
    "#         # Write slab to file ##############################        \n",
    "#         file_name_i = id_i + \"_\" + slab_id_i + \"_\" + facet_str + \"_0\" + \".cif\"\n",
    "#         slab_i.write(os.path.join(\n",
    "#             \"out_data\",\n",
    "#             file_name_i))\n",
    "\n",
    "#         # Rereading the structure file to get it back into ase format (instead of CatKit class)\n",
    "#         slab_i = io.read(os.path.join(\n",
    "#             \"out_data\",\n",
    "#             file_name_i))\n",
    "#         data_dict_i[\"slab_0\"] = slab_i\n",
    "#         # #################################################\n",
    "\n",
    "\n",
    "# # #########################################################\n",
    "# df_slab = pd.DataFrame(data_dict_list)\n",
    "# df_slab = df_slab.set_index(\"slab_id\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
