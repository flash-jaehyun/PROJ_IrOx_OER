{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from IrOx polymorph dataset\n",
    "---\n",
    "\n",
    "This notebook is time consuming, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/workflow/creating_slabs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import signal\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# #########################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import drop_columns\n",
    "from misc_modules.misc_methods import GetFriendlyID\n",
    "from ase_modules.ase_methods import view_in_vesta\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_dft, symmetrize_atoms,\n",
    "    get_structure_coord_df, remove_atoms)\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    analyse_local_coord_env, check_if_sys_processed,\n",
    "    remove_nonsaturated_surface_metal_atoms,\n",
    "    remove_noncoord_oxygens,\n",
    "    create_slab_from_bulk,\n",
    "    create_final_slab_master,\n",
    "    create_save_dataframe,\n",
    "    constrain_slab,\n",
    "    read_data_json,\n",
    "    calc_surface_area,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timelimit_seconds = 0.4 * 60\n",
    "timelimit_seconds = 10 * 60\n",
    "\n",
    "max_surf_a = 200\n",
    "\n",
    "# Distance from top z-coord of slab that we'll remove atoms from\n",
    "dz = 4\n",
    "\n",
    "facets = [\n",
    "    (1, 0, 0),\n",
    "    (0, 1, 0),\n",
    "    (0, 0, 1),\n",
    "\n",
    "    (1, 1, 1),\n",
    "\n",
    "    (0, 1, 1),\n",
    "    (1, 0, 1),\n",
    "    (1, 1, 0),\n",
    "\n",
    "    # (2, 0, 0),\n",
    "    # (0, 2, 0),\n",
    "    # (0, 0, 2),\n",
    "\n",
    "    # Weird cuts\n",
    "    (3, 1, 4),\n",
    "    # (2, 1, 4),\n",
    "    # (7, 1, 4),\n",
    "    ]\n",
    "\n",
    "facets = [t for t in (set(tuple(i) for i in facets))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_slab_old.shape: (376, 5)\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "# DFT dataframe\n",
    "df_dft = get_df_dft()\n",
    "\n",
    "# #########################################################\n",
    "# Previous df_slab dataframe\n",
    "path_i = os.path.join(\n",
    "    \"out_data\",\n",
    "    \"df_slab.pickle\")\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_slab_old = pickle.load(fle)\n",
    "else:\n",
    "    df_slab_old = pd.DataFrame()\n",
    "\n",
    "print(\"df_slab_old.shape:\", df_slab_old.shape)\n",
    "\n",
    "# #######################################################################\n",
    "# Bulks not to run, manually checked to be erroneous/bad\n",
    "data_path = os.path.join(\n",
    "    \"in_data/bulks_to_not_run.json\")\n",
    "with open(data_path, \"r\") as fle:\n",
    "    bulks_to_not_run = json.load(fle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create needed folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data/final_slabs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/slab_progression\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/df_coord_files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "data_path = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/creating_slabs/selecting_bulks\",\n",
    "    \"out_data/data.json\")\n",
    "with open(data_path, \"r\") as fle:\n",
    "    data = json.load(fle)\n",
    "# ########################################################\n",
    "\n",
    "bulk_ids__octa_unique = data[\"bulk_ids__octa_unique\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_dft.loc[bulk_ids__octa_unique]\n",
    "\n",
    "# Drop ids that were manually identified as bad\n",
    "ids_i = df_i.index.intersection(bulks_to_not_run)\n",
    "df_i = df_i.drop(labels=ids_i)\n",
    "\n",
    "df_i = df_i.sample(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from bulks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out which systems haven't been run yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_json()\n",
    "\n",
    "systems_that_took_too_long = data.get(\"systems_that_took_too_long\", []) \n",
    "\n",
    "systems_that_took_too_long_2 = []\n",
    "for i in systems_that_took_too_long:\n",
    "    systems_that_took_too_long_2.append(i[0] + \"_\" + i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_not_processed = []\n",
    "iterator = df_i.index.tolist()\n",
    "for i_cnt, bulk_id in enumerate(iterator):\n",
    "    row_i = df_dft.loc[bulk_id]\n",
    "\n",
    "    # #####################################################\n",
    "    # Row parameters ######################################\n",
    "    bulk_id_i = row_i.name\n",
    "    atoms = row_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    for facet in facets:\n",
    "        data_dict_i = dict()\n",
    "\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "    \n",
    "        facet_i = \"\".join([str(i) for i in list(facet)])\n",
    "        data_dict_i[\"facet\"] = facet_i\n",
    "\n",
    "        sys_processed = check_if_sys_processed(\n",
    "            bulk_id_i=bulk_id_i,\n",
    "            facet_str=facet_i,\n",
    "            df_slab_old=df_slab_old)\n",
    "\n",
    "        if not sys_processed:\n",
    "            id_comb = bulk_id + \"_\" + facet_i\n",
    "            if id_comb not in systems_that_took_too_long_2:\n",
    "                systems_not_processed.append(dict(bulk_id=bulk_id, facet=facet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom exception for the timeout\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "# Handler function to be called when SIGALRM is received\n",
    "def sigalrm_handler(signum, frame):\n",
    "    # We get signal!\n",
    "    raise TimeoutException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e865d5e9f5524053a79e57106cbe0e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='1st loop', max=96.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk_id_i: zhvh759485 facet 111\n"
     ]
    }
   ],
   "source": [
    "df_coord_dict = dict()\n",
    "iterator = tqdm(systems_not_processed, desc=\"1st loop\")\n",
    "for i_cnt, sys_i in enumerate(iterator):\n",
    "\n",
    "    # #####################################################\n",
    "    # Set up signal handler for SIGALRM, saving previous value\n",
    "    old_handler = signal.signal(signal.SIGALRM, sigalrm_handler)\n",
    "    # Start timer\n",
    "    signal.alarm(int(timelimit_seconds))\n",
    "    # #####################################################\n",
    "\n",
    "    data_dict_i = dict()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # #####################################################\n",
    "    bulk_id_i = sys_i[\"bulk_id\"]\n",
    "    data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "\n",
    "    facet = sys_i[\"facet\"]\n",
    "    facet_i = \"\".join([str(i) for i in list(facet)])\n",
    "    data_dict_i[\"facet\"] = facet_i\n",
    "    # #####################################################\n",
    "    row_i = df_dft.loc[bulk_id_i]\n",
    "    atoms = row_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    slab_id_i = GetFriendlyID(append_random_num=True)\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "\n",
    "    print(\"bulk_id_i:\", bulk_id_i, \"facet\", facet_i, end=\"\\r\", flush=True)\n",
    "\n",
    "    surf_a = calc_surface_area(atoms=atoms)\n",
    "    if surf_a > max_surf_a:\n",
    "        data_dict_i[\"status\"] = \"Too large of surface area\"\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            # slab_final = create_slab_from_bulk(\n",
    "            slab_0 = create_slab_from_bulk(\n",
    "                atoms=atoms, facet=facet)\n",
    "\n",
    "            slab_1 = create_final_slab_master(atoms=slab_0)\n",
    "\n",
    "            slab_2 = constrain_slab(atoms=slab_1)\n",
    "            slab_final = slab_2\n",
    "\n",
    "            df_coord_slab_final = get_structure_coord_df(slab_final)\n",
    "\n",
    "            # COMBAK\n",
    "            # Pickling data ###########################################\n",
    "            path_i = os.path.join(\n",
    "                \"out_data/df_coord_files\", slab_id_i + \".pickle\")\n",
    "            with open(path_i, \"wb\") as fle:\n",
    "                pickle.dump(df_coord_slab_final, fle)\n",
    "            # #########################################################\n",
    "\n",
    "            df_coord_dict[slab_id_i] = df_coord_slab_final\n",
    "\n",
    "            file_name_i = bulk_id_i + \"_\" + slab_id_i + \\\n",
    "                \"_\" + facet_i + \"_final\" + \".cif\"\n",
    "            slab_final.write(\n",
    "                os.path.join(\"out_data/final_slabs\", file_name_i))\n",
    "\n",
    "            # #####################################################\n",
    "            data_dict_i[\"slab_final\"] = slab_final\n",
    "\n",
    "        except TimeoutException:\n",
    "            data_dict_i[\"status\"] = \"Took too long\"\n",
    "\n",
    "            data = read_data_json()\n",
    "\n",
    "            systems_that_took_too_long = data.get(\"systems_that_took_too_long\", [])\n",
    "            systems_that_took_too_long.append((bulk_id_i, facet_i))\n",
    "\n",
    "            data[\"systems_that_took_too_long\"] = systems_that_took_too_long\n",
    "\n",
    "            data_path = os.path.join(\n",
    "                \"out_data/data.json\")\n",
    "            with open(data_path, \"w\") as fle:\n",
    "                json.dump(data, fle, indent=2)\n",
    "\n",
    "\n",
    "        finally:\n",
    "            # #################################################\n",
    "            signal.alarm(0)\n",
    "            signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "    # #####################################################\n",
    "    iter_time_i = time.time() - t0\n",
    "    data_dict_i[\"iter_time_i\"] = iter_time_i\n",
    "\n",
    "    data_dict_list = []\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "    df_slab_old = create_save_dataframe(\n",
    "        data_dict_list=data_dict_list,\n",
    "        df_slab_old=df_slab_old)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fruits = ['apple', 'banana', 'grape', 'strawberry', 'orange']\n",
    "# for f in fruits:\n",
    "#     # Set up signal handler for SIGALRM, saving previous value\n",
    "#     old_handler = signal.signal(signal.SIGALRM, sigalrm_handler)\n",
    "#     # Start timer\n",
    "#     signal.alarm(timelimit_seconds)\n",
    "\n",
    "#     try:\n",
    "#         mix(f)\n",
    "#         print(f, 'was mixed')\n",
    "#     except TimeoutException:\n",
    "#         print(f, 'took too long to mix')\n",
    "#     finally:\n",
    "#         # Turn off timer\n",
    "#         signal.alarm(0)\n",
    "#         # Restore handler to previous value\n",
    "#         signal.signal(signal.SIGALRM, old_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def read_data_json():\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     #| - read_data_json\n",
    "#     path_i = os.path.join(\n",
    "#         \"out_data\", \"data.json\")\n",
    "#     my_file = Path(path_i)\n",
    "#     if my_file.is_file():\n",
    "#         data_path = os.path.join(\n",
    "#             \"out_data/data.json\")\n",
    "#         with open(data_path, \"r\") as fle:\n",
    "#             data = json.load(fle)\n",
    "#     else:\n",
    "#         data = dict()\n",
    "\n",
    "#     return(data)\n",
    "#     #__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # df_dft_ab2_i = df_dft[df_dft.stoich == \"AB2\"].sort_values(\"dH\").iloc[0:10]\n",
    "# # df_dft_ab3_i = df_dft[df_dft.stoich == \"AB3\"].sort_values(\"dH\").iloc[0:10]\n",
    "\n",
    "# df_dft_ab2_i = df_dft[df_dft.stoich == \"AB2\"].sort_values(\"dH\").iloc[0:100]\n",
    "# df_dft_ab3_i = df_dft[df_dft.stoich == \"AB3\"].sort_values(\"dH\").iloc[0:75]\n",
    "\n",
    "# df_i = pd.concat([\n",
    "#     df_dft_ab2_i.sample(n=20),\n",
    "#     df_dft_ab3_i.sample(n=20),\n",
    "#     ])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
