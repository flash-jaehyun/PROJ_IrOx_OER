{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from IrOx polymorph dataset\n",
    "---\n",
    "\n",
    "This notebook is time consuming. Additional processing of the slab (correct vacuum applied, and bulk constraints, etc.) are done in `process_slabs.ipynb`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {},
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import signal\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import drop_columns\n",
    "from misc_modules.misc_methods import GetFriendlyID\n",
    "from ase_modules.ase_methods import view_in_vesta\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_dft, symmetrize_atoms,\n",
    "    get_structure_coord_df, remove_atoms)\n",
    "from methods import TimeoutException, sigalrm_handler\n",
    "\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    analyse_local_coord_env, check_if_sys_processed,\n",
    "    remove_nonsaturated_surface_metal_atoms,\n",
    "    remove_noncoord_oxygens,\n",
    "    create_slab_from_bulk,\n",
    "    create_final_slab_master,\n",
    "    create_save_dataframe,\n",
    "    constrain_slab,\n",
    "    read_data_json,\n",
    "    calc_surface_area,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timelimit_seconds = 0.4 * 60\n",
    "timelimit_seconds = 10 * 60\n",
    "\n",
    "max_surf_a = 200\n",
    "\n",
    "# Distance from top z-coord of slab that we'll remove atoms from\n",
    "dz = 4\n",
    "\n",
    "facets = [\n",
    "    (1, 0, 0),\n",
    "    (0, 1, 0),\n",
    "    (0, 0, 1),\n",
    "\n",
    "    (1, 1, 1),\n",
    "\n",
    "    (0, 1, 1),\n",
    "    (1, 0, 1),\n",
    "    (1, 1, 0),\n",
    "\n",
    "    # (2, 0, 0),\n",
    "    # (0, 2, 0),\n",
    "    # (0, 0, 2),\n",
    "\n",
    "    # Weird cuts\n",
    "    (3, 1, 4),\n",
    "    (2, 1, 4),\n",
    "\n",
    "    (3, 0, 2),\n",
    "    (3, 0, 3),\n",
    "\n",
    "    (0, 1, 2),\n",
    "    (1, 3, 1),\n",
    "\n",
    "    (3, 3, 1),\n",
    "\n",
    "    ]\n",
    "\n",
    "facets = [t for t in (set(tuple(i) for i in facets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_of_layered_to_include = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# DFT dataframe\n",
    "df_dft = get_df_dft()\n",
    "\n",
    "# #########################################################\n",
    "# Previous df_slab dataframe\n",
    "path_i = os.path.join(\n",
    "    \"out_data\",\n",
    "    \"df_slab.pickle\")\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_slab_old = pickle.load(fle)\n",
    "else:\n",
    "    df_slab_old = pd.DataFrame()\n",
    "\n",
    "print(\"df_slab_old.shape:\", df_slab_old.shape)\n",
    "\n",
    "# #######################################################################\n",
    "# Bulks not to run, manually checked to be erroneous/bad\n",
    "data_path = os.path.join(\n",
    "    \"in_data/bulks_to_not_run.json\")\n",
    "with open(data_path, \"r\") as fle:\n",
    "    bulks_to_not_run = json.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_xrd\n",
    "\n",
    "df_xrd = get_df_xrd()\n",
    "# df_xrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_bulk_manual_class\n",
    "\n",
    "df_bulk_manual_class = get_df_bulk_manual_class()\n",
    "\n",
    "# df_bulk_manual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_bulk_selection_data\n",
    "\n",
    "# #########################################################\n",
    "bulk_selection_data = get_bulk_selection_data()\n",
    "# #########################################################\n",
    "# bulk_selection_data.keys()\n",
    "bulk_ids__octa_unique = bulk_selection_data[\"bulk_ids__octa_unique\"]\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create needed folders"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data/final_slabs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/slab_progression\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/df_coord_files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/temp_out\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of bulk structures that are octahedral and unique:\",\n",
    "    \"\\n\",\n",
    "    len(bulk_ids__octa_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering bulk DFT data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only unique octahedral systems"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dft_i = df_dft.loc[bulk_ids__octa_unique]\n",
    "\n",
    "# Drop ids that were manually identified as bad\n",
    "ids_i = df_dft_i.index.intersection(bulks_to_not_run)\n",
    "df_dft_i = df_dft_i.drop(labels=ids_i)\n",
    "\n",
    "print(\"df_dft_i.shape:\", df_dft_i.shape[0])\n",
    "# df_dft_i = df_dft_i.sample(n=50)\n",
    "# df_dft_i = df_dft_i.sample(n=110)\n",
    "# df_dft_i = df_dft_i.sample(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing all layered bulks and adding in just a bit"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_layered_ids = df_bulk_manual_class[df_bulk_manual_class.layered == False]\n",
    "non_layered_ids = non_layered_ids.index\n",
    "\n",
    "layered_ids = df_bulk_manual_class[df_bulk_manual_class.layered == True]\n",
    "layered_ids = layered_ids.index\n",
    "\n",
    "print(df_dft_i.index.shape[0])\n",
    "print(non_layered_ids.shape[0])\n",
    "\n",
    "non_layered_ids_inter = df_dft_i.index.intersection(non_layered_ids)\n",
    "# df_dft_i = df_dft_i.loc[non_layered_ids_inter]\n",
    "\n",
    "num_non_layered = non_layered_ids_inter.shape[0]\n",
    "\n",
    "num_layered_to_inc = int(frac_of_layered_to_include * num_non_layered)\n",
    "\n",
    "layered_ids_to_use = np.random.choice(\n",
    "    layered_ids, size=num_layered_to_inc, replace=False)\n",
    "\n",
    "print(len(layered_ids_to_use))\n",
    "print(len(non_layered_ids_inter))\n",
    "\n",
    "non_layered_ids_inter.join(layered_ids_to_use).shape\n",
    "\n",
    "all_ids = non_layered_ids_inter.to_list() + list(layered_ids_to_use)\n",
    "\n",
    "# df_dft_i = df_dft_i.loc[all_ids]\n",
    "\n",
    "df_dft_i = df_dft_i.loc[df_dft_i.index.intersection(all_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ids\n",
    "\n",
    "# df_dft_i = \n",
    "# df_dft_i.loc[all_ids]\n",
    "\n",
    "# all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impose `dH-dH_hull` cutoff of 0.5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_dft_ab2 = df_dft_i[df_dft_i[\"stoich\"] == \"AB2\"]\n",
    "\n",
    "min_dH = df_dft_ab2.dH.min()\n",
    "df_dft_ab2.loc[:, \"dH_hull\"] = df_dft_ab2.dH - min_dH\n",
    "\n",
    "# #########################################################\n",
    "df_dft_ab3 = df_dft_i[df_dft_i[\"stoich\"] == \"AB3\"]\n",
    "\n",
    "min_dH = df_dft_ab3.dH.min()\n",
    "df_dft_ab3.loc[:, \"dH_hull\"] = df_dft_ab3.dH - min_dH\n",
    "\n",
    "# #########################################################\n",
    "print(df_dft_ab2.shape)\n",
    "print(df_dft_ab3.shape)\n",
    "\n",
    "df_dft_i = pd.concat([\n",
    "    df_dft_ab2,\n",
    "    df_dft_ab3,\n",
    "    ], )\n",
    "\n",
    "df_dft_i = df_dft_i[df_dft_i.dH_hull < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dft_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP\n",
    "# df_dft_i = df_dft_i.loc[[\"xtbocq9o6p\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from bulks"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which systems previously took too long"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_json()\n",
    "\n",
    "systems_that_took_too_long = data.get(\"systems_that_took_too_long\", []) \n",
    "\n",
    "systems_that_took_too_long_2 = []\n",
    "for i in systems_that_took_too_long:\n",
    "    systems_that_took_too_long_2.append(i[0] + \"_\" + i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out which systems haven't been run yet"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_not_processed = []\n",
    "iterator = df_dft_i.index.tolist()\n",
    "for i_cnt, bulk_id in enumerate(iterator):\n",
    "    row_i = df_dft.loc[bulk_id]\n",
    "\n",
    "    # #####################################################\n",
    "    # Row parameters ######################################\n",
    "    bulk_id_i = row_i.name\n",
    "    atoms = row_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_xrd_i = df_xrd.loc[bulk_id]\n",
    "    # #####################################################\n",
    "    top_facets_i = row_xrd_i.top_facets\n",
    "    # #####################################################\n",
    "\n",
    "    # for facet in facets:\n",
    "    for facet in top_facets_i:\n",
    "        data_dict_i = dict()\n",
    "\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "    \n",
    "        facet_i = \"\".join([str(i) for i in list(facet)])\n",
    "        data_dict_i[\"facet\"] = facet_i\n",
    "\n",
    "        sys_processed = check_if_sys_processed(\n",
    "            bulk_id_i=bulk_id_i,\n",
    "            facet_str=facet_i,\n",
    "            df_slab_old=df_slab_old)\n",
    "\n",
    "        if not sys_processed:\n",
    "            id_comb = bulk_id + \"_\" + facet_i\n",
    "            if id_comb not in systems_that_took_too_long_2:\n",
    "                systems_not_processed.append(dict(bulk_id=bulk_id, facet=facet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systems_not_processed = random.sample(systems_not_processed, 20)\n",
    "\n",
    "# systems_not_processed = random.sample(systems_not_processed, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP | Manually creating bulk_id + facet list to process"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# # TEMP\n",
    "# print(\"TEMP TEMP TEMP\")\n",
    "# systems_not_processed = [\n",
    "#     {\"bulk_id\": \"8l919k6s7p\", \"facet\": (1, 0, 0)},\n",
    "#     {\"bulk_id\": \"8l919k6s7p\", \"facet\": (1, 1, 1)},\n",
    "#     {\"bulk_id\": \"8l919k6s7p\", \"facet\": (1, 0, 1)},\n",
    "#     {\"bulk_id\": \"8l919k6s7p\", \"facet\": (0, 0, 1)},\n",
    "#     {\"bulk_id\": \"8l919k6s7p\", \"facet\": (1, 1, 0)},\n",
    "#     ]\n",
    "\n",
    "# #########################################################\n",
    "# systems_not_processed = [\n",
    "#     {'bulk_id': 'b19q9p6k72', 'facet': (1, 0, 1)},\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP |  Creating `systems_to_process` from already run jobs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from methods import get_df_jobs\n",
    "\n",
    "# df_jobs = get_df_jobs(exclude_wsl_paths=True)\n",
    "\n",
    "# systems_to_process = []\n",
    "# for job_id, row_i in df_jobs[[\"bulk_id\", \"facet\"]].iterrows():\n",
    "#     bulk_id_i = row_i.bulk_id\n",
    "#     facet_i = row_i.facet\n",
    "\n",
    "#     dict_out_i = {\n",
    "#         \"bulk_id\": bulk_id_i,\n",
    "#         \"facet\": facet_i}\n",
    "#     systems_to_process.append(dict_out_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_process = systems_not_processed\n",
    "\n",
    "systems_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating slabs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coord_dict = dict()\n",
    "# iterator = tqdm(systems_not_processed, desc=\"1st loop\")\n",
    "iterator = tqdm(systems_to_process, desc=\"1st loop\")\n",
    "for i_cnt, sys_i in enumerate(iterator):\n",
    "\n",
    "    # #####################################################\n",
    "    # Set up signal handler for SIGALRM, saving previous value\n",
    "    old_handler = signal.signal(signal.SIGALRM, sigalrm_handler)\n",
    "    # Start timer\n",
    "    signal.alarm(int(timelimit_seconds))\n",
    "    # #####################################################\n",
    "\n",
    "    data_dict_i = dict()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # #####################################################\n",
    "    bulk_id_i = sys_i[\"bulk_id\"]\n",
    "    data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "\n",
    "    facet = sys_i[\"facet\"]\n",
    "    facet_i = \"\".join([str(i) for i in list(facet)])\n",
    "    data_dict_i[\"facet\"] = facet_i\n",
    "    # #####################################################\n",
    "    row_i = df_dft.loc[bulk_id_i]\n",
    "    atoms = row_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    # Getting or generating id for slab (slab_id)\n",
    "    from methods import get_df_slab_ids, get_slab_id\n",
    "    df_slab_ids = get_df_slab_ids()\n",
    "    slab_id_i = get_slab_id(bulk_id_i, facet_i, df_slab_ids)\n",
    "    if slab_id_i is None:\n",
    "        slab_id_i = GetFriendlyID(append_random_num=True)\n",
    "\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "\n",
    "    print(\"bulk_id_i:\", bulk_id_i, \"facet\", facet_i, end=\"\\r\", flush=True)\n",
    "\n",
    "    surf_a = calc_surface_area(atoms=atoms)\n",
    "    if surf_a > max_surf_a:\n",
    "        data_dict_i[\"status\"] = \"Too large of surface area\"\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            # slab_final = create_slab_from_bulk(\n",
    "            slab_0 = create_slab_from_bulk(\n",
    "                atoms=atoms, facet=facet)\n",
    "\n",
    "            slab_1 = create_final_slab_master(atoms=slab_0)\n",
    "\n",
    "            slab_2 = constrain_slab(atoms=slab_1)\n",
    "            slab_final = slab_2\n",
    "\n",
    "            df_coord_slab_final = get_structure_coord_df(slab_final)\n",
    "    \n",
    "            # COMBAK\n",
    "            # Pickling data ###########################################\n",
    "            path_i = os.path.join(\n",
    "                \"out_data/df_coord_files\", slab_id_i + \".pickle\")\n",
    "            with open(path_i, \"wb\") as fle:\n",
    "                pickle.dump(df_coord_slab_final, fle)\n",
    "            # #########################################################\n",
    "\n",
    "            df_coord_dict[slab_id_i] = df_coord_slab_final\n",
    "\n",
    "            file_name_i = bulk_id_i + \"_\" + slab_id_i + \\\n",
    "                \"_\" + facet_i + \"_final\" + \".cif\"\n",
    "            slab_final.write(\n",
    "                os.path.join(\"out_data/final_slabs\", file_name_i))\n",
    "\n",
    "            # #####################################################\n",
    "            data_dict_i[\"slab_final\"] = slab_final\n",
    "\n",
    "        except TimeoutException:\n",
    "            data_dict_i[\"status\"] = \"Took too long\"\n",
    "\n",
    "            data = read_data_json()\n",
    "\n",
    "            systems_that_took_too_long = data.get(\"systems_that_took_too_long\", [])\n",
    "            systems_that_took_too_long.append((bulk_id_i, facet_i))\n",
    "\n",
    "            data[\"systems_that_took_too_long\"] = systems_that_took_too_long\n",
    "\n",
    "            data_path = os.path.join(\n",
    "                \"out_data/data.json\")\n",
    "            with open(data_path, \"w\") as fle:\n",
    "                json.dump(data, fle, indent=2)\n",
    "\n",
    "\n",
    "        finally:\n",
    "            # #################################################\n",
    "            signal.alarm(0)\n",
    "            signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "    # #####################################################\n",
    "    iter_time_i = time.time() - t0\n",
    "    data_dict_i[\"iter_time_i\"] = iter_time_i\n",
    "\n",
    "    data_dict_list = []\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "    df_slab_old = create_save_dataframe(\n",
    "        data_dict_list=data_dict_list,\n",
    "        df_slab_old=df_slab_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slab_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_slab\n",
    "\n",
    "df_slab = get_df_slab()\n",
    "\n",
    "# df_slab.shape: (136, 13)\n",
    "print(\"df_slab.shape:\", df_slab.shape)\n",
    "print(\"\")\n",
    "\n",
    "df_slab[df_slab.bulk_id == \"xtbocq9o6p\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# def get_bulk_selection_data():\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     #| - get_bulk_selection_data\n",
    "\n",
    "#     # ########################################################\n",
    "#     data_path = os.path.join(\n",
    "#         os.environ[\"PROJ_irox_oer\"],\n",
    "#         \"workflow/creating_slabs/selecting_bulks\",\n",
    "#         \"out_data/data.json\")\n",
    "#     with open(data_path, \"r\") as fle:\n",
    "#         data = json.load(fle)\n",
    "#     # ########################################################\n",
    "\n",
    "#     # bulk_ids__octa_unique = data[\"bulk_ids__octa_unique\"]\n",
    "\n",
    "#     return(data)\n",
    "#     #__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # ########################################################\n",
    "# data_path = os.path.join(\n",
    "#     os.environ[\"PROJ_irox_oer\"],\n",
    "#     \"workflow/creating_slabs/selecting_bulks\",\n",
    "#     \"out_data/data.json\")\n",
    "# with open(data_path, \"r\") as fle:\n",
    "#     data = json.load(fle)\n",
    "# # ########################################################\n",
    "\n",
    "# bulk_ids__octa_unique = data[\"bulk_ids__octa_unique\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# print(\"bulk_id_i:\", bulk_id_i, \"facet\", facet_i, end=\"\\r\", flush=True)\n",
    "\n",
    "# facet_i\n",
    "\n",
    "# create_save_dataframe?\n",
    "\n",
    "# df_slab_old[df_slab_old.bulk_id == \"8l919k6s7p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# iterator\n",
    "\n",
    "# for i_cnt, bulk_id in enumerate(iterator):\n",
    "\n",
    "# bulk_id\n",
    "\n",
    "# systems_not_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # #####################################################\n",
    "# row_xrd_i = df_xrd.loc[bulk_id]\n",
    "# # #####################################################\n",
    "# top_facets_i = row_xrd_i.top_facets\n",
    "# # #####################################################\n",
    "\n",
    "# top_facets_i\n",
    "# # facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# top_facets_i"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
