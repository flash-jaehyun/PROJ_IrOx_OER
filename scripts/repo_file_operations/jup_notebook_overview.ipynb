{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of jupyter notebooks in project\n",
    "---\n",
    "I will compile list of notebooks and check against the script that runs all notebooks. I want to make sure that everything is being run and no notebook is being neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/scripts/repo_file_operations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as  pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# #########################################################\n",
    "from jupyter_modules.jupyter_methods import get_df_jupyter_notebooks\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import get_notebooks_run_in__jobs_update\n",
    "from local_methods import get_files_exec_in_run_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_listed_dirs = [\n",
    "    \"sandbox\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# These notebooks are not intended to be run periodically, they are one-time use\n",
    "notebooks_to_ignore = [\n",
    "\n",
    "    # Only needs to be run manually, will set in motion many downstream jobs\n",
    "    \"dft_workflow/run_slabs/run_o_covered/setup_dft.ipynb\",\n",
    "\n",
    "    # #####################################################\n",
    "    # Slab creation notebooks\n",
    "    \"workflow/creating_slabs/create_slabs.ipynb\",\n",
    "    \"workflow/creating_slabs/creating_symm_slabs/creat_symm_slabs.ipynb\",\n",
    "    \"workflow/creating_slabs/modify_df_slab.ipynb\",\n",
    "    \"workflow/creating_slabs/process_slabs.ipynb\",\n",
    "    \"workflow/creating_slabs/quality_control/slab_size_xy_rep.ipynb\",\n",
    "    \"workflow/creating_slabs/selecting_bulks/select_bulks.ipynb\",\n",
    "    \"workflow/creating_slabs/slab_similarity/slab_similarity.ipynb\",\n",
    "    \"workflow/enumerate_adsorption/get_all_active_sites.ipynb\",\n",
    "    \"workflow/enumerate_adsorption/modify_df_active_sites.ipynb\",\n",
    "    \"workflow/enumerate_adsorption/test_dev_rdf_comp/opt_rdf_comp.ipynb\",\n",
    "    \"workflow/enumerate_adsorption/test_dev_rdf_comp/subtract_rdf_dfs.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Some notebooks in job_analysis\n",
    "    \"dft_workflow/job_analysis/id_resource_waste/id_resource_waste.ipynb\",\n",
    "    \"dft_workflow/job_analysis/measure_comp_resources/measure_cpu_hours.ipynb\",\n",
    "    \"dft_workflow/job_analysis/prepare_oer_sets/prepare_oer_sets.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Some notebooks in `compare_magmoms` aren't that useful anymore\n",
    "    \"dft_workflow/job_analysis/compare_magmoms/anal_magmoms.ipynb\",\n",
    "    \"dft_workflow/job_analysis/compare_magmoms/new_O_bare_magmom_comp/comp_new_O_bare_magmoms.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # DFT workflow bin scripts that are 'on occasion' use  only\n",
    "    \"dft_workflow/bin/anal_job_out.ipynb\",\n",
    "    \"dft_workflow/bin/delete_unsub_jobs.ipynb\",\n",
    "    \"dft_workflow/bin/run_unsub_jobs.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # One-time bulk processing scripts\n",
    "    \"workflow/process_bulk_dft/create_final_df_dft.ipynb\",\n",
    "    \"workflow/process_bulk_dft/get_bulk_coor_env.ipynb\",\n",
    "    \"workflow/process_bulk_dft/manually_classify_bulks/classify_bulks.ipynb\",\n",
    "    \"workflow/process_bulk_dft/read_json_to_new_ase.ipynb\",\n",
    "    \"workflow/process_bulk_dft/standardize_bulks/stan_bulks.ipynb\",\n",
    "    \"workflow/process_bulk_dft/write_atoms_json.ipynb\",\n",
    "\n",
    "    \"workflow/xrd_bulks/plot_xrd_patterns/plot_xrd_patterns.ipynb\",\n",
    "    \"workflow/xrd_bulks/xrd_bulks.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # These weren't too fruitful so no need to rerun\n",
    "    \"workflow/oer_vs_features/bivariate_combs_feat/bivariate_combs.ipynb\",\n",
    "    \"workflow/oer_vs_features/feature_covariance/feat_covar.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Don't run these things\n",
    "    \"scripts/git_scripts/get_modified_py_files.ipynb\",\n",
    "    \"scripts/git_scripts/git_check_size.ipynb\",\n",
    "    \"scripts/repo_file_operations/clean_jup.ipynb\",\n",
    "    \"scripts/repo_file_operations/jup_notebook_overview.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Don't run these things\n",
    "    \"workflow/__misc__/analysis_for_jens/analysis.ipynb\",\n",
    "    \"workflow/feature_engineering/catkit_form_oxid/catkit_form_oxid.ipynb\",\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # MISC\n",
    "    \"__misc__/00_group_meeting/group_meeting.ipynb\",\n",
    "    \"collated_plots.ipynb\",\n",
    "    \"dft_workflow/run_slabs/setup_jobs_from_oh/id_new_O_with_orig_OH/new_O_orig_OH.ipynb\",\n",
    "\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FILES_TO_IGNORE = [\n",
    "    \"dft_workflow/__misc__/finding_nonconstrained_mistakes/find_unconstr_slabs.ipynb\",\n",
    "    \"dft_workflow/job_analysis/slab_struct_drift/slab_struct_drift.ipynb\",\n",
    "    \"dft_workflow/job_analysis/systems_fully_ready/sys_fully_ready.ipynb\",\n",
    "    \"dft_workflow/job_processing/clean_dft_dirs.ipynb\",\n",
    "    \"workflow/oer_analysis/oer_scaling/oer_scaling.ipynb\",\n",
    "    ]\n",
    "\n",
    "notebooks_to_ignore.extend(TEMP_FILES_TO_IGNORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jupyter notebook dataframe\n",
    "PROJ_irox_path = os.environ[\"PROJ_irox_oer\"]\n",
    "\n",
    "# Get datafame of all jupyter notebooks in project\n",
    "df_ipynb = get_df_jupyter_notebooks(path=PROJ_irox_path)\n",
    "df_ipynb_i = df_ipynb\n",
    "\n",
    "# Read bash update method (calls all notebooks)\n",
    "df_ipynb_update = get_notebooks_run_in__jobs_update()\n",
    "\n",
    "df_run_all = get_files_exec_in_run_all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "for index_i, row_i in df_ipynb_i.iterrows():\n",
    "    # #####################################################\n",
    "    file_path_short_i = row_i.file_path_short\n",
    "    file_name_i = row_i.file_name\n",
    "    # #####################################################\n",
    "\n",
    "    # Skipping certain rows\n",
    "    if \"sandbox\" in file_name_i:\n",
    "        continue\n",
    "    if file_path_short_i in notebooks_to_ignore:\n",
    "        continue\n",
    "\n",
    "    black_listed = False\n",
    "    for dir_i in black_listed_dirs:\n",
    "        if file_path_short_i.find(dir_i) == 0:\n",
    "            black_listed = True\n",
    "    if black_listed:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    df_i = df_run_all[df_run_all.path_short == file_path_short_i]\n",
    "    if df_i.shape[0] == 0:\n",
    "        print(file_path_short_i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
