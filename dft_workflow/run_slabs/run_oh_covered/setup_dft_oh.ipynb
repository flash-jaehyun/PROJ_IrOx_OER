{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs/run_oh_covered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import MultiIndex\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# # from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_slab,\n",
    "    get_df_slabs_to_run,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_active_sites,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    get_df_slabs_oh,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import get_job_spec_dft_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slac queue to submit to\n",
    "slac_sub_queue_i = \"suncat2\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_slab = get_df_slab()\n",
    "df_slab = df_slab.set_index(\"slab_id\")\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "# #########################################################\n",
    "df_active_sites = get_df_active_sites()\n",
    "\n",
    "# #########################################################\n",
    "df_slabs_to_run = get_df_slabs_to_run()\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_slabs_oh = get_df_slabs_oh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data/dft_jobs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"__temp__\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "compenv = os.environ[\"COMPENV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for multiple *O calcs, need to make code more robust later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nersc', 'vuraruna_65', 'o')\n",
      "\n",
      "COMBAK CHECK THIS\n",
      "This was made when there was only 1 *O calc, make sure it's not creating new *OH jobs after running more *O calcs\n"
     ]
    }
   ],
   "source": [
    "var = \"o\"\n",
    "df_jobs_anal_o = df_jobs_anal.query('ads == @var')\n",
    "\n",
    "# #########################################################\n",
    "indices_to_remove = []\n",
    "# #########################################################\n",
    "group_cols = [\"compenv\", \"slab_id\", \"ads\", ]\n",
    "grouped = df_jobs_anal_o.groupby(group_cols)\n",
    "for name, group in grouped:\n",
    "\n",
    "    num_rows = group.shape[0]\n",
    "    # print(num_rows)\n",
    "    if num_rows > 1:\n",
    "        print(name)\n",
    "        print(\"\")\n",
    "        # print(num_rows)\n",
    "        print(\"COMBAK CHECK THIS\")\n",
    "        print(\"This was made when there was only 1 *O calc, make sure it's not creating new *OH jobs after running more *O calcs\")\n",
    "        # assert False\n",
    "\n",
    "        # group[group.att_num != 1]\n",
    "        group_index = group.index.to_frame()\n",
    "        group_index_i = group_index[group_index.att_num != 1]\n",
    "\n",
    "        indices_to_remove.extend(\n",
    "            group_index_i.index.tolist()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jobs_anal_completed.shape: (176, 8)\n",
      "df_jobs_anal_completed.index.to_frame().shape: (176, 5)\n"
     ]
    }
   ],
   "source": [
    "df_jobs_anal_completed = df_jobs_anal[df_jobs_anal.job_completely_done == True]\n",
    "\n",
    "df_jobs_anal_completed = df_jobs_anal_completed.drop(labels=indices_to_remove)\n",
    "\n",
    "print(\"df_jobs_anal_completed.shape:\", df_jobs_anal_completed.shape)\n",
    "print(\"df_jobs_anal_completed.index.to_frame().shape:\",\n",
    "      df_jobs_anal_completed.index.to_frame().shape)\n",
    "\n",
    "df_jobs_anal_completed = pd.concat([\n",
    "    df_jobs_anal_completed.index.to_frame(),\n",
    "    df_jobs_anal_completed,\n",
    "    ], axis=1)\n",
    "\n",
    "df_jobs_anal_completed = df_jobs_anal_completed[\n",
    "    [\"compenv\", \"slab_id\", \"job_id_max\", \"att_num\", ]]\n",
    "\n",
    "# For the purpose of picking preparing *OH jobs we only need completed \n",
    "var = \"o\"\n",
    "df_jobs_anal_completed = df_jobs_anal_completed.query('ads == @var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "compenv__slab_id__att_num__tuple = tuple(zip(\n",
    "    df_jobs_anal_completed.compenv,\n",
    "    df_jobs_anal_completed.slab_id,\n",
    "    df_jobs_anal_completed.att_num,\n",
    "    ))\n",
    "\n",
    "df_jobs_anal_completed[\"compenv__slab_id__att_num\"] = compenv__slab_id__att_num__tuple\n",
    "df_jobs_anal_completed = df_jobs_anal_completed.set_index(\"compenv__slab_id__att_num\", drop=False)\n",
    "\n",
    "# #########################################################\n",
    "compenv__slab_id__att_num__tuple = tuple(zip(\n",
    "    df_slabs_to_run.compenv,\n",
    "    df_slabs_to_run.slab_id,\n",
    "    df_slabs_to_run.att_num,\n",
    "    ))\n",
    "\n",
    "df_slabs_to_run[\"compenv__slab_id__att_num\"] = compenv__slab_id__att_num__tuple\n",
    "df_slabs_to_run = df_slabs_to_run.set_index(\"compenv__slab_id__att_num\", drop=False)\n",
    "\n",
    "# #########################################################\n",
    "shared_indices = df_jobs_anal_completed.index.intersection(df_slabs_to_run.index)\n",
    "df_i = pd.concat([\n",
    "    df_slabs_to_run.loc[shared_indices].status,\n",
    "    df_jobs_anal_completed.loc[shared_indices],\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_i[\n",
    "    (df_i.status == \"ok\")\n",
    "    # (df_i.status != \"bad\")\n",
    "    ]\n",
    "\n",
    "ind = MultiIndex.from_tuples(\n",
    "    df_i.index, sortorder=None,\n",
    "    names=[\"compenv\", \"slab_id\", \"att_num\", ])\n",
    "df_i = df_i.set_index(ind)\n",
    "\n",
    "# #########################################################\n",
    "df_i = df_i.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(\"TEMP TEMP TEMP\")\n",
    "print(\"Remove this to do other *OH systems on NERSC\")\n",
    "\n",
    "# df_i = df_i[\n",
    "#     (df_i.compenv == \"nersc\") & \\\n",
    "#     (df_i.slab_id == \"vuraruna_65\") & \\\n",
    "#     [True for i in range(len(df_i))]\n",
    "#     ]\n",
    "\n",
    "# compenv: nersc | slab_id: kalisule_45 | att_num: 1\n",
    "df_i = df_i[\n",
    "    (df_i.compenv == \"nersc\") & \\\n",
    "    (df_i.slab_id == \"kalisule_45\") & \\\n",
    "    [True for i in range(len(df_i))]\n",
    "    ]\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "compenv: nersc | slab_id: kalisule_45 | att_num: 1\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__62/00_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__62/01_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__62/02_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__62/03_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__67/00_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__67/01_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__67/02_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__67/03_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__68/00_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__68/01_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__68/02_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__68/03_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__71/00_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__71/01_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__71/02_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__71/03_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__73/00_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__73/01_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__73/02_attempt/_01\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/oh/active_site__73/03_attempt/_01\n"
     ]
    }
   ],
   "source": [
    "data_dict_list = []\n",
    "for name_i, row_i in df_i.iterrows():\n",
    "\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    att_num_i = name_i[2]\n",
    "\n",
    "    print(40 * \"=\")\n",
    "    print(\"compenv:\", compenv_i, \"|\", \"slab_id:\", slab_id_i, \"|\", \"att_num:\", att_num_i)\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_i = df_jobs[df_jobs.compenv == compenv_i]\n",
    "    row_jobs_i = df_jobs_i[df_jobs_i.job_id == job_id_max_i]\n",
    "    row_jobs_i = row_jobs_i.iloc[0]\n",
    "    # #####################################################\n",
    "    # att_num_i = row_jobs_i.att_num\n",
    "    bulk_id_i = row_jobs_i.bulk_id\n",
    "    facet_i = row_jobs_i.facet\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "    row_data_i = df_jobs_data_i[df_jobs_data_i.job_id == job_id_max_i]\n",
    "    row_data_i = row_data_i.iloc[0]\n",
    "    # #####################################################\n",
    "    slab_i = row_data_i.final_atoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_active_site_i = df_active_sites[df_active_sites.slab_id == slab_id_i]\n",
    "    row_active_site_i = row_active_site_i.iloc[0]\n",
    "    # #####################################################\n",
    "    active_sites_unique_i = row_active_site_i.active_sites_unique\n",
    "    num_active_sites_unique_i = row_active_site_i.num_active_sites_unique\n",
    "    # #####################################################\n",
    "\n",
    "    # print(\"TEMP manually defining active sites\")\n",
    "    # active_sites_unique_i = [62, 67, 68, 71, 73]\n",
    "    # active_sites_unique_i = [62, ]\n",
    "\n",
    "    for active_site_j in active_sites_unique_i:\n",
    "\n",
    "        df_slabs_oh_i = df_slabs_oh.loc[(compenv_i, slab_id_i, \"o\", active_site_j, att_num_i)]\n",
    "        for att_num_oh_k, row_k in df_slabs_oh_i.iterrows():\n",
    "            data_dict_i = dict()\n",
    "\n",
    "            slab_oh_k = row_k.slab_oh\n",
    "            num_atoms_k = slab_oh_k.get_global_number_of_atoms()\n",
    "\n",
    "            # #####################################################\n",
    "            # attempt = 1\n",
    "            rev = 1\n",
    "            path_i = os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "                \"dft_workflow/run_slabs/run_oh_covered\",\n",
    "                \"out_data/dft_jobs\",\n",
    "                compenv_i,\n",
    "                bulk_id_i, facet_i,\n",
    "                \"oh\",\n",
    "                \"active_site__\" + str(active_site_j).zfill(2),\n",
    "                str(att_num_oh_k).zfill(2) + \"_attempt\",  # Attempt\n",
    "                \"_\" + str(rev).zfill(2),  # Revision\n",
    "                )\n",
    "\n",
    "            # print(\"  active_site_j:\", active_site_j)\n",
    "            # print(\"    att_num:\", att_num_oh_k)\n",
    "            # print(active_site_j, att_num_oh_k)\n",
    "            # print(\"path_i:\", path_i)\n",
    "            print(path_i)\n",
    "\n",
    "            if os.path.exists(path_i):\n",
    "                tmp = 42\n",
    "                # print(\"Already exists\")\n",
    "                # print(\"DISJFIDSI\")\n",
    "\n",
    "            if not os.path.exists(path_i):\n",
    "                os.makedirs(path_i)\n",
    "\n",
    "                # #################################################\n",
    "                # Copy dft script to job folder\n",
    "                # #################################################\n",
    "                copyfile(\n",
    "                    os.path.join(\n",
    "                        os.environ[\"PROJ_irox_oer\"],\n",
    "                        \"dft_workflow/dft_scripts/slab_dft.py\"),\n",
    "                    os.path.join(\n",
    "                        path_i,\n",
    "                        \"model.py\"),\n",
    "                    )\n",
    "\n",
    "\n",
    "                # #################################################\n",
    "                # Copy atoms object to job folder\n",
    "                slab_oh_k.write(\n",
    "                    os.path.join(path_i, \"init.traj\")\n",
    "                    )\n",
    "\n",
    "                # #################################################\n",
    "                data_dict_i[\"compenv\"] = compenv_i\n",
    "                data_dict_i[\"slab_id\"] = slab_id_i\n",
    "                data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "                data_dict_i[\"att_num\"] = att_num_i\n",
    "                data_dict_i[\"rev_num\"] = rev\n",
    "                data_dict_i[\"active_site\"] = active_site_j\n",
    "                data_dict_i[\"facet\"] = facet_i\n",
    "                data_dict_i[\"slab_oh\"] = slab_oh_k\n",
    "                data_dict_i[\"num_atoms\"] = num_atoms_k\n",
    "                data_dict_i[\"path_i\"] = path_i\n",
    "                # #################################################\n",
    "                data_dict_list.append(data_dict_i)\n",
    "                # #################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_new = pd.DataFrame(data_dict_list)\n",
    "\n",
    "df_jobs_new[\"compenv__slab_id__att_num__active_site\"] = list(zip(\n",
    "    df_jobs_new.compenv,\n",
    "    df_jobs_new.slab_id,\n",
    "    df_jobs_new.att_num,\n",
    "    df_jobs_new.active_site))\n",
    "\n",
    "df_jobs_new = df_jobs_new.set_index(\"compenv__slab_id__att_num__active_site\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    compenv__slab_id__att_num__active_site_i = row_i.name\n",
    "    compenv_i, slab_id_i, att_num_i, active_site_i = row_i.name\n",
    "\n",
    "    compenv_i = row_i.compenv\n",
    "    num_atoms = row_i.num_atoms\n",
    "    path_i = row_i.path_i\n",
    "    # ####################################################\n",
    "    dft_params_i = get_job_spec_dft_params(\n",
    "        # compenv=compenv,\n",
    "        compenv=compenv_i,\n",
    "        slac_sub_queue=slac_sub_queue_i,\n",
    "        )\n",
    "    dft_params_i[\"ispin\"] = 2\n",
    "\n",
    "    # #####################################################\n",
    "    with open(os.path.join(path_i, \"dft-params.json\"), \"w+\") as fle:\n",
    "        json.dump(dft_params_i, fle, indent=2, skipkeys=True)\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"att_num\"] = att_num_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"compenv__slab_id__att_num__active_site\"] = \\\n",
    "        compenv__slab_id__att_num__active_site_i\n",
    "    data_dict_i[\"dft_params\"] = dft_params_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_dft_params = pd.DataFrame(data_dict_list)\n",
    "\n",
    "keys = [\"compenv\", \"slab_id\", \"att_num\", \"active_site\"]\n",
    "df_dft_params = df_dft_params.set_index(keys, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "setup_dft.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"setup_dft.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_i = df_jobs[df_jobs.compenv == compenv_i]\n",
    "# row_jobs_i = df_jobs_i[df_jobs_i.job_id == job_id_max_i]\n",
    "\n",
    "# # row_jobs_i = \n",
    "# # df_jobs_i[df_jobs_i.job_id == job_id_max_i]\n",
    "\n",
    "# # row_jobs_i = row_jobs_i.iloc[0]\n",
    "# # row_jobs_i\n",
    "\n",
    "# # job_id_max_i\n",
    "# # df_jobs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_i = \n",
    "# df_jobs[df_jobs.compenv == compenv_i]\n",
    "# compenv_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # #########################################################\n",
    "# # Only selecting rows from current compenv, jobs will be continued on same cluster that O* was run in\n",
    "# if compenv == \"wsl\":\n",
    "#     compenv = \"slac\"\n",
    "\n",
    "# df_i = df_i.loc[compenv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(compenv_i)\n",
    "# print(\"\")\n",
    "\n",
    "# # dft_params_i = \n",
    "# get_job_spec_dft_params(\n",
    "#     compenv=compenv_i,\n",
    "#     slac_sub_queue=slac_sub_queue_i,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dft_params"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
