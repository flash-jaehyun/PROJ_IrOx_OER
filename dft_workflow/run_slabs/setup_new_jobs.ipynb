{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing job sets (everything within a `02_attempt` dir for example)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import dictdiffer\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_anal,\n",
    "    cwd,\n",
    "    )\n",
    "\n",
    "from dft_workflow_methods import is_job_understandable, job_decision\n",
    "from dft_workflow_methods import transfer_job_files_from_old_to_new\n",
    "from dft_workflow_methods import is_job_compl_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_no_file_ops = True  # True if just testing around, False for production mode\n",
    "TEST_no_file_ops = False\n",
    "\n",
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat2\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jobs.shape: \t\t (534, 11)\n",
      "df_jobs_data.shape: \t (534, 36)\n"
     ]
    }
   ],
   "source": [
    "df_jobs = get_df_jobs()\n",
    "print(\"df_jobs.shape:\", 2 * \"\\t\", df_jobs.shape)\n",
    "df_jobs_data = get_df_jobs_data(drop_cols=False)\n",
    "print(\"df_jobs_data.shape:\", 1 * \"\\t\", df_jobs_data.shape)\n",
    "df_jobs_paths = get_df_jobs_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "df_resubmit = df_jobs_anal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_root_path = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/dft_scripts/out_data\")\n",
    "data_path = os.path.join(data_root_path, \"conservative_mixing_params.json\")\n",
    "with open(data_path, \"r\") as fle:\n",
    "    dft_calc_sett_mix = json.load(fle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter `df_resubmit` to only rows that are to be resubmitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_resubmit.shape: (30, 8)\n",
      "df_nosubmit.shape: (177, 8)\n"
     ]
    }
   ],
   "source": [
    "df_resubmit_tmp = copy.deepcopy(df_resubmit)\n",
    "\n",
    "# #########################################################\n",
    "mask_list = []\n",
    "for i in df_resubmit.decision.tolist():\n",
    "    if \"resubmit\" in i:\n",
    "        mask_list.append(True)\n",
    "    else:\n",
    "        mask_list.append(False)\n",
    "\n",
    "df_resubmit = df_resubmit_tmp[mask_list]\n",
    "df_nosubmit = df_resubmit_tmp[np.invert(mask_list)]\n",
    "\n",
    "print(\"df_resubmit.shape:\", df_resubmit.shape)\n",
    "print(\"df_nosubmit.shape:\", df_nosubmit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are jobs being left idle, nothing to do, fix it\n",
      "['rehurese_36']\n"
     ]
    }
   ],
   "source": [
    "df_i = df_nosubmit[df_nosubmit.job_completely_done == False]\n",
    "\n",
    "# df_i[df_i.decision == []]\n",
    "index_mask = []\n",
    "for name_i, row_i in df_i.iterrows():\n",
    "    decision_i = row_i.decision\n",
    "\n",
    "    if len(decision_i) == 0:\n",
    "        index_mask.append(name_i)\n",
    "df_i = df_i.loc[index_mask]\n",
    "\n",
    "if df_i.shape[0] > 0:\n",
    "    print(\"There are jobs being left idle, nothing to do, fix it\")\n",
    "    print(df_i.job_id_max.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new job directories and initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/nersc/8fxi6rmp75/012/bare/active_site__43/01_attempt/_02\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/bare/active_site__68/01_attempt/_02\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/nersc/b5cgvsb16w/111/bare/active_site__73/01_attempt/_02\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/nersc/z36lb3bdcq/001/bare/active_site__50/02_attempt/_02\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/bare/active_site__62/01_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/oh/active_site__62/00_attempt/_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/anaconda3/envs/PROJ_irox_oer/lib/python3.6/site-packages/ase/io/jsonio.py:122: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/oh/active_site__62/02_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/oh/active_site__63/00_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/oh/active_site__63/01_attempt/_09\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/oh/active_site__63/02_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/81meck64ba/110/oh/active_site__63/03_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/bare/active_site__62/01_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__62/00_attempt/_05\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__62/01_attempt/_04\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__67/02_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__67/03_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__68/01_attempt/_08\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__71/00_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__71/02_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__73/00_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/b5cgvsb16w/111/oh/active_site__73/03_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/vhnkve7ev1/001/bare/active_site__33/01_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/vhnkve7ev1/001/oh/active_site__32/00_attempt/_06\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/vhnkve7ev1/001/oh/active_site__33/01_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/vhnkve7ev1/001/oh/active_site__33/03_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/zw9pbrnabj/010/01_attempt/_08\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/mjctxrx3zf/011/bare/active_site__34/01_attempt/_08\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/z36lb3bdcq/001/bare/active_site__50/01_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/z36lb3bdcq/001/oh/active_site__50/01_attempt/_07\n",
      "****************************************\n",
      "/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/z36lb3bdcq/001/oh/active_site__50/03_attempt/_09\n"
     ]
    }
   ],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, (name_i, row_i) in enumerate(df_resubmit.iterrows()):\n",
    "    data_dict_i = dict()\n",
    "    print(40 * \"*\")\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    ads_i = name_i[2]\n",
    "    active_site_i = name_i[3]\n",
    "    att_num_i = name_i[4]\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    # compenv_i = row_i.compenv\n",
    "    dft_params_new = row_i.dft_params_new\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_i = df_jobs[df_jobs.compenv == compenv_i]\n",
    "    row_jobs_i = df_jobs_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    rev_num = row_jobs_i.rev_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "    row_paths_max_i = df_jobs_paths_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    gdrive_path = row_paths_max_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "    row_data_max_i = df_jobs_data_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    num_steps = row_data_max_i.num_steps\n",
    "    incar_params = row_data_max_i.incar_params\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        gdrive_path)\n",
    "\n",
    "    # #########################################################\n",
    "    # Copy files to new job dir\n",
    "    new_path_i = \"/\".join(path_i.split(\"/\")[0:-1] + [\"_\" + str(rev_num + 1).zfill(2)])\n",
    "    print(new_path_i)\n",
    "\n",
    "    if not TEST_no_file_ops:\n",
    "        if not os.path.exists(new_path_i):\n",
    "            os.makedirs(new_path_i)\n",
    "\n",
    "            \n",
    "    files_to_transfer_for_new_job = [\n",
    "        # [\"contcar_out.traj\", \"init.traj\"],\n",
    "        [\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/slab_dft.py\"),\n",
    "            \"model.py\",\n",
    "            ],\n",
    "        # \"model.py\",\n",
    "\n",
    "        \"WAVECAR\",\n",
    "        \"dft-params.json\",\n",
    "        [\"dir_dft_params/dft-params.json\", \"dft-params.json\"],\n",
    "        ]\n",
    "\n",
    "    # #########################################################\n",
    "    with cwd(path_i):\n",
    "        if num_steps > 0:\n",
    "            atoms = io.read(\"CONTCAR\")\n",
    "            atoms.write(\"contcar_out.traj\")\n",
    "            files_to_transfer_for_new_job.append(\n",
    "                [\"contcar_out.traj\", \"init.traj\"])\n",
    "\n",
    "\n",
    "        else:\n",
    "            atoms = io.read(\"init.traj\")\n",
    "            files_to_transfer_for_new_job.append(\n",
    "                \"init.traj\"\n",
    "                # [\"contcar_out.traj\", \"init.traj\"]\n",
    "                )\n",
    "\n",
    "        # If spin-polarized calculation then get magmoms from prev. job and pass to new job\n",
    "        if incar_params[\"ISPIN\"] == 2:\n",
    "            if num_steps > 0:\n",
    "                atoms_outcar = io.read(\"OUTCAR\")\n",
    "                magmoms_i_tmp = atoms_outcar.get_magnetic_moments()\n",
    "\n",
    "                data_path = os.path.join(\"out_data/magmoms_out.json\")\n",
    "                with open(data_path, \"w\") as outfile:\n",
    "                    json.dump(magmoms_i_tmp.tolist(), outfile)\n",
    "\n",
    "                files_to_transfer_for_new_job.append(\n",
    "                    [\"out_data/magmoms_out.json\", \"magmoms.json\"])\n",
    "\n",
    "        num_atoms = atoms.get_global_number_of_atoms()\n",
    "        \n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    if not TEST_no_file_ops:\n",
    "        transfer_job_files_from_old_to_new(\n",
    "            path_i=path_i,\n",
    "            new_path_i=new_path_i,\n",
    "            files_to_transfer_for_new_job=files_to_transfer_for_new_job,\n",
    "            )\n",
    "\n",
    "    # #####################################################\n",
    "    if not TEST_no_file_ops:\n",
    "        dft_params_path_i = os.path.join(\n",
    "            new_path_i,\n",
    "            \"dft-params.json\")\n",
    "        with open(dft_params_path_i, \"r\") as fle:\n",
    "            dft_params_current = json.load(fle)\n",
    "\n",
    "        # Update previous DFT parameters with new ones\n",
    "        dft_params_current.update(dft_params_new)\n",
    "\n",
    "        with open(dft_params_path_i, \"w\") as outfile:\n",
    "            json.dump(dft_params_current, outfile, indent=2)\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"path_i\"] = new_path_i\n",
    "    data_dict_i[\"num_atoms\"] = num_atoms\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "# #########################################################\n",
    "df_sub = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# magmoms_i_tmp.to_list()\n",
    "# magmoms_i_tmp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_resubmit.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# row_i\n",
    "\n",
    "\n",
    "# compenv_i = \"sherlock\"\n",
    "# slab_id_i = \"putarude_21\"\n",
    "# att_num_i = 1\n",
    "\n",
    "# df_tmp = df_resubmit[\n",
    "#     (df_resubmit.compenv == compenv_i) & \\\n",
    "#     (df_resubmit.slab_id == slab_id_i) & \\\n",
    "#     (df_resubmit.att_num == att_num_i) & \\\n",
    "#     [True for i in range(len(df_resubmit))]\n",
    "#     ]\n",
    "\n",
    "# print(\"job_id_max_i:\", df_tmp.iloc[0].job_id_max)\n",
    "\n",
    "# # Before when the notebook was breaking the job_id_max for this row was:\n",
    "# # dunosagi_96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_i = \n",
    "# df_jobs[df_jobs.compenv == compenv_i].loc[job_id_max_i]\n",
    "# row_jobs_i = df_jobs_i.loc[job_id_max_i]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
