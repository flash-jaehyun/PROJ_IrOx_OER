{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing job sets (everything within a `02_attempt` dir for example)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import dictdiffer\n",
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_slab,\n",
    "    cwd,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import (\n",
    "    is_job_understandable,\n",
    "    job_decision,\n",
    "    transfer_job_files_from_old_to_new,\n",
    "    is_job_compl_done,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_no_file_ops = True  # True if just testing around, False for production mode\n",
    "TEST_no_file_ops = False\n",
    "\n",
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat3\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_df_jobs()\n",
    "# print(\"df_jobs.shape:\", 2 * \"\\t\", df_jobs.shape)\n",
    "\n",
    "df_jobs_data = get_df_jobs_data(drop_cols=False)\n",
    "# print(\"df_jobs_data.shape:\", 1 * \"\\t\", df_jobs_data.shape)\n",
    "\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "df_slab = get_df_slab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing systems that were marked to be ignored\n",
    "from methods import get_systems_to_stop_run_indices\n",
    "\n",
    "indices_to_stop_running = get_systems_to_stop_run_indices(df_jobs_anal=df_jobs_anal)\n",
    "df_jobs_anal = df_jobs_anal.drop(index=indices_to_stop_running)\n",
    "\n",
    "df_resubmit = df_jobs_anal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs_anal[df_jobs_anal.job_id_max == \"satakihu_11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_root_path = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/dft_scripts/out_data\")\n",
    "data_path = os.path.join(data_root_path, \"conservative_mixing_params.json\")\n",
    "with open(data_path, \"r\") as fle:\n",
    "    dft_calc_sett_mix = json.load(fle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter `df_resubmit` to only rows that are to be resubmitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids_to_force_resubmit = [\n",
    "    # #####################################################\n",
    "    # \"fukudiko_66\",\n",
    "    # \"fipipida_61\",\n",
    "    # \"tibunane_36\",\n",
    "\n",
    "    # #####################################################\n",
    "    \"tisakuri_50\",\n",
    "    \"hukemena_85\",\n",
    "    \"sewedawu_95\",\n",
    "    \"rugagumu_31\",\n",
    "    \"gitogahu_48\",\n",
    "    \"bigufoha_89\",\n",
    "    \"pubahadu_79\",\n",
    "    \"hebomume_93\",\n",
    "    \"makaledi_83\",\n",
    "    \"fevupilo_12\",\n",
    "    \"ruvanaka_31\",\n",
    "    \"wirurabi_88\",\n",
    "    \"ludekatu_27\",\n",
    "    \"hogigova_47\",\n",
    "    \"koboguhi_40\",\n",
    "    \"kalidasa_91\",\n",
    "    \"larodaru_75\",\n",
    "    # ############\n",
    "    \"nonogase_08\",\n",
    "    \"gorofiwe_14\",\n",
    "    \"gatevehu_95\",\n",
    "    \"kebolapu_40\",\n",
    "    \"pesusero_02\",\n",
    "    # ############\n",
    "    \"parihobe_18\",\n",
    "    \"fuhegara_35\",\n",
    "    \"ropirema_45\",\n",
    "    \"sisipule_40\",\n",
    "    \"valagane_51\",\n",
    "    \"revotaho_43\",\n",
    "\n",
    "    \"pigenipi_49\",\n",
    "    \"nitisule_36\",\n",
    "\n",
    "    # ############\n",
    "    \"budubadi_27\",\n",
    "    \"mudebupo_43\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resubmit_tmp = copy.deepcopy(df_resubmit)\n",
    "\n",
    "# #########################################################\n",
    "mask_list = []\n",
    "\n",
    "# for i in df_resubmit.decision.tolist():\n",
    "for name_i, row_i in df_resubmit.iterrows():\n",
    "    decision_i = row_i.decision\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "\n",
    "    if \"resubmit\" in decision_i or job_id_max_i in job_ids_to_force_resubmit:\n",
    "        mask_list.append(True)\n",
    "    else:\n",
    "        mask_list.append(False)\n",
    "\n",
    "df_resubmit = df_resubmit_tmp[mask_list]\n",
    "df_nosubmit = df_resubmit_tmp[np.invert(mask_list)]\n",
    "\n",
    "# print(\"df_resubmit.shape:\", df_resubmit.shape)\n",
    "# print(\"df_nosubmit.shape:\", df_nosubmit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing `systems_to_stop_running`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are jobs being left idle, nothing to do, fix it\n",
      "['fokukuno_98']\n",
      "\n",
      "********************\n",
      "gdrive_path_i: dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/m2bs8w82x5/001/01_attempt/_02\n",
      "job_id_i: fokukuno_98\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i = df_nosubmit[df_nosubmit.job_completely_done == False]\n",
    "\n",
    "index_mask = []\n",
    "for name_i, row_i in df_i.iterrows():\n",
    "    decision_i = row_i.decision\n",
    "\n",
    "    if len(decision_i) == 0:\n",
    "        index_mask.append(name_i)\n",
    "    else:\n",
    "        add_name_to_mask = False\n",
    "        for decision_str_j in decision_i:\n",
    "            str_frags = [\"not understandable\", ]\n",
    "            for str_i in str_frags:\n",
    "                if str_i in decision_str_j:\n",
    "                    add_name_to_mask = True\n",
    "\n",
    "        if add_name_to_mask:\n",
    "            index_mask.append(name_i)\n",
    "df_i = df_i.loc[index_mask]\n",
    "\n",
    "# #########################################################\n",
    "# Only rerunning from slab generations > 1\n",
    "def method(row_i):\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    row_jobs_i = df_jobs.loc[job_id_max_i]\n",
    "    slab_id_i = row_jobs_i.slab_id\n",
    "    row_slab_i = df_slab.loc[slab_id_i]\n",
    "    phase_i = row_slab_i.phase\n",
    "    return(phase_i)\n",
    "df_i[\"phase\"] = df_i.apply(method,axis=1)\n",
    "df_i = df_i[df_i.phase > 1]\n",
    "\n",
    "# #########################################################\n",
    "if df_i.shape[0] > 0:\n",
    "    print(\"There are jobs being left idle, nothing to do, fix it\")\n",
    "    print(df_i.job_id_max.tolist())\n",
    "    print(\"\")\n",
    "\n",
    "    job_ids = df_i.job_id_max.tolist()\n",
    "\n",
    "    df_jobs_i = df_jobs.loc[job_ids]\n",
    "\n",
    "    for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "        row_path_i = df_jobs_paths.loc[job_id_i]\n",
    "        gdrive_path_i = row_path_i.gdrive_path\n",
    "\n",
    "        path_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            gdrive_path_i,\n",
    "            \"job.err\")\n",
    "\n",
    "        if verbose:\n",
    "            print(20 * \"*\")\n",
    "            print(\"gdrive_path_i:\", gdrive_path_i)\n",
    "            print(\"job_id_i:\", job_id_i)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                my_file = Path(path_i)\n",
    "                if my_file.is_file():\n",
    "\n",
    "                    with open(path_i, \"r\") as f:\n",
    "                        lines = f.read().splitlines()\n",
    "\n",
    "                    tmp = [print(i) for i in lines[-10:]]\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new job directories and initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "('sherlock', 'batipoha_75', 'bare', 38.0, 1)\n",
      "****************************************\n",
      "('sherlock', 'fugorumi_32', 'bare', 39.0, 1)\n",
      "****************************************\n",
      "('sherlock', 'fugorumi_32', 'bare', 44.0, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/anaconda3/envs/PROJ_irox_oer/lib/python3.6/site-packages/ase/io/jsonio.py:122: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "('sherlock', 'sedosuno_37', 'o', 'NaN', 1)\n",
      "****************************************\n",
      "('sherlock', 'tidadiva_44', 'o', 'NaN', 1)\n",
      "****************************************\n",
      "('sherlock', 'tinugono_42', 'o', 'NaN', 1)\n"
     ]
    }
   ],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, (name_i, row_i) in enumerate(df_resubmit.iterrows()):\n",
    "\n",
    "    data_dict_i = dict()\n",
    "    if verbose:\n",
    "        print(40 * \"*\")\n",
    "        print(name_i)\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    ads_i = name_i[2]\n",
    "    active_site_i = name_i[3]\n",
    "    att_num_i = name_i[4]\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    dft_params_new = row_i.dft_params_new\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_i = df_jobs[df_jobs.compenv == compenv_i]\n",
    "    row_jobs_i = df_jobs_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    rev_num = row_jobs_i.rev_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "    row_paths_max_i = df_jobs_paths_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    gdrive_path = row_paths_max_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "    row_data_max_i = df_jobs_data_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    num_steps = row_data_max_i.num_steps\n",
    "    incar_params = row_data_max_i.incar_params\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        gdrive_path)\n",
    "\n",
    "\n",
    "    from pathlib import Path\n",
    "    outcar_is_there = False\n",
    "    my_file = Path(os.path.join(path_i, \"OUTCAR\"))\n",
    "    if my_file.is_file():\n",
    "        outcar_is_there = True\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Copy files to new job dir\n",
    "    new_path_i = \"/\".join(path_i.split(\"/\")[0:-1] + [\"_\" + str(rev_num + 1).zfill(2)])\n",
    "\n",
    "    if not TEST_no_file_ops:\n",
    "        if not os.path.exists(new_path_i):\n",
    "            print(new_path_i)\n",
    "            os.makedirs(new_path_i)\n",
    "\n",
    "            \n",
    "    files_to_transfer_for_new_job = [\n",
    "        # [\"contcar_out.traj\", \"init.traj\"],\n",
    "        [\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/slab_dft.py\"),\n",
    "            \"model.py\",\n",
    "            ],\n",
    "        # \"model.py\",\n",
    "\n",
    "        \"WAVECAR\",\n",
    "        \"dft-params.json\",\n",
    "        [\"dir_dft_params/dft-params.json\", \"dft-params.json\"],\n",
    "        \"data_dict.json\",\n",
    "        ]\n",
    "\n",
    "\n",
    "    # if outcar_is_there:\n",
    "\n",
    "    contcar_is_there = False\n",
    "    my_file = Path(os.path.join(path_i, \"CONTCAR\"))\n",
    "    if my_file.is_file():\n",
    "        contcar_is_there = True\n",
    "\n",
    "    not_ready = False\n",
    "    if num_steps > 0 and not contcar_is_there:\n",
    "        print(\"num_steps > 0 but CONTCAR is not avail\", name_i)\n",
    "        not_ready = True\n",
    "\n",
    "\n",
    "    if not not_ready:\n",
    "        # #####################################################\n",
    "        with cwd(path_i):\n",
    "            if num_steps > 0:\n",
    "                atoms = io.read(\"CONTCAR\")\n",
    "                atoms.write(\"contcar_out.traj\")\n",
    "                files_to_transfer_for_new_job.append(\n",
    "                    [\"contcar_out.traj\", \"init.traj\"])\n",
    "\n",
    "\n",
    "            else:\n",
    "                atoms = io.read(\"init.traj\")\n",
    "                files_to_transfer_for_new_job.append(\n",
    "                    \"init.traj\"\n",
    "                    # [\"contcar_out.traj\", \"init.traj\"]\n",
    "                    )\n",
    "\n",
    "            # If spin-polarized calculation then get magmoms from prev. job and pass to new job\n",
    "\n",
    "            if outcar_is_there:\n",
    "                if incar_params[\"ISPIN\"] == 2:\n",
    "                    if num_steps > 0:\n",
    "                        atoms_outcar = io.read(\"OUTCAR\")\n",
    "                        magmoms_i_tmp = atoms_outcar.get_magnetic_moments()\n",
    "\n",
    "                        data_path = os.path.join(\"out_data/magmoms_out.json\")\n",
    "                        with open(data_path, \"w\") as outfile:\n",
    "                            json.dump(magmoms_i_tmp.tolist(), outfile)\n",
    "\n",
    "                        files_to_transfer_for_new_job.append(\n",
    "                            [\"out_data/magmoms_out.json\", \"magmoms.json\"])\n",
    "                # If previous job was the non-spin-pol calc, then copy through the magmoms.json file\n",
    "                elif incar_params[\"ISPIN\"] == 1:\n",
    "                    files_to_transfer_for_new_job.append(\"magmoms.json\")\n",
    "            else:\n",
    "                files_to_transfer_for_new_job.append(\"magmoms.json\")\n",
    "\n",
    "\n",
    "            num_atoms = atoms.get_global_number_of_atoms()\n",
    "\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        if not TEST_no_file_ops:\n",
    "            transfer_job_files_from_old_to_new(\n",
    "                path_i=path_i,\n",
    "                new_path_i=new_path_i,\n",
    "                files_to_transfer_for_new_job=files_to_transfer_for_new_job,\n",
    "                )\n",
    "\n",
    "        # #####################################################\n",
    "        if not TEST_no_file_ops:\n",
    "            dft_params_path_i = os.path.join(\n",
    "                new_path_i,\n",
    "                \"dft-params.json\")\n",
    "            with open(dft_params_path_i, \"r\") as fle:\n",
    "                dft_params_current = json.load(fle)\n",
    "\n",
    "            # Update previous DFT parameters with new ones\n",
    "            dft_params_current.update(dft_params_new)\n",
    "\n",
    "            with open(dft_params_path_i, \"w\") as outfile:\n",
    "                json.dump(dft_params_current, outfile, indent=2)\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i[\"path_i\"] = new_path_i\n",
    "        data_dict_i[\"num_atoms\"] = num_atoms\n",
    "        data_dict_list.append(data_dict_i)\n",
    "\n",
    "        # else:\n",
    "        #     print(\"OUTCAR file wasn't where it should be, probably need rclone\")\n",
    "        #     print(\"name_i:\", name_i)\n",
    "\n",
    "# #########################################################\n",
    "df_sub = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.052 min\n",
      "setup_new_jobs.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"setup_new_jobs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tmp = [print(i) for i in df_sub.path_i.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "# row_data_max_i = df_jobs_data_i.loc[job_id_max_i]\n",
    "\n",
    "# df_jobs_data_i.loc[[\"nitisule_36\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# row_i = df_i.iloc[0]\n",
    "\n",
    "# job_id_max_i = row_i.job_id_max\n",
    "\n",
    "# row_jobs_i = df_jobs.loc[job_id_max_i]\n",
    "# slab_id_i = row_jobs_i.slab_id\n",
    "\n",
    "# row_slab_i = df_slab.loc[slab_id_i]\n",
    "# phase_i = row_slab_i.phase\n",
    "\n",
    "# phase_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_resubmit[df_resubmit.job_id_max == \"pigenipi_49\"]\n",
    "\n",
    "# df_jobs_anal[df_jobs_anal.job_id_max == \"pigenipi_49\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# len(indices_to_stop_running)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
