{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs/run_o_covered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# # from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# # # #########################################################\n",
    "from methods import (\n",
    "    get_df_slab,\n",
    "    get_df_jobs,\n",
    "    )\n",
    "\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "# # #########################################################\n",
    "# from local_methods import (\n",
    "#     # mean_O_metal_coord,\n",
    "#     # calc_wall_time,\n",
    "#     )\n",
    "\n",
    "from dft_workflow_methods import (\n",
    "    get_job_spec_dft_params,\n",
    "    get_job_spec_scheduler_params,\n",
    "    submit_job,\n",
    "    calc_wall_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat2\"  # 'suncat', 'suncat2', 'suncat3'\n",
    "\n",
    "# TEMP\n",
    "dft_batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_slab = get_df_slab()\n",
    "df_slab = df_slab.set_index(\"slab_id\")\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data/dft_jobs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "compenv = os.environ[\"COMPENV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Slabs to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab = df_slab[~df_slab.index.isin(df_jobs.slab_id.tolist())]\n",
    "\n",
    "# #########################################################\n",
    "# Selecting smallest slabs\n",
    "df_slab = df_slab.loc[\n",
    "    df_slab.num_atoms.sort_values()[0:dft_batch_size].index\n",
    "    ]\n",
    "\n",
    "# #########################################################\n",
    "# df_slab = df_slab[df_slab.num_atoms < 40]\n",
    "# # df_slab = df_slab[(df_slab.num_atoms > 50) & (df_slab.num_atoms < 80)]\n",
    "\n",
    "# #########################################################\n",
    "# df_slab = df_slab.sample(n=dft_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the job folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_slab.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    slab_id = row_i.name\n",
    "    bulk_id = row_i.bulk_id\n",
    "    facet = row_i.facet\n",
    "    slab_final = row_i.slab_final\n",
    "    num_atoms = row_i.num_atoms\n",
    "    loop_time = row_i.loop_time\n",
    "    iter_time_i = row_i.iter_time_i\n",
    "    # #####################################################\n",
    "\n",
    "    attempt = 1\n",
    "    rev = 1\n",
    "\n",
    "    path_i = os.path.join(\n",
    "        \"out_data\",\n",
    "        \"dft_jobs\",\n",
    "        bulk_id,\n",
    "        facet,\n",
    "        str(attempt).zfill(2) + \"_attempt\",\n",
    "        \"_\" + str(rev).zfill(2)\n",
    "        )\n",
    "    if not os.path.exists(path_i):\n",
    "        os.makedirs(path_i)\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Copy dft script to job folder\n",
    "    # #####################################################\n",
    "    copyfile(\n",
    "        os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer\"],\n",
    "            \"dft_workflow/dft_scripts/slab_dft.py\"\n",
    "            ),\n",
    "        os.path.join(\n",
    "            path_i,\n",
    "            \"model.py\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    copyfile(\n",
    "        os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer\"],\n",
    "            \"dft_workflow/dft_scripts/slab_dft.py\"\n",
    "            ),\n",
    "        os.path.join(\n",
    "            path_i,\n",
    "            \"slab_dft.py\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    # Copy atoms object to job folder\n",
    "    # #####################################################\n",
    "    slab_final.write(\n",
    "        os.path.join(path_i, \"init.traj\")\n",
    "        )\n",
    "\n",
    "    data_dict_i[\"slab_id\"] = slab_id\n",
    "    data_dict_i[\"bulk_id\"] = bulk_id\n",
    "    data_dict_i[\"facet\"] = facet\n",
    "    data_dict_i[\"slab_final\"] = slab_final\n",
    "    data_dict_i[\"num_atoms\"] = num_atoms\n",
    "    data_dict_i[\"attempt\"] = attempt\n",
    "    data_dict_i[\"rev\"] = rev\n",
    "    data_dict_i[\"path_i\"] = path_i\n",
    "\n",
    "\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "df_jobs_new = pd.DataFrame(data_dict_list)\n",
    "df_jobs_new = df_jobs_new.set_index(\"slab_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs_new\n",
    "\n",
    "# slab_id\n",
    "# bulk_id\n",
    "# facet\n",
    "# slab_final\n",
    "# num_atoms\n",
    "# attempt\n",
    "# rev\n",
    "# path_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning job specific DFT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    slab_id = row_i.name\n",
    "    num_atoms = row_i.num_atoms\n",
    "    path_i =row_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    dft_params_dict = get_job_spec_dft_params(\n",
    "        compenv=compenv,\n",
    "        slac_sub_queue=\"suncat2\",\n",
    "        )\n",
    "\n",
    "    data_dict_i[\"slab_id\"] = slab_id\n",
    "    data_dict_i[\"dft_params\"] = dft_params_dict\n",
    "\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "df_dft_params = pd.DataFrame(data_dict_list)\n",
    "df_dft_params = df_dft_params.set_index(\"slab_id\")\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "# Writing DFT params to job directory\n",
    "for slab_id, row_i in df_dft_params.iterrows():\n",
    "\n",
    "    # #####################################################\n",
    "    dft_params = row_i.dft_params\n",
    "    # #####################################################\n",
    "    row_slab_i = df_jobs_new.loc[slab_id]\n",
    "    path_i = row_slab_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    with open(os.path.join(path_i, \"dft-params.json\"), \"w+\") as fle:\n",
    "        # json.dump(dft_params_dict, fle, indent=2, skipkeys=True)\n",
    "        json.dump(dft_params, fle, indent=2, skipkeys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting initial magnetic moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    # #####################################################\n",
    "    atoms = row_i.slab_final\n",
    "    path_i =row_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    O_magmom=0.2\n",
    "    M_magmom=1.2\n",
    "    magmoms_i = []\n",
    "    for atom in atoms:\n",
    "        if atom.symbol == \"O\":\n",
    "            magmom_i = O_magmom\n",
    "        else:\n",
    "            magmom_i = M_magmom\n",
    "        magmoms_i.append(magmom_i)\n",
    "\n",
    "    data_path = os.path.join(path_i, \"magmoms.json\")\n",
    "    with open(data_path, \"w\") as outfile:\n",
    "        json.dump(magmoms_i, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paths of new jobs:\")\n",
    "tmp = [print(i) for i in df_jobs_new.path_i.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"setup_dft.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some messages for user\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Manually change if statement to True to submit DFT jobs\")\n",
    "# print(\"    search for submit_job(\")\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs\n",
    "\n",
    "# out_dict = get_job_spec_scheduler_params(compenv=compenv)\n",
    "# wall_time_factor = out_dict[\"wall_time_factor\"]\n",
    "\n",
    "# for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "#     # #######################################\n",
    "#     num_atoms = row_i.num_atoms\n",
    "#     path_i =row_i.path_i\n",
    "#     # #######################################\n",
    "\n",
    "#     if False:\n",
    "#         submit_job(\n",
    "#             path_i=path_i,\n",
    "#             num_atoms=num_atoms,\n",
    "#             wall_time_factor=wall_time_factor,\n",
    "#             queue=slac_sub_queue,\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
