{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup initial *O slabs to run\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs/run_o_covered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_slab,\n",
    "    get_df_jobs,\n",
    "    )\n",
    "\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import (\n",
    "    get_job_spec_dft_params,\n",
    "    get_job_spec_scheduler_params,\n",
    "    submit_job,\n",
    "    calc_wall_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat3\"  # 'suncat', 'suncat2', 'suncat3'\n",
    "\n",
    "# COMPENV to submit to\n",
    "# compenv_i = \"slac\"\n",
    "compenv_i = \"sherlock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_slab = get_df_slab()\n",
    "df_slab = df_slab.set_index(\"slab_id\")\n",
    "df_slab_i = df_slab\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# # df_slab_i.loc[\"pumusuma_66\"]\n",
    "# df_slab_i.loc[\"romudini_21\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read `df_slabs_to_run` from `create_slabs.ipynb`, used to mark priority slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bulk_id</th>\n",
       "      <th>facet_str</th>\n",
       "      <th>facet</th>\n",
       "      <th>facet_rank</th>\n",
       "      <th>facet_abs_sum</th>\n",
       "      <th>source</th>\n",
       "      <th>sys_processed</th>\n",
       "      <th>took_too_long_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>9i6ixublcr</td>\n",
       "      <td>31-3</td>\n",
       "      <td>(3, 1, -3)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>xlbfb49wml</td>\n",
       "      <td>3-2-1-1</td>\n",
       "      <td>(3, -2, -1, -1)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>xfvhck9gcs</td>\n",
       "      <td>320</td>\n",
       "      <td>(3, 2, 0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>mrbine8k72</td>\n",
       "      <td>001</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>6qvlcl6iv2</td>\n",
       "      <td>201</td>\n",
       "      <td>(2, 0, 1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>6qvlcl6iv2</td>\n",
       "      <td>301</td>\n",
       "      <td>(3, 0, 1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>6qvlcl6iv2</td>\n",
       "      <td>302</td>\n",
       "      <td>(3, 0, 2)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>6qvlcl6iv2</td>\n",
       "      <td>310</td>\n",
       "      <td>(3, 1, 0)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>xrd</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bulk_id facet_str            facet  facet_rank  facet_abs_sum source  \\\n",
       "211  9i6ixublcr      31-3       (3, 1, -3)         1.0              7    xrd   \n",
       "217  xlbfb49wml   3-2-1-1  (3, -2, -1, -1)         3.0              7    xrd   \n",
       "250  xfvhck9gcs       320        (3, 2, 0)         0.0              5    xrd   \n",
       "256  mrbine8k72       001        (0, 0, 1)         0.0              1    xrd   \n",
       "263  6qvlcl6iv2       201        (2, 0, 1)         0.0              3    xrd   \n",
       "264  6qvlcl6iv2       301        (3, 0, 1)         1.0              4    xrd   \n",
       "265  6qvlcl6iv2       302        (3, 0, 2)         2.0              5    xrd   \n",
       "267  6qvlcl6iv2       310        (3, 1, 0)         4.0              4    xrd   \n",
       "\n",
       "     sys_processed  took_too_long_prev  \n",
       "211           True                True  \n",
       "217           True                True  \n",
       "250           True                True  \n",
       "256           True                True  \n",
       "263           True                True  \n",
       "264           True                True  \n",
       "265           True                True  \n",
       "267           True                True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/creating_slabs\",\n",
    "    \"out_data\")\n",
    "\n",
    "# #########################################################\n",
    "import pickle; import os\n",
    "path_i = os.path.join(\n",
    "    directory,\n",
    "    \"df_slabs_to_run.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_slabs_to_run = pickle.load(fle)\n",
    "# #########################################################\n",
    "\n",
    "\n",
    "indices_not_good = []\n",
    "for i_cnt, row_i in df_slabs_to_run.iterrows():\n",
    "    df = df_slab_i\n",
    "    df = df[\n",
    "        (df[\"bulk_id\"] == row_i.bulk_id) &\n",
    "        (df[\"facet\"] == row_i.facet_str) &\n",
    "        [True for i in range(len(df))]\n",
    "        ]\n",
    "    if df.shape[0] == 0:\n",
    "        indices_not_good.append(i_cnt)\n",
    "\n",
    "\n",
    "        # print(\"Not good\")\n",
    "\n",
    "#         display(\n",
    "#             row_i.to_frame().T\n",
    "#             )\n",
    "#     else:\n",
    "#         print(\"Good\")\n",
    "\n",
    "        # print(row_i.to_frame().T)\n",
    "        # print(row_i)\n",
    "\n",
    "df_slabs_to_run.loc[\n",
    "    indices_not_good\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Slabs to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping slabs that have been previously done\n",
    "df_jobs_i = df_jobs[df_jobs.ads == \"o\"]\n",
    "df_slab_i = df_slab_i.drop(\n",
    "    df_jobs_i.slab_id.unique()\n",
    "    )\n",
    "\n",
    "# Doing only phase 2 slabs for now\n",
    "df_slab_i = df_slab_i[df_slab_i.phase == 2]\n",
    "\n",
    "# #########################################################\n",
    "# Selecting smallest slabs\n",
    "df_slab_i = df_slab_i[df_slab_i.num_atoms < 80]\n",
    "\n",
    "# print(\"Just doing XRD facets for now\")\n",
    "# df_slab_i = df_slab_i[df_slab_i.source == \"xrd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# df_slab_i.loc[\"romudini_21\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering down to best slabs, no layered, all octahedra, 0.3 eV/atom above hull cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_slabs = []\n",
    "for slab_id_i, row_i in df_slab_i.iterrows():\n",
    "    # ####################################################\n",
    "    bulk_id_i = row_i.bulk_id\n",
    "    facet_i = row_i.facet\n",
    "    # ####################################################\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(bulk_id_i, slab_id_i)\n",
    "\n",
    "    df = df_slabs_to_run\n",
    "    df = df[\n",
    "        (df[\"bulk_id\"] == bulk_id_i) &\n",
    "        (df[\"facet_str\"] == facet_i) &\n",
    "        [True for i in range(len(df))]\n",
    "        ]\n",
    "    if df.shape[0] > 0:\n",
    "        # print(\"Good\")\n",
    "        good_slabs.append(slab_id_i)\n",
    "\n",
    "    # elif df.shape[0] == 0:\n",
    "    #     print(\"Bad\")\n",
    "\n",
    "df_slab_i = df_slab_i.loc[\n",
    "    good_slabs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# df_slab_i.loc[\"romudini_21\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# romudini_21 | 2\n",
    "# wafitemi_24 | 2\n",
    "# kapapohe_58 | 2\n",
    "# bekusuvu_00 | 2\n",
    "# pemupehe_18 | 2\n",
    "# hahesegu_39 | 2\n",
    "# migidome_55 | 2\n",
    "# semodave_57 | 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slab_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slab_i = df_slab_i.iloc[0:10]\n",
    "df_slab_i = df_slab_i.iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the job folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/nd919pnr6q/2-21/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/mrbine8k72/100/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/mrbine8k72/010/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/6qvlcl6iv2/100/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/110/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/200/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/111/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/001/01_attempt/_01\n"
     ]
    }
   ],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_slab_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    slab_id = row_i.name\n",
    "    bulk_id = row_i.bulk_id\n",
    "    facet = row_i.facet\n",
    "    slab_final = row_i.slab_final\n",
    "    num_atoms = row_i.num_atoms\n",
    "    loop_time = row_i.loop_time\n",
    "    iter_time_i = row_i.iter_time_i\n",
    "    # #####################################################\n",
    "\n",
    "    attempt = 1\n",
    "    rev = 1\n",
    "\n",
    "\n",
    "    # Checking if job dir exists for other comp. envs. (it shouldn't)\n",
    "    job_exists_in_another_compenv = False\n",
    "    path_already_exists = False\n",
    "    for compenv_j in [\"slac\", \"sherlock\", \"nersc\", ]:\n",
    "        \n",
    "        path_j = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            \"dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs\",\n",
    "            compenv_j,\n",
    "            bulk_id,\n",
    "            facet,\n",
    "            str(attempt).zfill(2) + \"_attempt\",\n",
    "            \"_\" + str(rev).zfill(2)\n",
    "            )\n",
    "        if os.path.exists(path_j) and compenv_j == compenv_i:\n",
    "            path_already_exists = True\n",
    "            print(\"This path already exists\", path_j)\n",
    "\n",
    "        elif os.path.exists(path_j):\n",
    "            job_exists_in_another_compenv = True\n",
    "            print(\"Job exists in another COMPENV\", path_j)\n",
    "\n",
    "    good_to_go = True\n",
    "    if job_exists_in_another_compenv:\n",
    "        good_to_go = False\n",
    "    if path_already_exists:\n",
    "        good_to_go = False\n",
    "\n",
    "\n",
    "    if good_to_go:\n",
    "        path_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            \"dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs\",\n",
    "            compenv_i,\n",
    "            bulk_id,\n",
    "            facet,\n",
    "            str(attempt).zfill(2) + \"_attempt\",\n",
    "            \"_\" + str(rev).zfill(2)\n",
    "            )\n",
    "\n",
    "        print(path_i)\n",
    "        if os.path.exists(path_i):\n",
    "            print(\"TEMP | This path already exists and it shouldn't\", path_i)\n",
    "\n",
    "        if not os.path.exists(path_i):\n",
    "            os.makedirs(path_i)\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        # Copy dft script to job folder\n",
    "        # #####################################################\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/slab_dft.py\"\n",
    "                ),\n",
    "            os.path.join(\n",
    "                path_i,\n",
    "                \"model.py\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/slab_dft.py\"\n",
    "                ),\n",
    "            os.path.join(\n",
    "                path_i,\n",
    "                \"slab_dft.py\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # #####################################################\n",
    "        # Copy atoms object to job folder\n",
    "        # #####################################################\n",
    "        slab_final.write(\n",
    "            os.path.join(path_i, \"init.traj\")\n",
    "            )\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i[\"slab_id\"] = slab_id\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id\n",
    "        data_dict_i[\"facet\"] = facet\n",
    "        data_dict_i[\"slab_final\"] = slab_final\n",
    "        data_dict_i[\"num_atoms\"] = num_atoms\n",
    "        data_dict_i[\"attempt\"] = attempt\n",
    "        data_dict_i[\"rev\"] = rev\n",
    "        data_dict_i[\"path_i\"] = path_i\n",
    "        # #####################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_new = pd.DataFrame(data_dict_list)\n",
    "df_jobs_new = df_jobs_new.set_index(\"slab_id\")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning job specific DFT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    slab_id = row_i.name\n",
    "    num_atoms = row_i.num_atoms\n",
    "    path_i =row_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    dft_params_dict = get_job_spec_dft_params(\n",
    "        compenv=compenv_i,\n",
    "        slac_sub_queue=\"suncat3\",\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"slab_id\"] = slab_id\n",
    "    data_dict_i[\"dft_params\"] = dft_params_dict\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df_dft_params = pd.DataFrame(data_dict_list)\n",
    "df_dft_params = df_dft_params.set_index(\"slab_id\")\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "# Writing DFT params to job directory\n",
    "for slab_id, row_i in df_dft_params.iterrows():\n",
    "    # #####################################################\n",
    "    dft_params = row_i.dft_params\n",
    "    # #####################################################\n",
    "    row_slab_i = df_jobs_new.loc[slab_id]\n",
    "    path_i = row_slab_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    with open(os.path.join(path_i, \"dft-params.json\"), \"w+\") as fle:\n",
    "        json.dump(dft_params, fle, indent=2, skipkeys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting initial magnetic moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    # #####################################################\n",
    "    atoms = row_i.slab_final\n",
    "    path_i =row_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    z_positions = atoms.positions[:, 2]\n",
    "    z_max = z_positions.max()\n",
    "\n",
    "    O_magmom=0.2\n",
    "    M_magmom=0.6\n",
    "    magmoms_i = []\n",
    "    for atom in atoms:\n",
    "        z_pos = atom.position[2]\n",
    "        dist_from_top = z_max - z_pos\n",
    "        # print(z_max - z_pos)\n",
    "\n",
    "        if dist_from_top < 4:\n",
    "            if atom.symbol == \"O\":\n",
    "                magmom_i = O_magmom\n",
    "            else:\n",
    "                magmom_i = M_magmom\n",
    "            magmoms_i.append(magmom_i)\n",
    "        else:\n",
    "            magmoms_i.append(0.)\n",
    "\n",
    "    data_path = os.path.join(path_i, \"magmoms.json\")\n",
    "    with open(data_path, \"w\") as outfile:\n",
    "        json.dump(magmoms_i, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths of new jobs:\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/nd919pnr6q/2-21/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/mrbine8k72/100/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/mrbine8k72/010/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/6qvlcl6iv2/100/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/110/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/200/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/111/01_attempt/_01\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/b583vr8hvw/001/01_attempt/_01\n"
     ]
    }
   ],
   "source": [
    "print(\"Paths of new jobs:\")\n",
    "tmp = [print(i) for i in df_jobs_new.path_i.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "setup_dft.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"setup_dft.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Some messages for user\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Manually change if statement to True to submit DFT jobs\")\n",
    "# print(\"    search for submit_job(\")\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Submit jobs\n",
    "\n",
    "# out_dict = get_job_spec_scheduler_params(compenv=compenv)\n",
    "# wall_time_factor = out_dict[\"wall_time_factor\"]\n",
    "\n",
    "# for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "#     # #######################################\n",
    "#     num_atoms = row_i.num_atoms\n",
    "#     path_i =row_i.path_i\n",
    "#     # #######################################\n",
    "\n",
    "#     if False:\n",
    "#         submit_job(\n",
    "#             path_i=path_i,\n",
    "#             num_atoms=num_atoms,\n",
    "#             wall_time_factor=wall_time_factor,\n",
    "#             queue=slac_sub_queue,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_new\n",
    "\n",
    "# slab_id\n",
    "# bulk_id\n",
    "# facet\n",
    "# slab_final\n",
    "# num_atoms\n",
    "# attempt\n",
    "# rev\n",
    "# path_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# directory = \"out_data/dft_jobs\"\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "# compenv = os.environ[\"COMPENV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_slab_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# lst_0 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "# lst_0[0:5]\n",
    "# lst_0[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ['relovalu_12',\n",
    "#  'hivovaru_77',\n",
    "#  'lawuduni_55',\n",
    "#  'pumisumi_35',\n",
    "#  'gesumule_22',\n",
    "#  'vovumota_03',\n",
    "#  'dafanapa_38',\n",
    "#  'papapesi_26',\n",
    "#  'fukuwevi_91',\n",
    "#  'nuriramu_38',\n",
    "#  'sabedabu_27',\n",
    "#  'votafefa_68',\n",
    "#  'bokawemu_25',\n",
    "#  'fewirefe_11',\n",
    "#  'gifopira_28']\n",
    "\n",
    "# df_slab_i.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_slab_i.shape\n",
    "\n",
    "# df_slab_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Pickling data ###########################################\n",
    "# import os; import pickle\n",
    "# # directory = \"out_data\"\n",
    "# directory = os.path.join(\n",
    "#     os.environ[\"PROJ_irox_oer\"],\n",
    "#     \"workflow/creating_slabs\",\n",
    "#     \"out_data\")\n",
    "# if not os.path.exists(directory): os.makedirs(directory)\n",
    "# with open(os.path.join(directory, \"df_slabs_to_run.pickle\"), \"wb\") as fle:\n",
    "#     df_slabs_to_run = df_to_run\n",
    "#     pickle.dump(df_slabs_to_run, fle)\n",
    "# # #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tmp_list = []\n",
    "# for slab_id_i, row_i in df_slab_i.iterrows():\n",
    "#     tmp = 42\n",
    "\n",
    "#     bulk_id_i = row_i.bulk_id\n",
    "#     facet_i = row_i.facet\n",
    "\n",
    "#     df = df_slabs_to_run\n",
    "#     df = df[\n",
    "#         (df[\"bulk_id\"] == bulk_id_i) &\n",
    "#         (df[\"facet_str\"] == facet_i) &\n",
    "#         # (df[\"\"] == \"\") &\n",
    "#         [True for i in range(len(df))]\n",
    "#         ]\n",
    "#     if df.shape[0] > 0:\n",
    "#         tmp_list.append(row_i)\n",
    "\n",
    "# df_tmp = pd.DataFrame(\n",
    "#     tmp_list\n",
    "#     )\n",
    "\n",
    "# df_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # row_i\n",
    "\n",
    "# facet_i"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
