{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pares Job Data\n",
    "---\n",
    "\n",
    "Applied job analaysis scripts to job directories and compiles."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import reorder_df_columns\n",
    "from vasp.vasp_methods import read_incar, get_irr_kpts_from_outcar\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_data_clusters,\n",
    "    )\n",
    "\n",
    "from local_methods import (\n",
    "    parse_job_err,\n",
    "    parse_finished_file,\n",
    "    parse_job_state,\n",
    "    is_job_submitted,\n",
    "    get_isif_from_incar,\n",
    "    get_number_of_ionic_steps,\n",
    "    analyze_oszicar,\n",
    "    read_data_pickle,\n",
    "    get_final_atoms,\n",
    "    get_init_atoms,\n",
    "    get_magmoms_from_job,\n",
    "    get_ads_from_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun job parsing on all existing jobs, needed if job parsing methods are updated\n",
    "# Check if error is caused by turning this on\n",
    "# rerun_all_jobs = True\n",
    "rerun_all_jobs = False\n",
    "\n",
    "verbose = False\n",
    "# verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "if rerun_all_jobs:\n",
    "    print(\"rerun_all_jobs=True\")\n",
    "    # print(\"Remember to turn off this flag under normal operation\")\n",
    "\n",
    "compenv = os.environ.get(\"COMPENV\", None)\n",
    "\n",
    "PROJ_irox_oer_gdrive = os.environ[\"PROJ_irox_oer_gdrive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compenv != \"wsl\":\n",
    "    rerun_all_jobs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs(exclude_wsl_paths=True)\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data_clusters = get_df_jobs_data_clusters()\n",
    "\n",
    "\n",
    "from methods import get_df_jobs_data\n",
    "df_jobs_data_old = get_df_jobs_data(exclude_wsl_paths=True, drop_cols=False)\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "# Checking if in local env\n",
    "if compenv == \"wsl\":\n",
    "    df_jobs_i = df_jobs\n",
    "else:\n",
    "    df_jobs_i = df_jobs[df_jobs.compenv == compenv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting job state loop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    job_id = row_i.job_id\n",
    "    att_num = row_i.att_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[\n",
    "        df_jobs_paths.compenv == compenv_i]\n",
    "    row_jobs_paths_i = df_jobs_paths_i.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    gdrive_path = row_jobs_paths_i.gdrive_path\n",
    "    path_job_root_w_att_rev = row_jobs_paths_i.path_job_root_w_att_rev\n",
    "    # #####################################################\n",
    "\n",
    "    data_dict_i[\"job_id\"] = job_id\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"att_num\"] = att_num\n",
    "\n",
    "    if compenv == \"wsl\":\n",
    "        path_full_i = os.path.join(\n",
    "            PROJ_irox_oer_gdrive,\n",
    "            gdrive_path)\n",
    "    else:\n",
    "        path_full_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer\"],\n",
    "            path_job_root_w_att_rev)\n",
    "\n",
    "    # #################################################\n",
    "    job_state_i = parse_job_state(path_full_i)\n",
    "    data_dict_i.update(job_state_i)\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #################################################\n",
    "\n",
    "\n",
    "df_jobs_state = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP\n",
    "# print(\"REMOVE THIS, THIS SHOULD NOT STAY!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "# df_jobs_i = df_jobs_i.loc[[\"guganono_69\"]]\n",
    "\n",
    "# df_jobs_i = df_jobs_i.iloc[0:10]\n",
    "# df_jobs_i = df_jobs_i.iloc[0:4]\n",
    "\n",
    "# df_jobs_i = df_jobs_i.loc[[\"voburula_03\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP\n",
    "# print(\"TEMP filtering df for testing\")\n",
    "# df_jobs_i = df_jobs_i.loc[[\"vurabamu_02\"]]\n",
    "\n",
    "# TEMP\n",
    "# print(\"TEMP | Filtering data\")\n",
    "# # df_jobs_i = df_jobs_i.loc[[\"fusoreva_23\"]]\n",
    "# df_jobs_i = df_jobs_i.loc[[\"wipuhite_59\"]]\n",
    "\n",
    "# df_jobs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_from_clusters = []\n",
    "rows_from_clusters = []\n",
    "rows_from_prev_df = []\n",
    "data_dict_list = []\n",
    "for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    bulk_id = row_i.bulk_id\n",
    "    slab_id = row_i.slab_id\n",
    "    job_id = row_i.job_id\n",
    "    facet = row_i.facet\n",
    "    ads = row_i.ads\n",
    "    compenv_i = row_i.compenv\n",
    "    compenv_i = row_i.compenv\n",
    "    att_num = row_i.att_num\n",
    "    rev_num = row_i.rev_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[\n",
    "        df_jobs_paths.compenv == compenv_i]\n",
    "    row_jobs_paths_i = df_jobs_paths_i.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    path_job_root_w_att_rev = row_jobs_paths_i.path_job_root_w_att_rev\n",
    "    gdrive_path = row_jobs_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_clusters_i = df_jobs_data_clusters[\n",
    "        df_jobs_data_clusters.compenv == compenv_i]\n",
    "    # #####################################################\n",
    "\n",
    "    # run_job_i = True\n",
    "    # job_grabbed_from_clusters = False\n",
    "    # if job_id_i in df_jobs_data_clusters_i.index:\n",
    "    #     row_cluster_i = df_jobs_data_clusters_i.loc[job_id_i]\n",
    "\n",
    "    #     if not rerun_all_jobs:\n",
    "    #         run_job_i = False\n",
    "    #         job_grabbed_from_clusters = True\n",
    "    #         # indices_from_clusters.append(job_id_i)\n",
    "    #         rows_from_clusters.append(row_cluster_i)\n",
    "\n",
    "    run_job_i = True\n",
    "    job_grabbed_from_clusters = False\n",
    "    job_grabbed_from_prev_df = False\n",
    "    if not rerun_all_jobs:\n",
    "\n",
    "        if job_id_i in df_jobs_data_clusters_i.index:\n",
    "            if verbose:\n",
    "                print(job_id_i, \"Grabbing from df_jobs_data_clusters\")\n",
    "            run_job_i = False\n",
    "            job_grabbed_from_clusters = True\n",
    "\n",
    "            row_cluster_i = df_jobs_data_clusters_i.loc[job_id_i]\n",
    "            rows_from_clusters.append(row_cluster_i)\n",
    "            # indices_from_clusters.append(job_id_i)\n",
    "\n",
    "        # if not job_grabbed_from_clusters and job_id_i in df_jobs_data_old.index:\n",
    "        elif job_id_i in df_jobs_data_old.index:\n",
    "            if verbose:\n",
    "                print(job_id_i, \"Grabbing from prev df_jobs_data\")\n",
    "            run_job_i = False\n",
    "            job_grabbed_from_clusters = True\n",
    "\n",
    "            row_from_prev_df = df_jobs_data_old.loc[job_id_i]\n",
    "            rows_from_prev_df.append(row_from_prev_df)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(job_id_i, \"Failed to grab job data from elsewhere\")\n",
    "\n",
    "\n",
    "\n",
    "    if compenv == \"wsl\":\n",
    "        path_full_i = os.path.join(\n",
    "            PROJ_irox_oer_gdrive,\n",
    "            gdrive_path)\n",
    "    else:\n",
    "        path_full_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer\"],\n",
    "            path_job_root_w_att_rev)\n",
    "\n",
    "    path_exists = False\n",
    "    my_file = Path(path_full_i)\n",
    "    if my_file.is_dir():\n",
    "        path_exists = True        \n",
    "\n",
    "    if run_job_i and path_exists:\n",
    "        if verbose:\n",
    "            print(\"running job\")\n",
    "\n",
    "        data_dict_i[\"facet\"] = facet\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id\n",
    "        data_dict_i[\"slab_id\"] = slab_id\n",
    "        data_dict_i[\"ads\"] = ads\n",
    "        data_dict_i[\"job_id\"] = job_id\n",
    "        data_dict_i[\"compenv\"] = compenv_i\n",
    "        data_dict_i[\"att_num\"] = att_num\n",
    "        data_dict_i[\"rev_num\"] = rev_num\n",
    "\n",
    "        # #################################################\n",
    "        job_err_out_i = parse_job_err(path_full_i, compenv=compenv_i)\n",
    "        finished_i = parse_finished_file(path_full_i)\n",
    "        job_state_i = parse_job_state(path_full_i)\n",
    "        job_submitted_i = is_job_submitted(path_full_i)\n",
    "        isif_i = get_isif_from_incar(path_full_i)\n",
    "        num_steps = get_number_of_ionic_steps(path_full_i)\n",
    "        oszicar_anal = analyze_oszicar(path_full_i)\n",
    "        incar_params = read_incar(path_full_i, verbose=verbose)\n",
    "        irr_kpts = get_irr_kpts_from_outcar(path_full_i)\n",
    "        pickle_data = read_data_pickle(path_full_i)\n",
    "        init_atoms = get_init_atoms(path_full_i)\n",
    "        final_atoms = get_final_atoms(path_full_i)\n",
    "        magmoms_i = get_magmoms_from_job(path_full_i)\n",
    "        # #################################################\n",
    "\n",
    "        # path_rel_to_proj_i = row_jobs_paths_i.path_rel_to_proj\n",
    "        # ads_i = get_ads_from_path(path_job_root_w_att_rev)\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_i.update(job_err_out_i)\n",
    "        data_dict_i.update(finished_i)\n",
    "        data_dict_i.update(job_state_i)\n",
    "        data_dict_i.update(job_submitted_i)\n",
    "        data_dict_i.update(isif_i)\n",
    "        data_dict_i.update(num_steps)\n",
    "        data_dict_i.update(oszicar_anal)\n",
    "        data_dict_i.update(pickle_data)\n",
    "        data_dict_i[\"incar_params\"] = incar_params\n",
    "        data_dict_i[\"irr_kpts\"] = irr_kpts\n",
    "        data_dict_i[\"init_atoms\"] = init_atoms\n",
    "        data_dict_i[\"final_atoms\"] = final_atoms\n",
    "        data_dict_i[\"magmoms\"] = magmoms_i\n",
    "        # data_dict_i[\"ads_i\"] = ads_i\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(\"\")\n",
    "\n",
    "    elif run_job_i and not path_exists:\n",
    "        print(\"A job needed to be processed but couldn't be found locally, or wasn't processed on the cluster\")\n",
    "        print(gdrive_path)\n",
    "        print(job_id_i)\n",
    "\n",
    "        \n",
    "\n",
    "df_jobs_data = pd.DataFrame(data_dict_list)\n",
    "df_jobs_data_clusters_tmp = pd.DataFrame(rows_from_clusters)\n",
    "df_jobs_data_from_prev = pd.DataFrame(rows_from_prev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(\"df_jobs_data.shape:\", df_jobs_data.shape[0])\n",
    "    print(\"df_jobs_data_clusters_tmp.shape:\", df_jobs_data_clusters_tmp.shape[0])\n",
    "    print(\"df_jobs_data_from_prev.shape:\", df_jobs_data_from_prev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs_data_clusters_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataframe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_jobs_data.shape[0] > 0:\n",
    "    df_jobs_data = reorder_df_columns([\"bulk_id\", \"slab_id\", \"job_id\", \"facet\", ], df_jobs_data)\n",
    "\n",
    "    # Set index to job_id\n",
    "    df_jobs_data = df_jobs_data.set_index(\"job_id\", drop=False)\n",
    "\n",
    "\n",
    "df_jobs_data_0 = df_jobs_data\n",
    "\n",
    "# Combine rows processed here with those already processed in the cluster\n",
    "df_jobs_data = pd.concat([\n",
    "    df_jobs_data_clusters_tmp,\n",
    "    df_jobs_data_0,\n",
    "    df_jobs_data_from_prev,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grabbing `job_state` column from `df_jobs_data_clusters`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_i = df_jobs_data\n",
    "df_i[\"unique_key\"] = list(zip(df_i[\"compenv\"], df_i[\"job_id\"], df_i[\"att_num\"], ))\n",
    "df_i = df_i.set_index(\"unique_key\", drop=False)\n",
    "df_jobs_data = df_i\n",
    "\n",
    "# print(\"df_jobs_data.shape:\", df_jobs_data.shape)\n",
    "\n",
    "# #########################################################\n",
    "df_i = df_jobs_data_clusters\n",
    "df_i[\"unique_key\"] = list(zip(df_i[\"compenv\"], df_i[\"job_id\"], df_i[\"att_num\"], ))\n",
    "df_i = df_i.set_index(\"unique_key\", drop=False)\n",
    "df_jobs_data_clusters = df_i\n",
    "\n",
    "# #########################################################\n",
    "df_i = df_jobs_state\n",
    "df_i[\"unique_key\"] = list(zip(df_i[\"compenv\"], df_i[\"job_id\"], df_i[\"att_num\"], ))\n",
    "df_i = df_i.set_index(\"unique_key\", drop=False)\n",
    "df_jobs_state = df_i\n",
    "\n",
    "df_jobs_state_i = df_jobs_state.drop(columns=[\"compenv\", \"job_id\", \"att_num\"])\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "if compenv != \"wsl\":\n",
    "    df1 = df_jobs_data.drop(columns=[\"job_state\"])\n",
    "    df2 = df_jobs_state_i.job_state\n",
    "\n",
    "    df_jobs_data = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "    df_jobs_data = df_jobs_data.set_index(\"job_id\", drop=False)\n",
    "\n",
    "# #########################################################\n",
    "if compenv == \"wsl\":\n",
    "\n",
    "    tmp = df_jobs_data.index.difference(df_jobs_data_clusters.index)\n",
    "    mess_i = \"Must be no differencec between df_jobs_data and df_jobs_data_clusters\"\n",
    "    mess_i += \"\\n\" + \"Usually this means you must rerun scripts on cluster\"\n",
    "#     assert len(tmp) == 0, mess_i\n",
    "\n",
    "    mess_i = \"Must be equal\"\n",
    "#     assert df_jobs_data.shape[0] == df_jobs_data_clusters.shape[0], mess_i\n",
    "\n",
    "    # #########################################################\n",
    "    df1 = df_jobs_data.drop(columns=[\"job_state\"])\n",
    "    df2 = df_jobs_data_clusters.job_state\n",
    "\n",
    "    # df_jobs_data = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    # df_jobs_data = pd.concat([df1, df2], axis=1, join=\"inner\")\n",
    "    # df_jobs_data = pd.concat([df1, df2], axis=1, )\n",
    "\n",
    "    df_jobs_data = pd.concat([\n",
    "        df1,\n",
    "        df2.loc[df2.index.intersection(df1.index)]\n",
    "        # df2,\n",
    "        ], axis=1, )\n",
    "\n",
    "    df_jobs_data = df_jobs_data.set_index(\"job_id\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(\"df1.shape:\", df1.shape[0])\n",
    "    print(\"df2.shape:\", df2.shape[0])\n",
    "    print(\"\")\n",
    "\n",
    "keys_missing_from_df2 = []\n",
    "for key_i in df1.index:\n",
    "    key_in_df = key_i in df2.index\n",
    "    if not key_in_df:\n",
    "        keys_missing_from_df2.append(key_i)\n",
    "if verbose:\n",
    "    print(\"len(keys_missing_from_df2)\", len(keys_missing_from_df2))\n",
    "\n",
    "\n",
    "keys_missing_from_df1 = []\n",
    "for key_i in df2.index:\n",
    "    key_in_df = key_i in df1.index\n",
    "    if not key_in_df:\n",
    "        keys_missing_from_df1.append(key_i)\n",
    "if verbose:\n",
    "    print(\"len(keys_missing_from_df1)\", len(keys_missing_from_df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map current `df_job_ids` to `df_jobs_data_clusters`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting DFT electronic energy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def method(row_i, argument_0, optional_arg=None):\n",
    "def method(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # #####################################################\n",
    "    final_atoms_i = row_i.final_atoms\n",
    "    # #####################################################\n",
    "    # print(row_i)\n",
    "\n",
    "    if final_atoms_i is not None:\n",
    "        pot_e_i = final_atoms_i.get_potential_energy()\n",
    "    else:\n",
    "        pot_e_i = None\n",
    "\n",
    "    return(pot_e_i)\n",
    "\n",
    "df_jobs_data[\"pot_e\"] = df_jobs_data.apply(method, axis=1)\n",
    "# df_jobs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write `df_jobs_data` to file"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_processing\",\n",
    "    \"out_data\")\n",
    "\n",
    "pre_path = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_processing\")\n",
    "\n",
    "if compenv == \"wsl\":\n",
    "    file_name_i = \"df_jobs_data.pickle\"\n",
    "    path_i = os.path.join(pre_path, directory, file_name_i)\n",
    "else:\n",
    "    file_name_i = \"df_jobs_data_\" + compenv + \".pickle\"\n",
    "    path_i = os.path.join(pre_path, directory, file_name_i)\n",
    "\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_jobs_data, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run rclone commnad (if in cluster) to sync `df_jobs_data` to Dropbox"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_i = path_i\n",
    "\n",
    "db_path = os.path.join(\n",
    "    \"01_norskov/00_git_repos/PROJ_IrOx_OER\",\n",
    "    \"dft_workflow/job_processing/out_data\" ,\n",
    "    file_name_i)\n",
    "\n",
    "rclone_remote = os.environ.get(\"rclone_dropbox\", \"raul_dropbox\")\n",
    "bash_comm = \"rclone copyto \" + file_path_i + \" \" + rclone_remote + \":\" + db_path\n",
    "\n",
    "if compenv != \"wsl\":\n",
    "    if verbose:\n",
    "        print(\"Running rclone command\")\n",
    "        print(\"bash_comm:\", bash_comm)\n",
    "    os.system(bash_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing dataframe, rereading from method"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_jobs_data\n",
    "\n",
    "df_jobs_data_new = get_df_jobs_data(exclude_wsl_paths=True)\n",
    "df_jobs_data_new.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"parse_job_data.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_ids =  [\n",
    "#     'wulumoha_81',\n",
    "#     'kefasowu_80',\n",
    "#     'supibepa_48',\n",
    "#     'kefadusu_22',\n",
    "#     'butapime_67',\n",
    "#     'lerosove_43',\n",
    "#     'kosivele_15',\n",
    "#     'bomowoto_88',\n",
    "#     'buwinalo_86',\n",
    "#     'tonipita_82',\n",
    "#     'lunosahi_89',\n",
    "#     'dirigufo_28',\n",
    "#     'dofusoba_54',\n",
    "#     'pufanime_49',\n",
    "#     'sebitubi_78',\n",
    "#     'betarara_84',\n",
    "#     'novadesu_38',\n",
    "#     'tuvavali_48',\n",
    "#     'mekififo_83',\n",
    "#     'weratovu_37',\n",
    "#     'mowavuna_20',\n",
    "#     'ronafaki_75',\n",
    "#     'higewedo_88',\n",
    "#     'wodurowa_29',\n",
    "#     'kilalawi_94',\n",
    "#     'kuvapabo_66',\n",
    "#     'dinibone_46',\n",
    "#     'kavuvuhe_72',\n",
    "#     'kusewage_44',\n",
    "#     'kufuwudi_46',\n",
    "#     ]\n",
    "\n",
    "# df_jobs_data.loc[job_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# final_atoms_list = df_jobs_data.final_atoms.tolist()\n",
    "\n",
    "# final_atoms_list = [i for i in final_atoms_list if i != None]\n",
    "# for final_atoms_i in final_atoms_list:\n",
    "#     tmp = 42\n",
    "\n",
    "\n",
    "# final_atoms_i.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# get_df_jobs_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {},
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# df_jobs_i = df_jobs_i[df_jobs_i.compenv != \"nersc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data_clusters.loc[\n",
    "#     (df_jobs_data_clusters.compenv == \"sherlock\") & \\\n",
    "#     (df_jobs_data_clusters.ads == \"oh\") & \\\n",
    "#     (df_jobs_data_clusters.facet == \"010\") & \\\n",
    "#     (df_jobs_data_clusters.slab_id == \"fogalonu_46\") & \\\n",
    "#     [True for i in range(len(df_jobs_data_clusters))]\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data_clusters_tmp.loc[\"fusoreva_23\"]\n",
    "# df_jobs_data.loc[\"fusoreva_23\"]\n",
    "# df_jobs_data_from_prev.loc[\"fusoreva_23\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# job_id_i = \"vurabamu_02\"\n",
    "\n",
    "# job_id_i in df_jobs_data_clusters_i.index\n",
    "# job_id_i in df_jobs_data_old.index\n",
    "\n",
    "# df_jobs_data_clusters_i.loc[\"vurabamu_02\"]\n",
    "\n",
    "# df_jobs_data_from_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data_clusters.loc[\n",
    "#     [('nersc', 'bokedolu_84', 1)]\n",
    "#     ]\n",
    "\n",
    "\n",
    "# # bokedolu_84\n",
    "# # df_jobs.loc[\"bokedolu_84\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# keys_missing_from_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {},
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# # df_jobs_data\n",
    "# # df2\n",
    "# # df1.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# pd.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
