{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing job sets (everything within a `02_attempt` dir for example)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_processing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import dictdiffer\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_paths,\n",
    "    cwd,\n",
    "    )\n",
    "\n",
    "from dft_workflow_methods import is_job_understandable, job_decision\n",
    "from dft_workflow_methods import transfer_job_files_from_old_to_new\n",
    "from dft_workflow_methods import is_job_compl_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# verbose = True\n",
    "\n",
    "TEST_no_file_ops = True  # True if just testing around, False for production mode\n",
    "# TEST_no_file_ops = False\n",
    "\n",
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat2\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_df_jobs()\n",
    "if verbose:\n",
    "    print(\"df_jobs.shape:\", 2 * \"\\t\", df_jobs.shape)\n",
    "\n",
    "df_jobs_data = get_df_jobs_data(drop_cols=False)\n",
    "if verbose:\n",
    "    print(\"df_jobs_data.shape:\", 1 * \"\\t\", df_jobs_data.shape)\n",
    "    \n",
    "df_jobs_paths = get_df_jobs_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique jobs : (103, 11)\n",
      "^^ Only counting hightest rev_num\n"
     ]
    }
   ],
   "source": [
    "group_cols = [\"compenv\", \"slab_id\", \"att_num\", \"ads\", \"active_site\"]\n",
    "# group_cols = [\"compenv\", \"slab_id\", \"att_num\", ]\n",
    "grouped = df_jobs.groupby(group_cols)\n",
    "max_job_row_list = []\n",
    "data_dict_list = []\n",
    "for name, group in grouped:\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    max_job = group[group.rev_num == group.rev_num.max()]\n",
    "    max_job_row_list.append(max_job.iloc[0])\n",
    "\n",
    "    compenv_i = name[0]\n",
    "    slab_id_i = name[1]\n",
    "    att_num_i = name[2]\n",
    "\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"att_num\"] = att_num_i\n",
    "    data_dict_i[\"group_key\"] = name\n",
    "\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "df_max_group_keys = pd.DataFrame(data_dict_list)\n",
    "df_jobs_max = pd.DataFrame(max_job_row_list)\n",
    "\n",
    "if verbose:\n",
    "    print(\"Number of unique jobs :\", df_jobs_max.shape)\n",
    "    print(\"^^ Only counting hightest rev_num\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering `df_jobs` by rows that are present in `df_jobs_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These job_ids weren't in df_jobs_data:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "df_jobs_i = df_jobs.loc[\n",
    "    df_jobs.index.intersection(df_jobs_data.index)\n",
    "    ]\n",
    "\n",
    "if verbose:\n",
    "    print(\n",
    "        \"These job_ids weren't in df_jobs_data:\",\n",
    "        \"\\n\",\n",
    "        df_jobs.index.difference(df_jobs_data.index).tolist(), sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "group_cols = [\"compenv\", \"slab_id\", \"att_num\", \"ads\", \"active_site\"]\n",
    "grouped = df_jobs_i.groupby(group_cols)\n",
    "for name, group in grouped:\n",
    "\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    if verbose:\n",
    "        print(40 * \"#\")\n",
    "        print(\"name:\", name)\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name[0]\n",
    "    slab_id = name[1]\n",
    "    att_num = name[2] \n",
    "    ads_i = name[3]\n",
    "    active_site_i = name[4]\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    max_job = group[group.rev_num == group.rev_num.max()]\n",
    "    assert max_job.shape[0] == 1, \"Must only have 1 there\"\n",
    "    row_max_i = max_job.iloc[0]\n",
    "    # #####################################################\n",
    "    job_id_max_i = row_max_i.job_id\n",
    "    # path_short = row_max_i.path_short\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "    row_paths_max_i = df_jobs_paths_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    path_job_root_w_att_rev = row_paths_max_i.path_job_root_w_att_rev\n",
    "    path_rel_to_proj = row_paths_max_i.path_rel_to_proj\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "    row_data_max_i = df_jobs_data_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    timed_out = row_data_max_i.timed_out\n",
    "    completed = row_data_max_i.completed\n",
    "    ediff_conv_reached = row_data_max_i.ediff_conv_reached\n",
    "    brmix_issue = row_data_max_i.brmix_issue\n",
    "    num_nonconv_scf = row_data_max_i.num_nonconv_scf\n",
    "    num_conv_scf = row_data_max_i.num_conv_scf\n",
    "    true_false_ratio = row_data_max_i.true_false_ratio\n",
    "    frac_true = row_data_max_i.frac_true\n",
    "    error = row_data_max_i.error\n",
    "    error_type = row_data_max_i.error_type\n",
    "    job_state = row_data_max_i.job_state\n",
    "    incar_params = row_data_max_i.incar_params\n",
    "    # #####################################################\n",
    "    if incar_params is not None:\n",
    "        ispin = incar_params.get(\"ISPIN\", None)\n",
    "    else:\n",
    "        ispin = None\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    job_completely_done = is_job_compl_done(\n",
    "        ispin=ispin, completed=completed)\n",
    "\n",
    "    # #####################################################\n",
    "    job_understandable = is_job_understandable(\n",
    "        timed_out=timed_out, completed=completed, error=error,\n",
    "        job_state=job_state, )\n",
    "    # #####################################################\n",
    "    job_decision_i = job_decision(\n",
    "        error=error, error_type=error_type,\n",
    "        timed_out=timed_out, completed=completed,\n",
    "        job_understandable=job_understandable, ediff_conv_reached=ediff_conv_reached,\n",
    "        incar_params=incar_params, brmix_issue=brmix_issue,\n",
    "        num_nonconv_scf=num_nonconv_scf, num_conv_scf=num_conv_scf,\n",
    "        true_false_ratio=true_false_ratio, frac_true=frac_true, job_state=job_state,\n",
    "        job_completely_done=job_completely_done, )\n",
    "    decision_i = job_decision_i[\"decision\"]\n",
    "    dft_params_i = job_decision_i[\"dft_params\"]\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"ads\"] = ads_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "\n",
    "    data_dict_i[\"job_understandable\"] = job_understandable\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id\n",
    "    data_dict_i[\"att_num\"] = att_num\n",
    "    data_dict_i[\"job_id_max\"] = job_id_max_i\n",
    "    data_dict_i[\"path_rel_to_proj\"] = path_rel_to_proj\n",
    "    # data_dict_i[\"path_short\"] = path_short\n",
    "    data_dict_i[\"timed_out\"] = timed_out\n",
    "    data_dict_i[\"completed\"] = completed\n",
    "    data_dict_i[\"brmix_issue\"] = brmix_issue\n",
    "    data_dict_i[\"decision\"] = decision_i\n",
    "    data_dict_i[\"dft_params_new\"] = dft_params_i\n",
    "    data_dict_i[\"job_completely_done\"] = job_completely_done\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = pd.DataFrame(data_dict_list)\n",
    "# df_jobs_anal = df_jobs_anal.set_index(\"job_id\", drop=False)\n",
    "# df_jobs_anal = df_jobs_anal.sort_values([\"compenv\", \"path_short\"])\n",
    "\n",
    "df_jobs_anal = df_jobs_anal.sort_values([\"compenv\", \"slab_id\", \"path_rel_to_proj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordering `df_jobs_anal` and setting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc_modules.pandas_methods import reorder_df_columns\n",
    "\n",
    "col_order_list = [\n",
    "    \"compenv\",\n",
    "    \"slab_id\",\n",
    "    \"att_num\",\n",
    "    \"ads\",\n",
    "    \"active_site\",\n",
    "    \"job_id_max\",\n",
    "\n",
    "    \"path_short\",\n",
    "\n",
    "    \"timed_out\",\n",
    "    \"completed\",\n",
    "    \"brmix_issue\",\n",
    "    \"job_understandable\",\n",
    "\n",
    "    \"decision\",\n",
    "    \"dft_params_new\",\n",
    "\n",
    "    \"path_rel_to_proj\",\n",
    "    ]\n",
    "df_jobs_anal = reorder_df_columns(col_order_list, df_jobs_anal)\n",
    "df_jobs_anal = df_jobs_anal.drop(columns=[\"path_rel_to_proj\", ])\n",
    "\n",
    "# #########################################################\n",
    "# Setting index\n",
    "index_keys = [\"compenv\", \"slab_id\", \"ads\", \"active_site\", \"att_num\"]\n",
    "df_jobs_anal = df_jobs_anal.set_index(index_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_processing/out_data\")\n",
    "file_name_i = \"df_jobs_anal.pickle\"\n",
    "path_i = os.path.join(directory, file_name_i)\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_jobs_anal, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "mask_list = []\n",
    "for i in df_jobs_anal.decision.tolist():\n",
    "    if \"nothing\" in i:\n",
    "        mask_list.append(True)\n",
    "    else:\n",
    "        mask_list.append(False)\n",
    "\n",
    "df_nothing = df_jobs_anal[mask_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mask_list = []\n",
    "for i in df_jobs_anal.decision.tolist():\n",
    "    if \"All done\" in i:\n",
    "        mask_list.append(False)\n",
    "    elif \"RUNNING\" in i:\n",
    "        mask_list.append(False)\n",
    "    else:\n",
    "        mask_list.append(True)\n",
    "\n",
    "# df_jobs_anal[mask_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "analyse_jobs.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"analyse_jobs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal[df_jobs_anal.job_completely_done == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"TEMP\")\n",
    "\n",
    "# df_jobs_i = df_jobs_i.loc[[\n",
    "\n",
    "#     # \"nebokipa_96\",\n",
    "#     # \"hotefihe_55\",\n",
    "#     # \"koduvaka_72\",\n",
    "\n",
    "#     \"kefadusu_22\",\n",
    "\n",
    "#     ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # df_jobs_anal[\n",
    "# #     (df_jobs_anal.compenv == \"sherlock\") & \\\n",
    "# #     (df_jobs_anal.slab_id == \"putarude_21\")\n",
    "# #     ]\n",
    "\n",
    "# df_jobs_anal_i = df_jobs_anal\n",
    "\n",
    "# var = \"sherlock\"\n",
    "# df_jobs_anal_i = df_jobs_anal_i.query('compenv == @var')\n",
    "\n",
    "# var = \"putarude_21\"\n",
    "# df_jobs_anal_i = df_jobs_anal_i.query('slab_id == @var')\n",
    "\n",
    "# df_jobs_anal_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal\n",
    "\n",
    "#         error=error, error_type=error_type,\n",
    "#         timed_out=timed_out, completed=completed,\n",
    "#         job_understandable=job_understandable, ediff_conv_reached=ediff_conv_reached,\n",
    "#         incar_params=incar_params, brmix_issue=brmix_issue,\n",
    "#         num_nonconv_scf=num_nonconv_scf, num_conv_scf=num_conv_scf,\n",
    "#         true_false_ratio=true_false_ratio, frac_true=frac_true, job_state=job_state,\n",
    "#         job_completely_done=job_completely_done, )\n",
    "\n",
    "# job_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"TEMP | Just for testing\")\n",
    "\n",
    "# save_dict = dict(\n",
    "#     error=error, error_type=error_type,\n",
    "#     timed_out=timed_out, completed=completed,\n",
    "#     job_understandable=job_understandable, ediff_conv_reached=ediff_conv_reached,\n",
    "#     incar_params=incar_params, brmix_issue=brmix_issue,\n",
    "#     num_nonconv_scf=num_nonconv_scf, num_conv_scf=num_conv_scf,\n",
    "#     true_false_ratio=true_false_ratio, frac_true=frac_true, job_state=job_state,\n",
    "#     job_completely_done=job_completely_done,\n",
    "#     )\n",
    "\n",
    "# save_object = save_dict\n",
    "\n",
    "# # Pickling data ###########################################\n",
    "# import os; import pickle\n",
    "# directory = os.path.join(\n",
    "#     os.environ[\"HOME\"],\n",
    "#     \"__temp__\")\n",
    "# if not os.path.exists(directory): os.makedirs(directory)\n",
    "# path_i = os.path.join(directory, \"temp_data.pickle\")\n",
    "# with open(path_i, \"wb\") as fle:\n",
    "#     pickle.dump(save_object, fle)\n",
    "# # #########################################################\n",
    "\n",
    "# # #########################################################\n",
    "# import pickle; import os\n",
    "# directory = os.path.join(\n",
    "#     os.environ[\"HOME\"],\n",
    "#     \"__temp__\")\n",
    "# path_i = os.path.join(directory, \"temp_data.pickle\")\n",
    "# with open(path_i, \"rb\") as fle:\n",
    "#     data = pickle.load(fle)\n",
    "# # #########################################################"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
