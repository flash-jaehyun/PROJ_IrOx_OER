{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean DFT Jobs\n",
    "---\n",
    "\n",
    "Run in computer cluster to perform a variety of job clean and processing\n",
    "\n",
    "Currently the following things are done:\n",
    "\n",
    "1. Process large `job.out` files, if `job.out` is larger than `job_out_size_limit` than creates new `job.out.new` file removes middle section of file and leaves behind the beginning and end of the original file\n",
    "1. Rclone copy the job directories to the Stanford Google Drive\n",
    "\n",
    "## TODO\n",
    "* Remove large files if they are newer revisions (Only time you need large VASP files are when starting a new job and therefore need WAVECAR or charge files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_processing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_anal,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    cwd, process_large_job_out,\n",
    "    rclone_sync_job_dir,\n",
    "    parse_job_state,\n",
    "    local_dir_matches_remote,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import (\n",
    "    get_other_job_ids_in_set,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "job_out_size_limit = 5  # MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compenv = os.environ.get(\"COMPENV\", None)\n",
    "\n",
    "proj_dir = os.environ.get(\"PROJ_irox_oer\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs = get_df_jobs(exclude_wsl_paths=False)\n",
    "\n",
    "if compenv != \"wsl\":\n",
    "    df_i = df_jobs[df_jobs.compenv == compenv]\n",
    "else:\n",
    "    df_i = df_jobs\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv]\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "if verbose:\n",
    "    print(60 * \"-\")\n",
    "    print(\"Directories being parsed\")\n",
    "    tmp = [print(i) for i in df_jobs_paths_i.path_rel_to_proj.tolist()]\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compenv != \"wsl\":\n",
    "\n",
    "    iterator = tqdm(df_i.index.tolist(), desc=\"1st loop\")\n",
    "    for index_i in iterator:\n",
    "        # #####################################################\n",
    "        row_i = df_i.loc[index_i]\n",
    "        # #####################################################\n",
    "        slab_id_i = row_i.slab_id\n",
    "        ads_i = row_i.ads\n",
    "        att_num_i = row_i.att_num\n",
    "        compenv_i = row_i.compenv\n",
    "        active_site_i = row_i.active_site\n",
    "        # #####################################################\n",
    "\n",
    "        if active_site_i == \"NaN\":\n",
    "            tmp = 42\n",
    "        elif np.isnan(active_site_i):\n",
    "            active_site_i = \"NaN\"\n",
    "\n",
    "        # #####################################################\n",
    "        df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "        row_jobs_paths_i = df_jobs_paths_i.loc[index_i]\n",
    "        # #####################################################\n",
    "        path_job_root_w_att_rev = row_jobs_paths_i.path_job_root_w_att_rev\n",
    "        path_full = row_jobs_paths_i.path_full\n",
    "        path_rel_to_proj = row_jobs_paths_i.path_rel_to_proj\n",
    "        gdrive_path_i = row_jobs_paths_i.gdrive_path\n",
    "        # #####################################################\n",
    "\n",
    "        # #####################################################\n",
    "        in_index = df_jobs_anal.index.isin(\n",
    "            [(compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]).any()\n",
    "        if in_index:\n",
    "            row_anal_i = df_jobs_anal.loc[\n",
    "                compenv_i, slab_id_i, ads_i, active_site_i, att_num_i]\n",
    "            # #################################################\n",
    "            job_completely_done_i = row_anal_i.job_completely_done\n",
    "            # #################################################\n",
    "        else:\n",
    "            job_completely_done_i = None\n",
    "\n",
    "        # if job_completely_done_i:\n",
    "        #     print(\"job done:\", path_full)\n",
    "\n",
    "        # #####################################################\n",
    "        if compenv != \"wsl\":\n",
    "\n",
    "            from proj_data import compenvs\n",
    "            compenv_in_path = None\n",
    "            for compenv_j in compenvs:\n",
    "                if compenv_j in path_rel_to_proj:\n",
    "                    compenv_in_path = compenv_j\n",
    "\n",
    "            if compenv_in_path is not None:\n",
    "                new_path_list = []\n",
    "                for i in path_rel_to_proj.split(\"/\"):\n",
    "                    if i != compenv_in_path:\n",
    "                        new_path_list.append(i)\n",
    "                path_rel_to_proj_new = \"/\".join(new_path_list)\n",
    "                path_rel_to_proj = path_rel_to_proj_new\n",
    "\n",
    "\n",
    "            path_i = os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                path_rel_to_proj)\n",
    "        else:\n",
    "            path_i = os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "                gdrive_path_i)\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"path_i:\", path_i)\n",
    "\n",
    "        my_file = Path(path_i)\n",
    "        if my_file.is_dir():\n",
    "\n",
    "            # Only do these operations on non-running jobs\n",
    "            job_state_dict = parse_job_state(path_i)\n",
    "            job_state_i = job_state_dict[\"job_state\"]\n",
    "\n",
    "            if verbose:\n",
    "                print(\"job_state_i:\", job_state_i)\n",
    "\n",
    "            # #########################################\n",
    "            if job_state_i != \"RUNNING\":\n",
    "                # print(\"Doing large job processing\")\n",
    "                process_large_job_out(\n",
    "                    path_i, job_out_size_limit=job_out_size_limit)\n",
    "\n",
    "            # #########################################\n",
    "            rclone_sync_job_dir(\n",
    "                path_job_root_w_att_rev=path_job_root_w_att_rev,\n",
    "                path_rel_to_proj=path_rel_to_proj,\n",
    "                verbose=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove left over large job.out files\n",
    "For some reason some are left over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824512e5371749478343999d05015ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='1st loop', max=2834.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/8p937183bh/100/oh/active_site__16/00_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/926dnunrxf/010/bare/active_site__48/01_attempt/_01\n",
      "Large job.out, but no job.out.short /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/926dnunrxf/010/bare/active_site__49/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/949rnem5z2/010/bare/active_site__43/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/949rnem5z2/010/oh/active_site__42/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/949rnem5z2/200/oh/active_site__42/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/b49kx4c19q/021/bare/active_site__64/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/bgcpc2vabf/001/oh/active_site__64/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/bgcpc2vabf/010/oh/active_site__64/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/bgcpc2vabf/010/oh/active_site__64/02_attempt/_02\n",
      "Large job.out, but no job.out.short /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/bgcpc2vabf/010/oh/active_site__64/02_attempt/_03\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/m2bs8w82x5/001/01_attempt/_02\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/n36axdbw65/001/bare/active_site__32/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/n36axdbw65/001/01_attempt/_02\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/nrml6dms9l/002/oh/active_site__64/00_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/nrml6dms9l/002/oh/active_site__64/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/nrml6dms9l/002/oh/active_site__64/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/xlbfb49wml/100/oh/active_site__66/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/xw9y6rbkxr/2-1-112/bare/active_site__48/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/xw9y6rbkxr/2-1-112/bare/active_site__53/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/sherlock/zimixdvdxd/111/active_site__44/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/85z4msnl6o/001/oh/active_site__32/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/8l919k6s7p/2-202/active_site__63/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/8l919k6s7p/2-202/active_site__64/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/8p8evt9pcg/220/oh/active_site__24/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/8p8evt9pcg/220/oh/active_site__24/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/8p8evt9pcg/002/oh/active_site__24/03_attempt/_02\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/8p937183bh/010/oh/active_site__16/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/949rnem5z2/002/bare/active_site__32/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/949rnem5z2/110/bare/active_site__67/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/949rnem5z2/110/oh/active_site__66/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/949rnem5z2/110/bare/active_site__64/01_attempt/_02\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/949rnem5z2/110/oh/active_site__64/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/bpc2nk6qz1/210/bare/active_site__66/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/bpc2nk6qz1/210/oh/active_site__64/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/ck638t75z3/020/01_attempt/_02\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/cq7smr6lvj/001/oh/active_site__28/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/cq7smr6lvj/010/oh/active_site__26/03_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/mwmg9p7s6o/001/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/mwmg9p7s6o/111/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/nrml6dms9l/200/oh/active_site__64/00_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/nrml6dms9l/200/oh/active_site__64/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/nrml6dms9l/020/oh/active_site__64/00_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/xemg9wb27y/001/oh/active_site__27/00_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/xw9y6rbkxr/001/active_site__24/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/xw9y6rbkxr/001/oh/active_site__24/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/zimixdvdxd/010/oh/active_site__26/00_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/zimixdvdxd/010/oh/active_site__24/01_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs/slac/zimixdvdxd/010/01_attempt/_02\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/slac/zimixdvdxd/010/oh/active_site__26/02_attempt/_01\n",
      "Removing job.out /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/slac/zw9pbrnabj/010/bare/active_site__29/01_attempt/_02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if compenv == \"wsl\":\n",
    "    iterator = tqdm(df_i.index.tolist(), desc=\"1st loop\")\n",
    "    for index_i in iterator:\n",
    "        # #####################################################\n",
    "        row_i = df_i.loc[index_i]\n",
    "        # #####################################################\n",
    "        slab_id_i = row_i.slab_id\n",
    "        ads_i = row_i.ads\n",
    "        att_num_i = row_i.att_num\n",
    "        compenv_i = row_i.compenv\n",
    "        active_site_i = row_i.active_site\n",
    "        # #####################################################\n",
    "\n",
    "        # #####################################################\n",
    "        df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "        row_jobs_paths_i = df_jobs_paths_i.loc[index_i]\n",
    "        # #####################################################\n",
    "        gdrive_path_i = row_jobs_paths_i.gdrive_path\n",
    "        # #####################################################\n",
    "\n",
    "        path_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            gdrive_path_i)\n",
    "        if Path(path_i).is_dir():\n",
    "\n",
    "            # #############################################\n",
    "            path_job_short = os.path.join(path_i, \"job.out.short\")\n",
    "            if Path(path_job_short).is_file():\n",
    "                path_job = os.path.join(path_i, \"job.out\")\n",
    "                if Path(path_job).is_file():\n",
    "                    print(\"Removing job.out\", path_i)\n",
    "                    os.remove(path_job)\n",
    "\n",
    "            # #############################################\n",
    "            path_job = os.path.join(path_i, \"job.out\")\n",
    "            if Path(path_job).is_file():\n",
    "                if not Path(path_job_short).is_file():\n",
    "                    file_size = os.path.getsize(path_job)\n",
    "                    file_size_mb = file_size / 1000 / 1000\n",
    "                    \n",
    "                    if file_size_mb > job_out_size_limit:\n",
    "                        print(\"Large job.out, but no job.out.short\", path_i)\n",
    "                        process_large_job_out(\n",
    "                            path_i, job_out_size_limit=job_out_size_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove systems that are completely done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Removing job folders/data that are no longer needed\n",
      "Removing job folders/data that are no longer needed\n",
      "Removing job folders/data that are no longer needed\n",
      "Removing job folders/data that are no longer needed\n",
      "Removing job folders/data that are no longer needed\n",
      "Removing job folders/data that are no longer needed\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(5 * \"\\n\")\n",
    "print(80 * \"*\")\n",
    "print(80 * \"*\")\n",
    "print(80 * \"*\")\n",
    "print(80 * \"*\")\n",
    "print(\"Removing job folders/data that are no longer needed\")\n",
    "print(\"Removing job folders/data that are no longer needed\")\n",
    "print(\"Removing job folders/data that are no longer needed\")\n",
    "print(\"Removing job folders/data that are no longer needed\")\n",
    "print(\"Removing job folders/data that are no longer needed\")\n",
    "print(\"Removing job folders/data that are no longer needed\")\n",
    "print(2 * \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9866691f1fe483ca7d0b8c8cf8b17b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='1st loop', max=2834.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterator = tqdm(df_i.index.tolist(), desc=\"1st loop\")\n",
    "for job_id_i in iterator:\n",
    "    # #####################################################\n",
    "    row_i = df_i.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    slab_id_i = row_i.slab_id\n",
    "    ads_i = row_i.ads\n",
    "    att_num_i = row_i.att_num\n",
    "    compenv_i = row_i.compenv\n",
    "    active_site_i = row_i.active_site\n",
    "    # #####################################################\n",
    "\n",
    "    if active_site_i == \"NaN\":\n",
    "        tmp = 42\n",
    "    elif np.isnan(active_site_i):\n",
    "        active_site_i = \"NaN\"\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "    row_jobs_paths_i = df_jobs_paths_i.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    path_job_root_w_att_rev = row_jobs_paths_i.path_job_root_w_att_rev\n",
    "    path_full = row_jobs_paths_i.path_full\n",
    "    path_rel_to_proj = row_jobs_paths_i.path_rel_to_proj\n",
    "    gdrive_path_i = row_jobs_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    in_index = df_jobs_anal.index.isin(\n",
    "        [(compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]).any()\n",
    "    if in_index:\n",
    "        row_anal_i = df_jobs_anal.loc[\n",
    "            compenv_i, slab_id_i, ads_i, active_site_i, att_num_i]\n",
    "        # #################################################\n",
    "        job_completely_done_i = row_anal_i.job_completely_done\n",
    "        # #################################################\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    path_i = os.path.join(os.environ[\"PROJ_irox_oer\"], path_rel_to_proj)\n",
    "\n",
    "\n",
    "\n",
    "    delete_job = False\n",
    "\n",
    "    if not job_completely_done_i:\n",
    "        df_job_set_i = get_other_job_ids_in_set(job_id_i, df_jobs=df_jobs)\n",
    "\n",
    "        num_revs_list = df_job_set_i.num_revs.unique()\n",
    "        assert len(num_revs_list) == 1, \"kisfiisdjf\"\n",
    "        num_revs = num_revs_list[0]\n",
    "\n",
    "        df_jobs_to_delete = df_job_set_i[df_job_set_i.rev_num < num_revs - 1]\n",
    "\n",
    "        if job_id_i in df_jobs_to_delete.index.tolist():\n",
    "            delete_job = True\n",
    "\n",
    "    # #####################################################\n",
    "    if job_completely_done_i:\n",
    "        delete_job = True\n",
    "\n",
    "    if delete_job:\n",
    "\n",
    "        # #####################################################\n",
    "        # Check that the directory exists\n",
    "        my_file = Path(path_i)\n",
    "        dir_exists = False\n",
    "        if my_file.is_dir():\n",
    "            dir_exists = True\n",
    "\n",
    "        # #####################################################\n",
    "        # Check if .dft_clean file is present\n",
    "        dft_clean_file_path = os.path.join(path_i, \".dft_clean\")\n",
    "        my_file = Path(dft_clean_file_path)\n",
    "        dft_clean_already_exists = False\n",
    "        if my_file.is_file():\n",
    "            dft_clean_already_exists = True\n",
    "\n",
    "        # #####################################################\n",
    "        if dir_exists:\n",
    "            # Creating .dft_clean file\n",
    "            if not dft_clean_already_exists:\n",
    "                if compenv != \"wsl\":\n",
    "                    with open(dft_clean_file_path, \"w\") as file:\n",
    "                        file.write(\"\")\n",
    "\n",
    "        # #####################################################\n",
    "        # Remove directory\n",
    "        if dir_exists and dft_clean_already_exists and compenv != \"wsl\":\n",
    "            local_dir_matches_remote_i = local_dir_matches_remote(\n",
    "                path_i=path_i,\n",
    "                gdrive_path_i=gdrive_path_i,\n",
    "                )\n",
    "            print(40 * \"*\")\n",
    "            print(path_i)\n",
    "            if local_dir_matches_remote_i:\n",
    "                print(\"Removing:\")\n",
    "                shutil.rmtree(path_i)\n",
    "            else:\n",
    "                print(\"Gdrive doesn't match local\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
