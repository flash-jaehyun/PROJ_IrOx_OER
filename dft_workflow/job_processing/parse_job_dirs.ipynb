{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Job Directories\n",
    "---\n",
    "\n",
    "Meant to be run within one of the computer clusters on which jobs are run (Nersc, Sherlock, Slac). Will `os.walk` through `jobs_root_dir` and cobble together all the job directories and then upload the data to Dropbox.\n",
    "This script is meant primarily to get simple job information, for more detailed info run the `parse_job_data.ipynb` notebook."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import reorder_df_columns\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    is_attempt_dir,\n",
    "    is_rev_dir,\n",
    "    get_job_paths_info,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compenv = os.environ.get(\"COMPENV\", \"wsl\")\n",
    "if verbose:\n",
    "    print(\"compenv:\", compenv)\n",
    "\n",
    "if compenv == \"wsl\":\n",
    "    # This is a test compenv\n",
    "    # jobs_root_dir = os.path.join(\n",
    "    #     os.environ[\"PROJ_irox_oer\"],\n",
    "    #     \"__test__/dft_workflow\")\n",
    "    jobs_root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow\")\n",
    "\n",
    "elif compenv == \"nersc\" or compenv == \"sherlock\" or compenv == \"slac\":\n",
    "    jobs_root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering prelim info, get all base job dirs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_rel_to_proj(full_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #| - get_path_rel_to_proj\n",
    "    subdir = full_path\n",
    "\n",
    "    PROJ_dir = os.environ[\"PROJ_irox_oer\"]\n",
    "\n",
    "    search_term = PROJ_dir.split(\"/\")[-1]\n",
    "    ind_tmp = subdir.find(search_term)\n",
    "    if ind_tmp == -1:\n",
    "        search_term = \"PROJ_irox_oer\"\n",
    "        ind_tmp = subdir.find(search_term)\n",
    "\n",
    "    path_rel_to_proj = subdir[ind_tmp:]\n",
    "    path_rel_to_proj = \"/\".join(path_rel_to_proj.split(\"/\")[1:])\n",
    "\n",
    "\n",
    "\n",
    "    # subdir = full_path\n",
    "    # PROJ_dir = os.environ[\"PROJ_irox_oer\"]\n",
    "    # ind_tmp = subdir.find(PROJ_dir.split(\"/\")[-1])\n",
    "    # path_rel_to_proj = subdir[ind_tmp:]\n",
    "    # path_rel_to_proj = \"/\".join(path_rel_to_proj.split(\"/\")[1:])\n",
    "\n",
    "    return(path_rel_to_proj)\n",
    "    #__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_root_dir = \"/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/__test__/dft_workflow/run_slabs/run_oh_covered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for subdir, dirs, files in os.walk(jobs_root_dir):\n",
    "\n",
    "    data_dict_i = dict()\n",
    "    data_dict_i[\"path_full\"] = subdir\n",
    "\n",
    "    last_dir = jobs_root_dir.split(\"/\")[-1]\n",
    "    path_i = os.path.join(last_dir, subdir[len(jobs_root_dir) + 1:])\n",
    "    # path_i = subdir[len(jobs_root_dir) + 1:]\n",
    "\n",
    "\n",
    "    if \"dft_jobs\" not in subdir:\n",
    "        continue\n",
    "    if \".old\" in subdir:\n",
    "        continue\n",
    "    if path_i == \"\":\n",
    "        continue\n",
    "\n",
    "    if verbose:\n",
    "        print(path_i)\n",
    "\n",
    "\n",
    "\n",
    "    path_rel_to_proj = get_path_rel_to_proj(subdir)\n",
    "    data_dict_i[\"path_rel_to_proj\"] = path_rel_to_proj\n",
    "\n",
    "    # # #####################################################\n",
    "    # # TEMP\n",
    "    # if not \"v59s9lxdxr/131/oh/active_site__86/03_attempt\" in path_i:\n",
    "    #     continue\n",
    "    # print(path_i)\n",
    "\n",
    "    \n",
    "    out_dict = get_job_paths_info(path_i)\n",
    "    data_dict_i.update(out_dict)\n",
    "\n",
    "    # Only add job directory if it's been submitted\n",
    "    my_file = Path(os.path.join(subdir, \".SUBMITTED\"))\n",
    "    if my_file.is_file():\n",
    "        data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "if len(data_dict_list) == 0:\n",
    "    df_cols = [\n",
    "        \"path_full\",\n",
    "        \"path_rel_to_proj\",\n",
    "        \"path_job_root\",\n",
    "        \"path_job_root_w_att_rev\",\n",
    "        \"att_num\",\n",
    "        \"rev_num\",\n",
    "        \"is_rev_dir\",\n",
    "        \"is_attempt_dir\",\n",
    "        \"path_job_root_w_att\",\n",
    "        \"gdrive_path\",\n",
    "        ]\n",
    "\n",
    "    df = pd.DataFrame(columns=df_cols)\n",
    "else:\n",
    "    df = pd.DataFrame(data_dict_list)\n",
    "    df = df[~df.path_job_root_w_att_rev.isna()]\n",
    "    df = df.drop_duplicates(subset=[\"path_job_root_w_att_rev\", ], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "assert df.index.is_unique, \"Index must be unique here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_i = \"/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/v59s9lxdxr/131/oh/active_site__86/03_attempt/_01\"\n",
    "# path_i = \"/home/raulf2012/rclone_temp/PROJ_irox_oer/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/v59s9lxdxr/131/oh/active_site__86/03_attempt/_01\"\n",
    "# path_i = \"dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/sherlock/v59s9lxdxr/131/oh/active_site__86/03_attempt/_01\"\n",
    "# path_i = \"dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/v59s9lxdxr/131/oh/active_site__86/03_attempt\"\n",
    "\n",
    "# out_dict = \n",
    "# get_job_paths_info(path_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_gdrive_job_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_methods import get_gdrive_job_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def get_job_paths_info(path_i):\n",
    "# #     \"\"\"\n",
    "# #     \"\"\"\n",
    "# #| - get_job_paths_info\n",
    "# out_dict = dict()\n",
    "\n",
    "# # #####################################################\n",
    "# start_ind_to_remove = None\n",
    "# rev_num_i = None\n",
    "# att_num_i = None\n",
    "# path_job_root_i = None\n",
    "# path_job_root_w_att_rev = None\n",
    "# is_rev_dir_i_i = None\n",
    "# is_attempt_dir_i = None\n",
    "\n",
    "# path_job_root_w_att = None\n",
    "# gdrive_path = None\n",
    "# # #####################################################\n",
    "\n",
    "\n",
    "# # #########################################################\n",
    "# #  Getting the compenv\n",
    "# compenvs = [\"slac\", \"sherlock\", \"nersc\", ]\n",
    "# compenv_out = None\n",
    "# got_compenv_from_path = False\n",
    "# for compenv_i in compenvs:\n",
    "#     compenv_in_path = compenv_i in path_i\n",
    "#     if compenv_in_path:\n",
    "#         compenv_out = compenv_i\n",
    "#         got_compenv_from_path = True\n",
    "# if compenv_out is None:\n",
    "#     compenv_out = os.environ[\"COMPENV\"]\n",
    "\n",
    "\n",
    "# # #########################################################\n",
    "# path_split_i = path_i.split(\"/\")\n",
    "# # print(\"path_split_i:\", path_split_i)  # TEMP\n",
    "# for i_cnt, dir_i in enumerate(path_split_i):\n",
    "\n",
    "#     out_dict_i = is_rev_dir(dir_i)\n",
    "#     is_rev_dir_i = out_dict_i[\"is_rev_dir\"]\n",
    "#     rev_num_i = out_dict_i[\"rev_num\"]\n",
    "#     if is_rev_dir_i:\n",
    "#         dir_im1 = path_split_i[i_cnt - 1]\n",
    "\n",
    "#         out_dict_i = is_attempt_dir(dir_im1)\n",
    "#         is_attempt_dir_i = out_dict_i[\"is_attempt_dir\"]\n",
    "#         att_num_i = out_dict_i[\"att_num\"]\n",
    "#         if is_attempt_dir_i:\n",
    "#             start_ind_to_remove = i_cnt - 1\n",
    "\n",
    "# if start_ind_to_remove:\n",
    "#     print(\"PISDJFIJDSIJFIJDSIJFIDJSIFJIJ\")\n",
    "#     path_job_root_i = path_split_i[:start_ind_to_remove]\n",
    "#     path_job_root_i = \"/\".join(path_job_root_i)\n",
    "\n",
    "#     path_job_root_w_att_rev = path_split_i[:start_ind_to_remove + 2]\n",
    "#     path_job_root_w_att_rev = \"/\".join(path_job_root_w_att_rev)\n",
    "\n",
    "#     path_job_root_w_att = path_split_i[:start_ind_to_remove + 1]\n",
    "#     path_job_root_w_att = \"/\".join(path_job_root_w_att)\n",
    "\n",
    "\n",
    "#     #  print(\"path_job_root_w_att_rev:\", path_job_root_w_att_rev)\n",
    "#     #  print(\"path_job_root_i:\", path_job_root_i)\n",
    "\n",
    "#     # print(compenv_out)\n",
    "#     # if compenv_out is not None:\n",
    "#     if got_compenv_from_path:\n",
    "#         # compenv_out = os.environ[\"COMPENV\"]\n",
    "#         gdrive_path = path_job_root_w_att_rev\n",
    "#     else:\n",
    "#         gdrive_path = get_gdrive_job_path(path_job_root_w_att_rev)\n",
    "\n",
    "\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "# out_dict[\"compenv\"] = compenv_out\n",
    "# out_dict[\"path_job_root\"] = path_job_root_i\n",
    "# out_dict[\"path_job_root_w_att_rev\"] = path_job_root_w_att_rev\n",
    "# out_dict[\"att_num\"] = att_num_i\n",
    "# out_dict[\"rev_num\"] = rev_num_i\n",
    "# out_dict[\"is_rev_dir\"] = is_rev_dir_i\n",
    "# out_dict[\"is_attempt_dir\"] = is_attempt_dir_i\n",
    "# out_dict[\"path_job_root_w_att\"] = path_job_root_w_att\n",
    "# out_dict[\"gdrive_path\"] = gdrive_path\n",
    "\n",
    "# # return(out_dict)\n",
    "# #__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_job_root_w_att_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df.gdrive_path.tolist()\n",
    "\n",
    "# df_i = df[\n",
    "#     (df.compenv == \"sherlock\") & \\\n",
    "#     (df.bulk_id == \"v59s9lxdxr\") & \\\n",
    "#     (df.facet == \"131\") & \\\n",
    "#     (df.ads == \"oh\") & \\\n",
    "#     (df.att_num == 3)\n",
    "#     ]\n",
    "\n",
    "# df_i.gdrive_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # out_dict = \n",
    "\n",
    "# # get_job_paths_info(path_i)\n",
    "\n",
    "# path_i\n",
    "\n",
    "# compenvs = [\"slac\", \"sherlock\", \"nersc\", ]\n",
    "# compenv_out = None\n",
    "# for compenv_i in compenvs:\n",
    "#     compenv_in_path = compenv_i in path_i\n",
    "#     if compenv_in_path:\n",
    "#         compenv_out = compenv_i\n",
    "# if compenv_out is None:\n",
    "#     compenv_out = os.environ[\"COMPENV\"]\n",
    "\n",
    "# print(compenv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for subdir, dirs, files in os.walk(jobs_root_dir):\n",
    "\n",
    "# # data_dict_i = dict()\n",
    "# # data_dict_i[\"path_full\"] = subdir\n",
    "\n",
    "# last_dir = jobs_root_dir.split(\"/\")[-1]\n",
    "# # path_i = \n",
    "# os.path.join(last_dir, subdir[len(jobs_root_dir) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_dir\n",
    "\n",
    "# jobs_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     out_dict = \n",
    "# path_i\n",
    "# get_job_paths_info(path_i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path_rel_to_proj = \n",
    "# # get_path_rel_to_proj(subdir)\n",
    "\n",
    "# # subdir = full_path\n",
    "\n",
    "# PROJ_dir = os.environ[\"PROJ_irox_oer\"]\n",
    "\n",
    "# search_term = PROJ_dir.split(\"/\")[-1]\n",
    "# ind_tmp = subdir.find(search_term)\n",
    "# if ind_tmp == -1:\n",
    "#     search_term = \"PROJ_irox_oer\"\n",
    "#     ind_tmp = subdir.find(search_term)\n",
    "\n",
    "# path_rel_to_proj = subdir[ind_tmp:]\n",
    "# path_rel_to_proj = \"/\".join(path_rel_to_proj.split(\"/\")[1:])\n",
    "\n",
    "# path_rel_to_proj\n",
    "\n",
    "# # return(path_rel_to_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(PROJ_dir)\n",
    "# print(subdir)\n",
    "\n",
    "# PROJ_dir.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facet_bulk_id(row_i):\n",
    "    # row_i = df.iloc[0]\n",
    "\n",
    "    new_column_values_dict = {\n",
    "        \"bulk_id\": None,\n",
    "        \"facet\": None,\n",
    "        }\n",
    "\n",
    "    # #####################################################\n",
    "    path_job_root = row_i.path_job_root\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # Check if the job is a *O calc (different than other adsorbates)\n",
    "    if \"run_o_covered\" in path_job_root or \"run_o_covered\" in jobs_root_dir:\n",
    "        path_split = path_job_root.split(\"/\")\n",
    "\n",
    "        facet_i = path_split[-1]\n",
    "        bulk_id_i = path_split[-2]\n",
    "        ads_i = \"o\"\n",
    "        # active_site_i = None\n",
    "        active_site_i = np.nan\n",
    "\n",
    "        \n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    elif \"run_bare_oh_covered\" in path_job_root or \"run_bare_oh_covered\" in jobs_root_dir:\n",
    "        path_split = path_job_root.split(\"/\")\n",
    "\n",
    "        if \"/bare/\" in path_job_root:\n",
    "            ads_i = \"bare\"\n",
    "        elif \"/oh/\" in path_job_root:\n",
    "            ads_i = \"oh\"\n",
    "        else:\n",
    "            print(\"Couldn't parse the adsorbate from here\")\n",
    "            ads_i = None\n",
    "\n",
    "        active_site_parsed = False\n",
    "        for i in path_split:\n",
    "            if \"active_site__\" in i:\n",
    "                active_site_path_seg = i.split(\"_\")\n",
    "                active_site_i = active_site_path_seg[-1]\n",
    "                active_site_i = int(active_site_i)\n",
    "                active_site_parsed = True\n",
    "        if not active_site_parsed:\n",
    "            print(\"PROBLEM | Couldn't parse active site for following dir:\")\n",
    "            print(path_job_root)\n",
    "\n",
    "        facet_i = path_split[-3]\n",
    "        bulk_id_i = path_split[-4]\n",
    "        # ads_i = \"bare\"\n",
    "\n",
    "        # Check that the parsed facet makes sense\n",
    "        all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_i])\n",
    "        mess_i = \"All characters of parsed facet must be numeric\"\n",
    "        assert all_facet_chars_are_numeric, mess_i\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    elif \"run_oh_covered\" in path_job_root or \"run_oh_covered\" in jobs_root_dir:\n",
    "        path_split = path_job_root.split(\"/\")\n",
    "\n",
    "        if \"/bare/\" in path_job_root:\n",
    "            ads_i = \"bare\"\n",
    "        elif \"/oh/\" in path_job_root:\n",
    "            ads_i = \"oh\"\n",
    "        else:\n",
    "            print(\"Couldn't parse the adsorbate from here\")\n",
    "            ads_i = None\n",
    "\n",
    "        active_site_parsed = False\n",
    "        for i in path_split:\n",
    "            if \"active_site__\" in i:\n",
    "                active_site_path_seg = i.split(\"_\")\n",
    "                active_site_i = active_site_path_seg[-1]\n",
    "                active_site_i = int(active_site_i)\n",
    "                active_site_parsed = True\n",
    "        if not active_site_parsed:\n",
    "            print(\"PROBLEM | Couldn't parse active site for following dir:\")\n",
    "            print(path_job_root)\n",
    "\n",
    "        facet_i = path_split[-3]\n",
    "        bulk_id_i = path_split[-4]\n",
    "\n",
    "        # Check that the parsed facet makes sense\n",
    "        all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_i])\n",
    "        mess_i = \"All characters of parsed facet must be numeric\"\n",
    "        assert all_facet_chars_are_numeric, mess_i\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    else:\n",
    "        print(\"Couldn't figure out what to do here\")\n",
    "        print(path_job_root)\n",
    " \n",
    "        facet_i = None\n",
    "        bulk_id_i = None\n",
    "        ads_i = None\n",
    "\n",
    "        pass\n",
    "\n",
    "    # #####################################################\n",
    "    new_column_values_dict[\"facet\"] = facet_i\n",
    "    new_column_values_dict[\"bulk_id\"] = bulk_id_i\n",
    "    new_column_values_dict[\"ads\"] = ads_i\n",
    "    new_column_values_dict[\"active_site\"] = active_site_i\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "\n",
    "    return(row_i)\n",
    "\n",
    "df = df.apply(\n",
    "    get_facet_bulk_id,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"out_data/df_dirs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.gdrive_path.tolist()\n",
    "\n",
    "df_i = df[\n",
    "    (df.compenv == \"sherlock\") & \\\n",
    "    (df.bulk_id == \"v59s9lxdxr\") & \\\n",
    "    (df.facet == \"131\") & \\\n",
    "    (df.ads == \"oh\") & \\\n",
    "    (df.att_num == 3)\n",
    "    ]\n",
    "\n",
    "df_i.gdrive_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.att_num = df.att_num.astype(int)\n",
    "df.rev_num = df.rev_num.astype(int)\n",
    "\n",
    "# df[\"compenv\"] = compenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "grouped = df.groupby([\"path_job_root_w_att\", ])\n",
    "for name, group in grouped:\n",
    "    num_revs = group.shape[0]\n",
    "\n",
    "    group[\"num_revs\"] = num_revs\n",
    "    groups.append(group)\n",
    "\n",
    "if len(groups) == 0:\n",
    "    pass\n",
    "else:\n",
    "    df = pd.concat(groups, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = [\n",
    "    \"compenv\",\n",
    "\n",
    "    \"bulk_id\",\n",
    "    \"facet\",\n",
    "    \"ads\",\n",
    "\n",
    "    \"att_num\",\n",
    "    \"rev_num\",\n",
    "    \"num_revs\",\n",
    "\n",
    "    \"is_rev_dir\",\n",
    "    \"is_attempt_dir\",\n",
    "\n",
    "    \"path_job_root\",\n",
    "    \"path_job_root_w_att_rev\",\n",
    "    ]\n",
    "\n",
    "df = reorder_df_columns(new_col_order, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data and uploading to Dropbox"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "# directory = \"out_data\"\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_processing\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "file_name_i = \"df_jobs_base_\" + compenv + \".pickle\"\n",
    "file_path_i = os.path.join(directory, file_name_i)\n",
    "with open(file_path_i, \"wb\") as fle:\n",
    "    pickle.dump(df, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(\n",
    "    \"01_norskov/00_git_repos/PROJ_IrOx_OER\",\n",
    "    \"dft_workflow/job_processing/out_data\" ,\n",
    "    file_name_i)\n",
    "\n",
    "rclone_remote = os.environ.get(\"rclone_dropbox\", \"raul_dropbox\")\n",
    "bash_comm = \"rclone copyto \" + file_path_i + \" \" + rclone_remote + \":\" + db_path\n",
    "if verbose:\n",
    "    print(\"bash_comm:\", bash_comm)\n",
    "\n",
    "os.system(bash_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"parse_job_dirs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df[\n",
    "    (df.compenv == \"sherlock\") & \\\n",
    "    (df.bulk_id == \"v59s9lxdxr\") & \\\n",
    "    (df.facet == \"131\") & \\\n",
    "    (df.ads == \"oh\") & \\\n",
    "    (df.att_num == 3)\n",
    "    ]\n",
    "df_i.gdrive_path.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
