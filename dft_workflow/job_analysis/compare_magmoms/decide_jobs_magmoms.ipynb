{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect DFT data into *, *O, *OH collections\n",
    "---\n",
    "\n",
    "Notes:\n",
    "  * If there exists only a single slab for a particular adsorbate, and that slab has a averaged absolute magmom per atom of less than XXX, then we should check if there are slabs of different adsorbates in that set to tranplant the magmoms from"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_jobs_anal\n",
    "from methods import get_df_jobs_data\n",
    "from methods import get_df_atoms_sorted_ind\n",
    "from methods import get_df_magmoms\n",
    "from methods import get_df_jobs_paths\n",
    "from methods import get_df_jobs_oh_anal\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import read_magmom_comp_data, save_magmom_comp_data\n",
    "from local_methods import process_group_magmom_comp\n",
    "from local_methods import get_oer_set\n",
    "from local_methods import analyze_O_in_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# verbose = True\n",
    "\n",
    "redo_all_jobs = False\n",
    "# redo_all_jobs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "# #########################################################\n",
    "magmom_data_dict = read_magmom_comp_data()\n",
    "\n",
    "# #########################################################\n",
    "df_magmoms = get_df_magmoms()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "# #########################################################\n",
    "df_magmoms = df_magmoms.set_index(\"job_id\")\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_oh_anal = get_df_jobs_oh_anal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs_oh_anal"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data objects"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing `df_jobs_anal` (only completed job sets, filter out *O)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# Only completed jobs will be considered\n",
    "df_jobs_anal_i = df_jobs_anal[df_jobs_anal.job_completely_done == True]\n",
    "\n",
    "# #########################################################\n",
    "# Remove the *O slabs for now\n",
    "# The fact that they have NaN active sites will mess up the groupby\n",
    "ads_list = df_jobs_anal_i.index.get_level_values(\"ads\").tolist()\n",
    "ads_list_no_o = [i for i in list(set(ads_list)) if i != \"o\"]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "df_jobs_anal_no_o = df_jobs_anal_i.loc[idx[:, :, ads_list_no_o, :, :], :]\n",
    "\n",
    "# #########################################################\n",
    "# Only keep OER job sets that have all adsorbates present and completed\n",
    "indices_to_keep = []\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "for name_i, group in grouped:\n",
    "\n",
    "    # print(\"TEMP\")\n",
    "    # index_i = ('slac', 'fagumoha_68', 'oh', 62.0, 3)\n",
    "    # if index_i in group.index:\n",
    "    #     print(name_i)\n",
    "\n",
    "    group_index = group.index.to_frame()\n",
    "    ads_list = list(group_index.ads.unique())\n",
    "    oh_present = \"oh\" in ads_list\n",
    "    bare_present = \"bare\" in ads_list\n",
    "    all_req_ads_present = oh_present and bare_present\n",
    "    if all_req_ads_present:\n",
    "        indices_to_keep.extend(group.index.tolist())\n",
    "\n",
    "df_jobs_anal_no_o_all_ads_pres = df_jobs_anal_no_o.loc[\n",
    "    indices_to_keep    \n",
    "    ]\n",
    "df_i = df_jobs_anal_no_o_all_ads_pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process `df_jobs_oh_anal`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_oh_anal = df_jobs_oh_anal.set_index(\n",
    "    [\"compenv\", \"slab_id\", \"active_site\", ], drop=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if there are OER sets that have slabs with magmom 0'ed out"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff for how low the magmoms of slab can go before I rerun with different spin\n",
    "magmom_cutoff = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "verbose_local = False\n",
    "# #########################################################\n",
    "\n",
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_i.groupby(groupby_cols)\n",
    "for i_cnt, (name_i, group) in enumerate(grouped):\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    if verbose_local:\n",
    "        print(40 * \"*\")\n",
    "        print(\"name_i:\", name_i)\n",
    "\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #########################################################\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    group_i = get_oer_set(\n",
    "        group=group,\n",
    "        compenv=compenv_i,\n",
    "        slab_id=slab_id_i,\n",
    "        df_jobs_anal=df_jobs_anal,\n",
    "        )\n",
    "\n",
    "    # #########################################################\n",
    "    magmom_data_out = analyze_O_in_set(\n",
    "        data_dict_i,\n",
    "        group_i,\n",
    "        df_magmoms,\n",
    "        magmom_cutoff=magmom_cutoff,\n",
    "        compenv=compenv_i,\n",
    "        slab_id=slab_id_i,\n",
    "        active_site=active_site_i,\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i.update(magmom_data_out)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df_m = pd.DataFrame(data_dict_list)\n",
    "df_m = df_m.set_index([\"compenv\", \"slab_id\", \"active_site\", ], drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict_i.update(\n",
    "# data_dict_tmp\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_diff_0 = df_jobs_oh_anal.index.difference(df_m.index)\n",
    "index_diff_1 = df_m.index.difference(df_jobs_oh_anal.index)\n",
    "\n",
    "# index_diff_0.shape[0]\n",
    "\n",
    "mess_i = \"This shouldn't be, look into it\"\n",
    "assert index_diff_1.shape[0] == 0, mess_i\n",
    "\n",
    "# #########################################################\n",
    "shared_index = df_jobs_oh_anal.index.intersection(df_m.index)\n",
    "\n",
    "df_jobs_oh_anal = df_jobs_oh_anal.loc[shared_index]\n",
    "df_m = df_m.loc[shared_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_0 = list(df_m.columns)\n",
    "list_1 = list(df_jobs_oh_anal.columns)\n",
    "\n",
    "shared_cols = list(set(list_0).intersection(list_1))\n",
    "\n",
    "# shared_cols\n",
    "# df_m.drop(columns=shared_cols)\n",
    "\n",
    "df_list = [\n",
    "    df_m.drop(columns=shared_cols),\n",
    "    df_jobs_oh_anal,\n",
    "    ]\n",
    "\n",
    "df_m2 = pd.concat(df_list, axis=1)\n",
    "df_m2 = df_m2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3 = df_m2[\n",
    "    (df_m2[\"*O_w_low_magmoms\"] == True) & \\\n",
    "    (df_m2[\"*O_w_not_low_magmoms\"] == False) & \\\n",
    "    (df_m2[\"all_oh_attempts_done\"] == True) & \\\n",
    "    [True for i in range(len(df_m2))]\n",
    "    ]\n",
    "\n",
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_m3.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    \n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    active_site_i = row_i.active_site\n",
    "    all_oh_attempts_done_i = row_i.all_oh_attempts_done\n",
    "    job_ids_sorted_energy_i = row_i.job_ids_sorted_energy\n",
    "    job_id_most_stable_i = row_i.job_id_most_stable\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_magmoms_i = df_magmoms.loc[job_id_most_stable_i]\n",
    "    # #####################################################\n",
    "    sum_abs_magmoms_pa_i = row_magmoms_i.sum_abs_magmoms_pa\n",
    "    # #####################################################\n",
    "\n",
    "    # print(\"sum_abs_magmoms_pa_i:\", sum_abs_magmoms_pa_i)\n",
    "\n",
    "    rerun_from_oh = False\n",
    "    # if sum_abs_magmoms_pa_i > magmom_cutoff:\n",
    "    if sum_abs_magmoms_pa_i > 0.07:\n",
    "        rerun_from_oh = True\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"rerun_from_oh\"] = rerun_from_oh\n",
    "    # data_dict_i[\"all_oh_attempts_done_i\"] = all_oh_attempts_done_i\n",
    "    # data_dict_i[\"job_ids_sorted_energy_i\"] = job_ids_sorted_energy_i\n",
    "    # data_dict_i[\"job_id_most_stable_i\"] = job_id_most_stable_i\n",
    "    # data_dict_i[\"\"] = \n",
    "    # #####################################################\n",
    "    data_dict_i.update(row_i.to_dict())\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df_rerun_from_oh = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rerun_from_oh.head()\n",
    "# df_rerun_from_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data to pickle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/compare_magmoms\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "path_i = os.path.join(directory, \"df_rerun_from_oh.pickle\")\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_rerun_from_oh, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_rerun_from_oh\n",
    "df_rerun_from_oh_tmp = get_df_rerun_from_oh()\n",
    "df_rerun_from_oh_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #########################################################\n",
    "# import pickle; import os\n",
    "# directory = os.path.join(\n",
    "#     os.environ[\"PROJ_irox_oer\"],\n",
    "#     \"dft_workflow/job_analysis/compare_magmoms\",\n",
    "#     \"out_data\")\n",
    "# path_i = os.path.join(directory, \"df_rerun_from_oh.pickle\")\n",
    "# with open(path_i, \"rb\") as fle:\n",
    "#     df_rerun_from_oh = pickle.load(fle)\n",
    "# # #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rerun_from_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compenv_i\n",
    "# compenv_i = \n",
    "# row_i.compenv\n",
    "\n",
    "# compenv_i = row_i.compenv\n",
    "# slab_id_i = row_i.slab_id\n",
    "# active_site_i = row_i.active_site\n",
    "# all_oh_attempts_done_i = row_i.all_oh_attempts_done\n",
    "# job_ids_sorted_energy_i = row_i.job_ids_sorted_energy\n",
    "# job_id_most_stable_i = row_i.job_id_most_stable\n",
    "\n",
    "# row_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_magmoms[\n",
    "#     (df_magmoms.compenv == compenv_i) & \\\n",
    "#     (df_magmoms.slab_id == slab_id_i) & \\\n",
    "#     # (df_magmoms.active_site == active_site_i) & \\\n",
    "#     [True for i in range(len(df_magmoms))]\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_oh_anal_i\n",
    "# row_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the slabs with the smallest magmoms to file to manually inspect"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_magmoms[df_magmoms.sum_abs_magmoms_pa > 1e-5]\n",
    "df_i = df_i.sort_values(\"sum_abs_magmoms_pa\", ascending=True)\n",
    "\n",
    "for i_cnt, (job_id_i, row_i) in enumerate(df_i.iloc[0:20].iterrows()):\n",
    "\n",
    "    # #####################################################\n",
    "    row_paths_i = df_jobs_paths.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    gdrive_path_i = row_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        gdrive_path_i,\n",
    "        \"final_with_calculator.traj\")\n",
    "\n",
    "    directory = os.path.join(\n",
    "        \"__temp__/low_magmom_slabs\")\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        directory,\n",
    "        str(i_cnt).zfill(3) + \"_\" + job_id_i + \".traj\")\n",
    "\n",
    "    shutil.copyfile(\n",
    "        path_i,\n",
    "        out_path)\n",
    "\n",
    "df_i.iloc[0:20]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_index_i = df_i.index.to_frame()\n",
    "\n",
    "# df_index_tmp = df_index_i[\n",
    "    \n",
    "#     # (df_index_i.compenv == \"sherlock\") & \\\n",
    "#     # (df_index_i.slab_id == \"vuvunira_55\") & \\\n",
    "#     # (df_index_i.active_site == 68.) & \\\n",
    "\n",
    "#     (df_index_i.compenv == \"sherlock\") & \\\n",
    "#     (df_index_i.slab_id == \"kipatalo_90\") & \\\n",
    "#     (df_index_i.active_site == 81.) & \\\n",
    "\n",
    "#     [True for i in range(len(df_index_i))]\n",
    "#     ]\n",
    "\n",
    "\n",
    "# # print(\"TEMP\")\n",
    "# # df_i = df_i.loc[\n",
    "# #     df_index_tmp.index\n",
    "# #     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "    # write_atoms_objets = True\n",
    "\n",
    "\n",
    "    # out_dict = magmom_data_dict.get(name_i, None)\n",
    "\n",
    "    # if out_dict is None:\n",
    "    #     run_job = True\n",
    "    # else:\n",
    "    #     run_job = False\n",
    "\n",
    "    # if redo_all_jobs:\n",
    "    #     run_job = True\n",
    "\n",
    "\n",
    "    # if run_job:\n",
    "    #     out_dict = process_group_magmom_comp(\n",
    "    #         group=group_w_o,\n",
    "    #         # write_atoms_objects=False,\n",
    "    #         write_atoms_objects=True,\n",
    "    #         verbose=False,\n",
    "    #         # verbose=True,\n",
    "    #         )\n",
    "\n",
    "\n",
    "    # magmom_data_dict[name_i] = out_dict\n",
    "\n",
    "    # save_magmom_comp_data(magmom_data_dict)\n",
    "    # if verbose_local:\n",
    "    #     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# def analyze_O_in_set():\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     #| - analyze_O_in_set\n",
    "# sys_w_not_low_magmoms = False\n",
    "# sys_w_low_magmoms = False\n",
    "\n",
    "# # #########################################################\n",
    "# # Check for *O slabs first\n",
    "# df_index_i = group_i.index.to_frame()\n",
    "# df_index_i = df_index_i[df_index_i.ads == \"o\"]\n",
    "# # #########################################################\n",
    "# group_o = group_i.loc[df_index_i.index]\n",
    "# # #########################################################\n",
    "# for name_i, row_i in group_o.iterrows():\n",
    "\n",
    "#     # #####################################################\n",
    "#     job_id_i = row_i.job_id_max\n",
    "#     # #####################################################\n",
    "\n",
    "#     # #####################################################\n",
    "#     row_magmoms_i = df_magmoms.loc[job_id_i]\n",
    "#     # #####################################################\n",
    "#     sum_magmoms_i = row_magmoms_i.sum_magmoms\n",
    "#     # #####################################################\n",
    "\n",
    "#     # #####################################################\n",
    "#     row_magmoms_i = df_magmoms.loc[job_id_i]\n",
    "#     # #####################################################\n",
    "#     sum_magmoms_i = row_magmoms_i.sum_magmoms\n",
    "#     sum_abs_magmoms_i = row_magmoms_i.sum_abs_magmoms\n",
    "#     sum_magmoms_pa_i = row_magmoms_i.sum_magmoms_pa\n",
    "#     sum_abs_magmoms_pa = row_magmoms_i.sum_abs_magmoms_pa\n",
    "#     # #####################################################\n",
    "\n",
    "#     if sum_abs_magmoms_pa < magmom_cutoff:\n",
    "#         sys_w_low_magmoms = True\n",
    "#     if sum_abs_magmoms_pa > 0.1:\n",
    "#         sys_w_not_low_magmoms = True\n",
    "\n",
    "\n",
    "# # #####################################################\n",
    "# data_dict_i[\"compenv\"] = compenv_i\n",
    "# data_dict_i[\"slab_id\"] = slab_id_i\n",
    "# data_dict_i[\"active_site\"] = active_site_i\n",
    "# # #####################################################\n",
    "# data_dict_i[\"*O_w_low_magmoms\"] = sys_w_low_magmoms\n",
    "# data_dict_i[\"*O_w_not_low_magmoms\"] = sys_w_not_low_magmoms\n",
    "# # data_dict_i[\"\"] = \n",
    "# # #####################################################\n",
    "# data_dict_list.append(data_dict_i)\n",
    "# # #####################################################\n",
    "\n",
    "# #__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# pd.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_oh_anal.loc[('slac', 'fagumoha_68', 62.0)]\n",
    "\n",
    "# df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal_no_o.loc[\n",
    "#     ('slac', 'fagumoha_68', 'oh', 62.0, 3)\n",
    "#     ]\n",
    "\n",
    "# grouped.get_group(('slac', 'fagumoha_68', 62.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# if is_there_any_sys_w_low_magmoms:\n",
    "#     tmp = 42\n",
    "\n",
    "# group_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "    # # #####################################################\n",
    "    # row_oh_anal_i = df_jobs_oh_anal[\n",
    "    #     (df_jobs_oh_anal.compenv == compenv_i) & \\\n",
    "    #     (df_jobs_oh_anal.slab_id == slab_id_i) & \\\n",
    "    #     (df_jobs_oh_anal.active_site == active_site_i) & \\\n",
    "    #     [True for i in range(len(df_jobs_oh_anal))]\n",
    "    #     ]\n",
    "    # # #####################################################\n",
    "    # all_oh_attempts_done_i = row_oh_anal_i.all_oh_attempts_done\n",
    "    # job_ids_sorted_energy_i = row_oh_anal_i.job_ids_sorted_energy\n",
    "    # job_id_most_stable_i = row_oh_anal_i.job_id_most_stable\n",
    "    # # #####################################################"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
