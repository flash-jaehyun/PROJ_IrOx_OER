{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect DFT data into *, *O, *OH collections\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_analysis/compare_magmoms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_jobs_anal\n",
    "from methods import get_df_jobs_data\n",
    "from methods import get_df_atoms_sorted_ind\n",
    "from methods import get_df_init_slabs\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import read_magmom_comp_data, save_magmom_comp_data\n",
    "from local_methods import process_group_magmom_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# verbose = True\n",
    "\n",
    "redo_all_jobs = False\n",
    "# redo_all_jobs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_init_slabs = get_df_init_slabs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# Only completed jobs will be considered\n",
    "df_jobs_anal_i = df_jobs_anal[df_jobs_anal.job_completely_done == True]\n",
    "\n",
    "# #########################################################\n",
    "# Remove the *O slabs for now\n",
    "# The fact that they have NaN active sites will mess up the groupby\n",
    "ads_list = df_jobs_anal_i.index.get_level_values(\"ads\").tolist()\n",
    "ads_list_no_o = [i for i in list(set(ads_list)) if i != \"o\"]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "df_jobs_anal_no_o = df_jobs_anal_i.loc[idx[:, :, ads_list_no_o, :, :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_keep = []\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "for name_i, group in grouped:\n",
    "    group_index = group.index.to_frame()\n",
    "    ads_list = list(group_index.ads.unique())\n",
    "    oh_present = \"oh\" in ads_list\n",
    "    bare_present = \"bare\" in ads_list\n",
    "    all_req_ads_present = oh_present and bare_present\n",
    "    if all_req_ads_present:\n",
    "        indices_to_keep.extend(group.index.tolist())\n",
    "\n",
    "df_jobs_anal_no_o_all_ads_pres = df_jobs_anal_no_o.loc[\n",
    "    indices_to_keep    \n",
    "    ]\n",
    "df_i = df_jobs_anal_no_o_all_ads_pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP | Filtering dataframe for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_i = df_i.index.to_frame()\n",
    "\n",
    "df_index_tmp = df_index_i[\n",
    "    \n",
    "    # (df_index_i.compenv == \"sherlock\") & \\\n",
    "    # (df_index_i.slab_id == \"vuvunira_55\") & \\\n",
    "    # (df_index_i.active_site == 68.) & \\\n",
    "\n",
    "    (df_index_i.compenv == \"sherlock\") & \\\n",
    "    (df_index_i.slab_id == \"kipatalo_90\") & \\\n",
    "    (df_index_i.active_site == 81.) & \\\n",
    "\n",
    "    [True for i in range(len(df_index_i))]\n",
    "    ]\n",
    "\n",
    "\n",
    "# print(\"TEMP\")\n",
    "# df_i = df_i.loc[\n",
    "#     df_index_tmp.index\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magmom comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "name_i: ('sherlock', 'fogalonu_46', 16.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'kenukami_73', 84.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'kenukami_73', 86.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'kipatalo_90', 78.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'kipatalo_90', 81.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'rakawavo_17', 25.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'tofiwadi_49', 45.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'tofiwadi_49', 46.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'tofiwadi_49', 47.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'vuvunira_55', 65.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'vuvunira_55', 68.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'vuvunira_55', 70.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'vuvunira_55', 72.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'vuvunira_55', 73.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'vuvunira_55', 75.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'wefedifi_91', 94.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'wefedifi_91', 96.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('sherlock', 'wefedifi_91', 98.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('slac', 'fagumoha_68', 63.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('slac', 'kalisule_45', 62.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('slac', 'kalisule_45', 67.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('slac', 'kalisule_45', 71.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('slac', 'kalisule_45', 73.0)\n",
      "False\n",
      "\n",
      "****************************************\n",
      "name_i: ('slac', 'mesufagi_19', 32.0)\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "verbose_local = False\n",
    "# #########################################################\n",
    "\n",
    "# data_dict = dict()\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_i.groupby(groupby_cols)\n",
    "for i_cnt, (name_i, group) in enumerate(grouped):\n",
    "    \n",
    "    if verbose_local:\n",
    "        print(40 * \"*\")\n",
    "        print(\"name_i:\", name_i)\n",
    "\n",
    "    # #########################################################\n",
    "    magmom_data_dict = read_magmom_comp_data()\n",
    "    # #########################################################\n",
    "    # print(\"len(magmom_data_dict):\", len(magmom_data_dict))\n",
    "\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #########################################################\n",
    "\n",
    "    df_index = df_jobs_anal_i.index.to_frame()\n",
    "\n",
    "    df_index_i = df_index[\n",
    "        (df_index.compenv == compenv_i) & \\\n",
    "        (df_index.slab_id == slab_id_i) & \\\n",
    "        (df_index.ads == \"o\") & \\\n",
    "        [True for i in range(len(df_index))]\n",
    "        ]\n",
    "    df_index_i.shape\n",
    "\n",
    "    mess_i = \"ISJIdfjisdjij\"\n",
    "    assert df_index_i.shape[0] == 1, mess_i\n",
    "\n",
    "\n",
    "    row_o_i = df_jobs_anal_i.loc[\n",
    "        df_index_i.index    \n",
    "        ]\n",
    "\n",
    "    # #########################################################\n",
    "    group_w_o = pd.concat(\n",
    "        [ \n",
    "            group,\n",
    "            row_o_i,\n",
    "            ],\n",
    "        axis=0)\n",
    "\n",
    "    write_atoms_objets = True\n",
    "\n",
    "    \n",
    "    out_dict = magmom_data_dict.get(name_i, None)\n",
    "\n",
    "    if out_dict is None:\n",
    "        run_job = True\n",
    "    else:\n",
    "        run_job = False\n",
    "    \n",
    "    if redo_all_jobs:\n",
    "        run_job = True\n",
    "\n",
    "\n",
    "\n",
    "    if verbose_local:\n",
    "        print(run_job)\n",
    "\n",
    "    if run_job:\n",
    "        out_dict = process_group_magmom_comp(\n",
    "            group=group_w_o,\n",
    "            write_atoms_objects=False,\n",
    "            # write_atoms_objects=True,\n",
    "            verbose=False,\n",
    "            # verbose=True,\n",
    "            )\n",
    "\n",
    "\n",
    "    magmom_data_dict[name_i] = out_dict\n",
    "\n",
    "    save_magmom_comp_data(magmom_data_dict)\n",
    "    if verbose_local:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magmom_data_dict\n",
    "# list(magmom_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying which slabs have zero magmoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = []\n",
    "data_dict_list = []\n",
    "for name_i, row_i in df_jobs_anal_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    ads_i = name_i[2]\n",
    "    active_site_i = name_i[3]\n",
    "    att_num_i = name_i[4]\n",
    "    # #########################################################\n",
    "\n",
    "    # #########################################################\n",
    "    job_id_i = row_i.job_id_max\n",
    "    # #########################################################\n",
    "    \n",
    "    # #########################################################\n",
    "    row_atoms_i = df_atoms_sorted_ind.loc[\n",
    "        (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]\n",
    "    # #########################################################\n",
    "    atoms = row_atoms_i.atoms_sorted_good\n",
    "    magmoms_i = row_atoms_i.magmoms_sorted_good\n",
    "    # #########################################################\n",
    "    \n",
    "    if atoms.calc != None:\n",
    "        magmoms_i = atoms.get_magnetic_moments()\n",
    "    else:\n",
    "        magmoms_i = magmoms_i\n",
    "\n",
    "    sum_magmoms_i = np.sum(magmoms_i)\n",
    "    sum_abs_magmoms = np.sum(np.abs(magmoms_i))\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"ads\"] = ads_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"att_num\"] = att_num_i\n",
    "    # #########################################################\n",
    "    data_dict_i[\"job_id\"] = job_id_i\n",
    "    data_dict_i[\"sum_magmoms\"] = sum_magmoms_i\n",
    "    data_dict_i[\"sum_abs_magmoms\"] = sum_abs_magmoms\n",
    "    # #########################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #########################################################\n",
    "\n",
    "df_magmoms = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs_anal_i[df_jobs_anal_i.job_id_max == \"donepote_14\"]\n",
    "\n",
    "# df_magmoms.loc[\n",
    "#     \"donepote_14\"\n",
    "#     ]\n",
    "\n",
    "# df_magmoms[df_magmoms.job_id == \"donepote_14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_magmoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing magmoms by number of atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    ads_i = row_i.ads\n",
    "    active_site_i = row_i.active_site\n",
    "    att_num_i = row_i.att_num\n",
    "\n",
    "    sum_magmoms_i = row_i.sum_magmoms\n",
    "    sum_abs_magmoms_i = row_i.sum_abs_magmoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    name_i = (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)\n",
    "    row_slab_i = df_init_slabs.loc[name_i]\n",
    "    # #####################################################\n",
    "    num_atoms_i = row_slab_i.num_atoms\n",
    "    # #####################################################\n",
    "\n",
    "    sum_magmoms_pa = sum_magmoms_i / num_atoms_i\n",
    "    sum_abs_magmoms_pa = sum_abs_magmoms_i / num_atoms_i\n",
    "\n",
    "    # #####################################################\n",
    "    row_i[\"sum_magmoms_pa\"] = sum_magmoms_pa\n",
    "    row_i[\"sum_abs_magmoms_pa\"] = sum_abs_magmoms_pa\n",
    "    # #####################################################\n",
    "    return(row_i)\n",
    "\n",
    "df_magmoms = df_magmoms.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further analysis of magmom comparison (collapse into dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for name_i in magmom_data_dict.keys():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    magmom_data_dict[name_i].keys()\n",
    "\n",
    "    from IPython.display import display\n",
    "    df_magmoms_comp_i = magmom_data_dict[name_i][\"df_magmoms_comp\"]\n",
    "    # display(df_magmoms_comp_i)\n",
    "    min_val_i = df_magmoms_comp_i.sum_norm_abs_magmom_diff.min()\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"name\"] = name_i\n",
    "    data_dict_i[\"min_sum_norm_abs_magmom_diff\"] = min_val_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "df.min_sum_norm_abs_magmom_diff.min()\n",
    "\n",
    "df = df.sort_values(\"min_sum_norm_abs_magmom_diff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    # \"workflow/compare_magmoms\",\n",
    "    \"dft_workflow/job_analysis/compare_magmoms\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "path_i = os.path.join(directory, \"df_magmoms.pickle\")\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_magmoms, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_magmoms\n",
    "\n",
    "df_magmoms_tmp = get_df_magmoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"analyse_jobs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# assert False\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict.keys()\n",
    "\n",
    "# magmom_dict_i = data_dict[\n",
    "#     ('sherlock', 'kipatalo_90', 81.0)\n",
    "#     ]\n",
    "\n",
    "# list(magmom_dict_i.keys())\n",
    "\n",
    "# df_magmoms_comp = magmom_dict_i[\"df_magmoms_comp\"]\n",
    "# good_triplet_comb = magmom_dict_i[\"good_triplet_comb\"]\n",
    "# pair_wise_magmom_comp_data = magmom_dict_i[\"pair_wise_magmom_comp_data\"]\n",
    "\n",
    "# # ['df_magmoms_comp', 'good_triplet_comb', 'pair_wise_magmom_comp_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving misc data objects for more testing elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object = group_w_o\n",
    "\n",
    "# # Pickling data ###########################################\n",
    "# import os; import pickle\n",
    "# directory = os.path.join(\n",
    "#     os.environ[\"HOME\"],\n",
    "#     \"__temp__\")\n",
    "# if not os.path.exists(directory): os.makedirs(directory)\n",
    "# path_i = os.path.join(directory, \"temp_data.pickle\")\n",
    "# with open(path_i, \"wb\") as fle:\n",
    "#     pickle.dump(save_object, fle)\n",
    "# # #########################################################\n",
    "\n",
    "# # #########################################################\n",
    "# import pickle; import os\n",
    "# directory = os.path.join(\n",
    "#     os.environ[\"HOME\"],\n",
    "#     \"__temp__\")\n",
    "# path_i = os.path.join(directory, \"temp_data.pickle\")\n",
    "# with open(path_i, \"rb\") as fle:\n",
    "#     data = pickle.load(fle)\n",
    "# # #########################################################"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
