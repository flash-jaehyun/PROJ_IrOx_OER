{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect DFT data into *, *O, *OH collections\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_analysis/compare_magmoms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_data,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    get_df_init_slabs,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    read_magmom_comp_data,\n",
    "    save_magmom_comp_data,\n",
    "    process_group_magmom_comp,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_all_jobs = False\n",
    "# redo_all_jobs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_init_slabs = get_df_init_slabs()\n",
    "\n",
    "magmom_data_dict = read_magmom_comp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# Only completed jobs will be considered\n",
    "df_jobs_anal_i = df_jobs_anal[df_jobs_anal.job_completely_done == True]\n",
    "\n",
    "# #########################################################\n",
    "# Dropping rows that failed atoms sort, now it's just one job that blew up \n",
    "# job_id = \"dubegupi_27\"\n",
    "df_failed_to_sort = df_atoms_sorted_ind[\n",
    "    df_atoms_sorted_ind.failed_to_sort == True]\n",
    "df_jobs_anal_i = df_jobs_anal_i.drop(labels=df_failed_to_sort.index)\n",
    "\n",
    "# #########################################################\n",
    "# Remove the *O slabs for now\n",
    "# The fact that they have NaN active sites will mess up the groupby\n",
    "ads_list = df_jobs_anal_i.index.get_level_values(\"ads\").tolist()\n",
    "ads_list_no_o = [i for i in list(set(ads_list)) if i != \"o\"]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "df_jobs_anal_no_o = df_jobs_anal_i.loc[idx[:, :, ads_list_no_o, :, :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "indices_to_keep = []\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "for name_i, group in grouped:\n",
    "    group_index = group.index.to_frame()\n",
    "    ads_list = list(group_index.ads.unique())\n",
    "    oh_present = \"oh\" in ads_list\n",
    "    bare_present = \"bare\" in ads_list\n",
    "    all_req_ads_present = oh_present and bare_present\n",
    "    if all_req_ads_present:\n",
    "        indices_to_keep.extend(group.index.tolist())\n",
    "\n",
    "df_jobs_anal_no_o_all_ads_pres = df_jobs_anal_no_o.loc[\n",
    "    indices_to_keep    \n",
    "    ]\n",
    "df_i = df_jobs_anal_no_o_all_ads_pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magmom comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(20 * \"TEMP \\n\")\n",
    "# # name_i = ('slac', 'relovalu_12', 24.0)\n",
    "# name_i = ('sherlock', 'vevarehu_32', 63.0)\n",
    "\n",
    "# idx = pd.IndexSlice\n",
    "# df_i = df_i.loc[idx[name_i[0], name_i[1], :, name_i[2], :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_atoms_sorted_ind = df_atoms_sorted_ind.set_index(\"job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_atoms_sorted_ind.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccfc1ec3d554105bd15f60558935618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='1st loop', max=171.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'batipoha_75', 36.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'batipoha_75', 37.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'bidoripi_03', 37.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'fugorumi_32', 38.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'fugorumi_32', 42.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'fugorumi_32', 43.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'kosofeki_50', 48.0)\n",
      "Not all job_ids have row in df_atoms_sorted_ind: ('sherlock', 'semaripe_29', 40.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "groups_to_process = []\n",
    "# #########################################################\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_i.groupby(groupby_cols)\n",
    "# #########################################################\n",
    "iterator = tqdm(grouped, desc=\"1st loop\")\n",
    "for i_cnt, (name_i, group) in enumerate(iterator):\n",
    "    # print(name_i)\n",
    "\n",
    "# if True:\n",
    "#     name_i = ('sherlock', 'batipoha_75', 36.0)\n",
    "#     group = grouped.get_group(name_i)\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    df_index = df_jobs_anal_i.index.to_frame()\n",
    "\n",
    "    df_index_i = df_index[\n",
    "        (df_index.compenv == compenv_i) & \\\n",
    "        (df_index.slab_id == slab_id_i) & \\\n",
    "        (df_index.ads == \"o\") & \\\n",
    "        [True for i in range(len(df_index))]\n",
    "        ]\n",
    "\n",
    "    row_o_i = df_jobs_anal_i.loc[\n",
    "        df_index_i.index    \n",
    "        ]\n",
    "\n",
    "    group_w_o = pd.concat([group, row_o_i, ], axis=0)\n",
    "    # #####################################################\n",
    "    job_ids_list = group_w_o.job_id_max.tolist()\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Deciding whether to reprocess the job or not\n",
    "    # #####################################################\n",
    "    out_dict_i = magmom_data_dict.get(name_i, None)\n",
    "    # #####################################################\n",
    "    if out_dict_i is None:\n",
    "        run_job = True\n",
    "    else:\n",
    "        run_job = False\n",
    "\n",
    "        job_ids_prev = out_dict_i.get(\"job_ids\", None)\n",
    "        if job_ids_prev is None:\n",
    "            run_job = True\n",
    "        else:\n",
    "            if list(np.sort(job_ids_prev)) != list(np.sort(job_ids_list)):\n",
    "                run_job = True\n",
    "\n",
    "    if redo_all_jobs:\n",
    "        run_job = True\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Testing whether all entries in df_atoms_sorted_ind exist\n",
    "    job_ids_list = list(set(group_w_o.job_id_max.tolist()))\n",
    "\n",
    "    all_pairs_ready_list = []\n",
    "    for job_id_i in job_ids_list:\n",
    "        job_id_in_sorted = False\n",
    "        if job_id_i in df_atoms_sorted_ind.job_id.tolist():\n",
    "            job_id_in_sorted = True\n",
    "        all_pairs_ready_list.append(job_id_in_sorted)\n",
    "    all_pairs_ready = all(all_pairs_ready_list)\n",
    "\n",
    "    if not all_pairs_ready:\n",
    "        print(\"Not all job_ids have row in df_atoms_sorted_ind:\", name_i)\n",
    "        run_job = False\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    if run_job:\n",
    "        # print(\"This is good:\", name_i)\n",
    "\n",
    "        groups_to_process.append(name_i)\n",
    "\n",
    "        # COMMENT THIS OUT TO RUN PARALLEL!!!!!!!\n",
    "\n",
    "#         out_dict_i = process_group_magmom_comp(\n",
    "#             name=name_i,\n",
    "#             group=group_w_o,\n",
    "#             write_atoms_objects=False,\n",
    "#             verbose=False,\n",
    "#             )\n",
    "\n",
    "#         save_magmom_comp_data(name_i, out_dict_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_atoms_sorted_ind.set_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_ready_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running magmom comparison in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_wrap(input_dict):\n",
    "    group_w_o = input_dict[\"group_w_o\"]\n",
    "    name_i = input_dict[\"name_i\"]\n",
    "\n",
    "    out_dict_i = process_group_magmom_comp(\n",
    "        name=name_i,\n",
    "        group=group_w_o,\n",
    "        write_atoms_objects=False,\n",
    "        verbose=False,\n",
    "        )\n",
    "\n",
    "    save_magmom_comp_data(name_i, out_dict_i)\n",
    "\n",
    "\n",
    "input_list = []\n",
    "for name_i in groups_to_process:\n",
    "    # #####################################################\n",
    "    group_i = grouped.get_group(name_i)\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    df_index = df_jobs_anal_i.index.to_frame()\n",
    "\n",
    "    df_index_i = df_index[\n",
    "        (df_index.compenv == compenv_i) & \\\n",
    "        (df_index.slab_id == slab_id_i) & \\\n",
    "        (df_index.ads == \"o\") & \\\n",
    "        [True for i in range(len(df_index))]\n",
    "        ]\n",
    "\n",
    "    row_o_i = df_jobs_anal_i.loc[\n",
    "        df_index_i.index    \n",
    "        ]\n",
    "\n",
    "    group_w_o = pd.concat([group_i, row_o_i, ], axis=0)\n",
    "\n",
    "\n",
    "    input_dict_i = dict(\n",
    "        group_w_o=group_w_o,\n",
    "        name_i=name_i,\n",
    "        )\n",
    "    input_list.append(input_dict_i)\n",
    "\n",
    "variables_dict = dict()\n",
    "traces_all = Pool().map(\n",
    "    partial(\n",
    "        method_wrap,  # METHOD\n",
    "        **variables_dict,  # KWARGS\n",
    "        ),\n",
    "    input_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying which slabs have zero magmoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_list = []\n",
    "data_dict_list = []\n",
    "for name_i, row_i in df_jobs_anal_i.iterrows():\n",
    "    # #########################################################\n",
    "    data_dict_i = dict()\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    ads_i = name_i[2]\n",
    "    active_site_i = name_i[3]\n",
    "    att_num_i = name_i[4]\n",
    "    # #####################################################\n",
    "\n",
    "    process_row = False\n",
    "\n",
    "    # #####################################################\n",
    "    job_id_i = row_i.job_id_max\n",
    "    # #####################################################\n",
    "    \n",
    "    # #####################################################\n",
    "    if name_i in df_atoms_sorted_ind.index:\n",
    "        process_row = True\n",
    "        row_atoms_i = df_atoms_sorted_ind.loc[name_i]\n",
    "        # #################################################\n",
    "        atoms = row_atoms_i.atoms_sorted_good\n",
    "        magmoms_i = row_atoms_i.magmoms_sorted_good\n",
    "        # #################################################\n",
    "\n",
    "            # (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]\n",
    "    # row_atoms_i = df_atoms_sorted_ind.loc[\n",
    "    #     (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]\n",
    "    \n",
    "    if process_row:\n",
    "        if atoms.calc != None:\n",
    "            magmoms_i = atoms.get_magnetic_moments()\n",
    "        else:\n",
    "            magmoms_i = magmoms_i\n",
    "\n",
    "        sum_magmoms_i = np.sum(magmoms_i)\n",
    "        sum_abs_magmoms = np.sum(np.abs(magmoms_i))\n",
    "\n",
    "        # #########################################################\n",
    "        data_dict_i[\"compenv\"] = compenv_i\n",
    "        data_dict_i[\"slab_id\"] = slab_id_i\n",
    "        data_dict_i[\"ads\"] = ads_i\n",
    "        data_dict_i[\"active_site\"] = active_site_i\n",
    "        data_dict_i[\"att_num\"] = att_num_i\n",
    "        # #########################################################\n",
    "        data_dict_i[\"job_id\"] = job_id_i\n",
    "        data_dict_i[\"sum_magmoms\"] = sum_magmoms_i\n",
    "        data_dict_i[\"sum_abs_magmoms\"] = sum_abs_magmoms\n",
    "        # #########################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #########################################################\n",
    "\n",
    "df_magmoms = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing magmoms by number of atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    ads_i = row_i.ads\n",
    "    active_site_i = row_i.active_site\n",
    "    att_num_i = row_i.att_num\n",
    "    sum_magmoms_i = row_i.sum_magmoms\n",
    "    sum_abs_magmoms_i = row_i.sum_abs_magmoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    name_i = (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)\n",
    "    row_slab_i = df_init_slabs.loc[name_i]\n",
    "    # #####################################################\n",
    "    num_atoms_i = row_slab_i.num_atoms\n",
    "    # #####################################################\n",
    "\n",
    "    sum_magmoms_pa = sum_magmoms_i / num_atoms_i\n",
    "    sum_abs_magmoms_pa = sum_abs_magmoms_i / num_atoms_i\n",
    "\n",
    "    # #####################################################\n",
    "    row_i[\"sum_magmoms_pa\"] = sum_magmoms_pa\n",
    "    row_i[\"sum_abs_magmoms_pa\"] = sum_abs_magmoms_pa\n",
    "    # #####################################################\n",
    "    return(row_i)\n",
    "    # #####################################################\n",
    "\n",
    "df_magmoms = df_magmoms.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further analysis of magmom comparison (collapse into dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for name_i in magmom_data_dict.keys():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    magmom_data_dict[name_i].keys()\n",
    "\n",
    "    from IPython.display import display\n",
    "    df_magmoms_comp_i = magmom_data_dict[name_i][\"df_magmoms_comp\"]\n",
    "    # display(df_magmoms_comp_i)\n",
    "    min_val_i = df_magmoms_comp_i.sum_norm_abs_magmom_diff.min()\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"name\"] = name_i\n",
    "    data_dict_i[\"min_sum_norm_abs_magmom_diff\"] = min_val_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "df.min_sum_norm_abs_magmom_diff.min()\n",
    "\n",
    "df = df.sort_values(\"min_sum_norm_abs_magmom_diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/compare_magmoms\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "path_i = os.path.join(directory, \"df_magmoms.pickle\")\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_magmoms, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_magmoms\n",
    "\n",
    "df_magmoms_tmp = get_df_magmoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"analyse_jobs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_failed_to_sort = df_atoms_sorted_ind[df_atoms_sorted_ind.failed_to_sort == True]\n",
    "\n",
    "# df_i = df_i.drop(labels=df_failed_to_sort.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# groups_to_process = groups_to_process[0:2]\n",
    "\n",
    "# groups_to_process"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
